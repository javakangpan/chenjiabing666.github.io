<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Navicat Premium 12破解版安装]]></title>
      <url>%2F2020%2F08%2F27%2FNavicat%20Premium%2012%E5%85%8D%E8%B4%B9%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85%2F</url>
      <content type="text"><![CDATA[前言 这几年的工作过程中使用了很多的数据库工具，比如Sqlyog，DBeaver,sqlplus等工具，但是个人觉得很好用的还是Navicat。 不如人意的就是目前Navicat都在收费，今天就来分享下如何安装免费的Navicat。 免费版本安装 首先去官网下载Navicat_12的安装包，根据自己电脑的配置下载合适的。 下载成功之后，直接安装，启动即可 启动时选择试用版本。 打开神秘的包包，找到匹配的，如下： 将其中的文件全部复制到Navicat_12的根目录，文件如下： 重新启动Navicat，出现以下界面，表示安装成功，如下： 如何连接Oracle 如果本地未安装过Oracle数据库，新安装的Navicat默认是连接不上oracle的，需要配置一下oci.dll。 选择工具-&gt;选项 指定oci.dll的路径，如下： 重新启动，即可连接。 注意：Navicat_12自带的oci.dll如果版本不合适，可以去官网下载对应的版本。 如何连接Sql server 如果本地未安装过SQL Server数据库，Navicat是不能连接上数据库的，具体解决方案如下： 在Navicat的根目录下找到sqlncli_x64.msi双击安装即可，当然如果版本不合适，可以自己去官网下载。 安装成功后，重启启动，即可连接。 总结 安装免费版的Navicat很简单，只需要一个神秘的包包，哈哈。 老规矩，关注公众号【码猿技术专栏】回复关键词Navicat12即可获取。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mybatis Log plugin破解，亲测可用！！！]]></title>
      <url>%2F2020%2F08%2F26%2Fplugin%E7%A0%B4%E8%A7%A3%EF%BC%8C%E4%BA%B2%E6%B5%8B%E5%8F%AF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[前言 今天重新装了IDEA2020，顺带重装了一些插件，毕竟这些插件都是习惯一直在用，其中一款就是Mybatis Log plugin，按照往常的思路，在IDEA插件市场搜索安装，艹，眼睛一瞟，竟然收费了，对于我这种支持盗版的人来说太难了，于是自己开始捣鼓各种尝试破解，下文分享自己的破解方式。 什么是Mybatis Log plugin 举个栗子，通常在找bug的时候都会查看执行了什么SQL，想把这条SQL拼接出来执行调试，可能有些小白还在傻傻的把各个参数复制出来，补到?占位符中，哈哈。 简单的说就是能根据log4j的打印的sql日志一键生成执行的sql语句。 类似如下一个日志信息： 如果使用Log plugin这个插件，将会很容易的把参数添加到sql语句中得到一条完整的sql，效果如下： 一旦开启了Mybatis Log plugin这个插件，在程序运行过程中只要是有SQL语句都会自动生成在Mybatis Log这个界面，当然也可以自己关掉。 如何安装 Setting-&gt;plugin-&gt;Marketplace搜索框输入Mybatis Log plugin，如下： 很遗憾的是， IDEA2020中已经开始收费了，艹，对于一向支持盗版的我来说，很不爽~ 如何破解 下载jar包plugin.intellij.assistant.mybaitslog-2020.1-1.0.3.jar，文末附有下载方式。 setting-&gt;plguin-&gt;设置-&gt; install plugin from Disk... 如何使用 日志中从Preparing到Parameters这两行的参数选中，右键选择restore sql from Selection 此时将会在Mybatis Log界面出现完整的SQL语句。 总结 对于复杂的SQL语句来说，Mybatis Log plugin这款插件简直是太爱了，能够自动拼接参数生成执行的SQL语句。 老规矩，关注公众号【码猿技术专栏】，公众号回复mybatis log获取破解包。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[select语句做了什么？]]></title>
      <url>%2F2020%2F05%2F08%2Fselect%E8%AF%AD%E5%8F%A5%E5%81%9A%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[导读 Mysql在中小型企业中是个香饽饽，目前主流的数据库之一，几乎没有一个后端开发者不会使用的，但是作为一个老司机，仅仅会用真的不够。今天陈某透过一个简单的查询语句来讲述在Mysql内部的执行过程。select from table where id=10;撸它首先通过一张图片来了解一下Mysql的基础架构，如下：从上图可以看出，Mysql大致分为Server层和存储引擎层两部分。Server层包括连接器、查询缓存、分析器、优化器等，其中包含了Mysql的大多数核心功能以及所有的内置函数（如日期，时间函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。存储引擎层负责数据的存储和提取。它的架构是可插拔式的，支持InnoDB、MyISAM等多个存储引擎。Mysql中主流的存储引擎是InnoDB，由于它对事务的支持让它从Mysql5.5.5版本开始成为了默认的存储引擎。大致了解了整体架构，现在说说每一个基础的模块都承担着怎样的责任。1. 连接器顾名思义，是客户端和Mysql之间连接的媒介，负责登录、获取权限、维持连接和管理连接。连接命令一般如下：mysql [-h] ip [- P] port -u [user] -p在完成经典的TCP握手后，连接器会开始认证身份，要求输入密码。密码认证通过，连接器会查询出拥有的权限，即使管理员修改了权限，也不会影响你这次的连接，只有重新连接才会生效。密码认证失败，会收到提示信息Access denied for user。连接完成后，没有后续动作的连接将会变成空闲连接，你可以输入show processlist命令看到它。如下图，其中的Command这一列显示为sleep的这一行表示在系统里面有一个空闲连接。客户端如果太长时间没有执行动作，连接器将会自动断开，这个时间由参数wait_timeout控制，默认值是8小时。如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。2. 查询缓存【废材，8.0 版本完全删除】连接建立完成后，你就可以select语句了，执行之前会查询缓存。查询缓存在Mysql中的是默认关闭的，因为缓存命中率非常低，只要有对表执行一个更新操作，这个表的所有查询缓存都将被清空。怎么样？一句废材足以形容了！！！废材的东西不必多讲，主流的Redis的缓存你不用，别再搞这废材了。3. 分析器如果没有命中查询缓存，就要执行查询了，但是在执行查询之前，需要对SQL语句做解析，判断你这条语句有没有语法错误。分析器会做 ‘词法分析’ ，你输入的无非可就是多个字符串和空格组成的SQL语句，MYSQL需要识别出里面的字符串是什么，代表什么，有没有关键词等。MYSQL会从你输入的select 这个关键字识别出来是一个查询语句，table是表名，id是列名。做完这些会做 ‘语法分析’ ，根据MYSQL定义的规则来判断你的SQL语句有没有语法错误，如果你的语法不对，就会收到类似如下的提醒：ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ‘elect from t where ID=1’ at line 1一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。4. 优化器经过分析器词法和语法的分析，此时就能知道这条SQL语句是干什么的。但是在开始执行之前，MYSQL底层还要使用优化器对这条SQL语句进行优化处理。MYSQL内部会对这条SQL进行评估，比如涉及到多个索引会比较使用哪个索引代价更小、多表join的时候会考虑决定各个表的连接顺序。优化器的作用一句话总结：根据MYSQL内部的算法决定如何执行这条SQL语句来达到MYSQL认为代价最小目的。优化器阶段完成后，这个语句的执行方案就确定了，接下来就交给执行器执行了。5. 执行器MYSQL通过分析器知道了要做什么，通过优化器知道了如何做，于是就进入了执行器阶段。执行器开始执行之前，需要检查一下用户对表table有没有执行的权限，没有返回权限不足的错误，有的话就执行。执行也是分类的，如果Id不是索引则全表扫描，一行一行的查找，如果是索引则在索引组织表中查询，索引的查询很复杂，其中涉及到B+树等算法，这里不再详细介绍。总结一条SQL语句在MYSQL内部执行的过程涉及到的内部模块有：连接器、查询缓存、分析器、优化器、执行器、存储引擎。至此，MYSQL的基础架构已经讲完了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[IDEA2020永久破解，亲测可用！！！]]></title>
      <url>%2F2020%2F05%2F08%2FIDEA2020%E6%B0%B8%E4%B9%85%E7%A0%B4%E8%A7%A3%EF%BC%8C%E4%BA%B2%E6%B5%8B%E5%8F%AF%E7%94%A8%EF%BC%81%EF%BC%81%EF%BC%81%2F</url>
      <content type="text"><![CDATA[前言 随着 IDEA 的 2020 版本的发布，新增和优化了很多的功能，今天陈某不说新增的功能，来讲一讲如何永久破解。 不说别的，先上破解后的效果图： 如何破解？ 破解过程很简单，基本是傻瓜式的，过程如下。 1. 下载安装 官网下载IDEA 2020.1，下载地址自己动手百度吧。 安装成功后，启动 IDEA，选择试用启动 IDEA。 下载破解包 公众号回复关键词IDEA破解包下载，其实就是一个 jar 包，如下： 开始破解 直接将jetbrains-agent.jar文件用鼠标拖进idea 界面，然后一路重启或者确定，中间出现什么拖放失败不用理会，直接点确定就好是正常现象。如下图： 重启成功后，会跳出如下界面，直接点击为IDEA安装，如下图： 继续点击是安装，如下图： 验证是否破解成功 根据上述的步骤 99%的可能破解成功，此时打开 IDEA，点击help-&gt;Register查看是否破解成功，出现下图将是破解成功，如下： 里面的激活码是重启之上自动填入的，如果不行找到下载的压缩文件 lib 下的 ACTIVATION_CODE.txt 换一个激活码或者查看 README.pdf 帮助。 总结 至此 IDEA 2020 已经破解成功了，按照陈某的步骤 99%的朋友保证能够破解成功，文中的破解文件在公众号回复IDEA破解包即可获取。 另外如果想要 IDEA 2020 的安装包，回复关键词IDEA2020即可获取。 关注我微信公众号【码猿技术专栏】]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[线上Bug无法复现怎么办？老司机教你一招，SpringBoot远程调试不用愁！]]></title>
      <url>%2F2020%2F04%2F28%2F%E7%BA%BF%E4%B8%8ABug%E6%97%A0%E6%B3%95%E5%A4%8D%E7%8E%B0%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F%E8%80%81%E5%8F%B8%E6%9C%BA%E6%95%99%E4%BD%A0%E4%B8%80%E6%8B%9B%EF%BC%8CSpringBoot%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95%E4%B8%8D%E7%94%A8%E6%84%81%EF%BC%81%2F</url>
      <content type="text"><![CDATA[前言 在部署线上项目时，相信大家都会遇到一个问题，线上的 Bug 但是在本地不会复现，多么无奈。 此时最常用的就是取到前端传递的数据用接口测试工具测试，比如 POSTMAN，复杂不，难受不？ 今天陈某教你一招，让你轻松调试线上的 Bug。文章目录如下： 什么是 JPDA？ JPDA(Java Platform Debugger Architecture)，即 Java 平台调试体系，具体结构图如下图所示。 其中实现调试功能的主要协议是JDWP协议，在 Java SE 5 以前版本，JVM 端的实现接口是 JVMPI(Java Virtual Machine Profiler Interface)，而在 Java SE 5 及以后版本，使用 JVMTI(Java Virtual Machine Tool Interface) 来替代 JVMPI。 因此，如果使用 Java SE 5 之前版本，使用调试功能的命令为： 1java -Xdebug -Xrunjdwp:... 而 Java SE 5 及之后版本，使用调试功能的命令为： 1java -agentlib:jdwp=... 调试命令 现在开发中最常见的一条远程调试的的命令如下： 1java -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=9091 -jar xxx.jar 参数说明 基于前面的调试命令，我们来分析一下基本的参数代表什么意思。 transport 指定运行的被调试应用和调试者之间的通信协议，它由几个可选值： dt_socket：主要的方式，采用socket方式连接。 dt_shmem：采用共享内存方式连接，仅支持 Windows 平台。 server 指定当前应用作为调试服务端还是客户端，默认为n。 如果你想将当前应用作为被调试应用，设置该值为 y,如果你想将当前应用作为客户端，作为调试的发起者，设置该值为n。 suspend 当前应用启动后，是否阻塞应用直到被连接，默认值为 y。 在大部分的应用场景，这个值为 n，即不需要应用阻塞等待连接。一个可能为 y的应用场景是，你的程序在启动时出现了一个故障，为了调试，必须等到调试方连接上来后程序再启动。 address 暴露的调试连接端口，默认值为 8000。 此端口一定不能与项目端口重复，且必须是服务器开放的端口。 onthrow 当程序抛出设定异常时，中断调试。 onuncaught 当程序抛出未捕获异常时，是否中断调试，默认值为 n。 launch 当调试中断时，执行的程序。 timeout 该参数限定为java -agentlib:jdwp=…可用，单位为毫秒ms。 当 suspend = y 时，该值表示等待连接的超时；当 suspend = n 时，该值表示连接后的使用超时。 参考命令 -agentlib:jdwp=transport=dt_socket,server=y,address=8000：以 Socket 方式监听 8000 端口，程序启动阻塞（suspend 的默认值为 y）直到被连接。 -agentlib:jdwp=transport=dt_socket,server=y,address=localhost:8000,timeout=5000：以 Socket 方式监听 8000 端口，当程序启动后 5 秒无调试者连接的话终止，程序启动阻塞（suspend 的默认值为 y）直到被连接。 -agentlib:jdwp=transport=dt_shmem,server=y,suspend=n：选择可用的共享内存连接地址并使用 stdout 打印，程序启动不阻塞。 -agentlib:jdwp=transport=dt_socket,address=myhost:8000：以 socket 方式连接到 myhost:8000上的调试程序，在连接成功前启动阻塞。 -agentlib:jdwp=transport=dt_socket,server=y,address=8000,onthrow=java.io.IOException,launch=/usr/local/bin/debugstub：以 Socket 方式监听 8000 端口，程序启动阻塞（suspend 的默认值为 y）直到被连接。当抛出 IOException 时中断调试，转而执行 usr/local/bin/debugstub程序。 IDEA 远程调试示例 首先打包 SpringBoot 项目，在服务器上运行，执行以下命令： 1java -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=9190 -jar debug-demo.jar 出现下图的界面，表示运行成功： 然后在 IDEA 中，点击 Edit Configurations，在弹框中点击 + 号，然后选择Remote。 填写服务器的地址及端口，点击 OK 即可。 配置完毕后，DEBUG 调试运行即可。 配置完毕后点击保存即可，因为我配置的 suspend=n，因此服务端程序无需阻塞等待我们的连接。我们点击 IDEA 调试按钮，当我访问某一接口时，能够正常调试。 小福利 作者为大家准备了接近 10M 的面试题，涵盖后端各个技术维度，老规矩，公众号内回复关键词JAVA面试题即可免费获取。 关注微信公众号回复关键词：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[大白话布隆过滤器，又能和面试官扯皮了！！！]]></title>
      <url>%2F2020%2F04%2F26%2F%E5%A4%A7%E7%99%BD%E8%AF%9D%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%EF%BC%8C%E5%8F%88%E8%83%BD%E5%92%8C%E9%9D%A2%E8%AF%95%E5%AE%98%E6%89%AF%E7%9A%AE%E4%BA%86%EF%BC%81%EF%BC%81%EF%BC%81%2F</url>
      <content type="text"><![CDATA[前言 文章首发于微信公众号大白话布隆过滤器，又能和面试官扯皮了~ 近期在做推荐系统中已读内容去重的相关内容，刚好用到了布隆过滤器，于是写了一篇文章记录分享一下。 文章的篇幅不是很长，主要讲了布隆过滤器的核心思想，目录如下： 什么是布隆过滤器？ 布隆过滤器是由一个长度为m比特的位数组与k个哈希函数组成的数据结构。比特数组均初始化为0，所有哈希函数都可以分别把输入数据尽量均匀地散列。 当插入一个元素时，将其数据通过k个哈希函数转换成k个哈希值，这k个哈希值将作为比特数组的下标，并将数组中的对应下标的值置为1。 当查询一个元素时，同样会将其数据通过k个哈希函数转换成k个哈希值（数组下标），查询数组中对应下标的值，如果有一个下标的值为0表明该元素一定不在集合中，如果全部下标的值都为1，表明该元素有可能在集合中。至于为什么有可能在集合中？ 因为有可能某个或者多个下标的值为 1 是受到其他元素的影响，这就是所谓的假阳性，下文会详细讲述。 无法删除一个元素，为什么呢？因为你删除的元素的哈希值可能和集合中的某个元素的哈希值有相同的，一旦删除了这个元素会导致其他的元素也被删除。 下图示出一个m=18, k=3的布隆过滤器示例。集合中的 x、y、z 三个元素通过 3 个不同的哈希函数散列到位数组中。当查询元素 w 时，因为有一个比特为 0，因此 w 不在该集合中。 假阳性概率的计算 假阳性是布隆过滤器的一个痛点，因此需要不择一切手段来使假阳性的概率降低，此时就需要计算一下假阳性的概率了。 假设我们的哈希函数选择位数组中的比特时，都是等概率的。当然在设计哈希函数时，也应该尽量满足均匀分布。 在位数组长度m的布隆过滤器中插入一个元素，它的其中一个哈希函数会将某个特定的比特置为1。因此，在插入元素后，该比特仍然为 0 的概率是： 现有k个哈希函数，并插入n个元素，自然就可以得到该比特仍然为 0 的概率是： 反过来讲，它已经被置为1的概率就是： 也就是说，如果在插入n个元素后，我们用一个不在集合中的元素来检测，那么被误报为存在于集合中的概率（也就是所有哈希函数对应的比特都为1的概率）为： 当n比较大时，根据极限公式，可以近似得出假阳性率： 所以，在哈希函数个数k一定的情况下有如下结论： 位数组长度 m 越大，假阳性率越低。 已插入元素的个数 n 越大，假阳性率越高。 优点 用比特数组表示，不用存储数据本身，对空间的节省相比于传统方式占据绝对的优势。 时间效率很高，无论是插入还是查询，只需要简单的经过哈希函数转换，时间复杂度均为O(k)。 缺点 存在假阳性的概率，准确率要求高的场景不太适用。 只能插入和查询，不能删除了元素。 应用场景 布隆过滤器的用途很多，但是主要的作用就是去重，这里列举几个使用场景。 爬虫重复 URL 检测 试想一下，百度是一个爬虫，它会定时搜集各大网站的信息，文章，那么它是如何保证爬取到文章信息不重复，它会将 URL 存放到布隆过滤器中，每次爬取之前先从布隆过滤器中判断这个 URL 是否存在，这样就避免了重复爬取。当然这种存在假阳性的可能，但是只要你的比特数组足够大，假阳性的概率会很低，另一方面，你认为百度会在意这种的误差吗，你的一篇文章可能因为假阳性概率没有收录到，对百度有影响吗？ 抖音推荐功能 读者朋友们应该没人没刷过抖音吧，每次刷的时候抖音给你的视频有重复的吗？他是如何保证推荐的内容不重复的呢？ 最容易想到的就是抖音会记录用户的历史观看记录，然后从历史记录中排除。这是一种解决办法，但是性能呢？不用多说了，有点常识的都知道这不可能。 解决这种重复的问题，布隆过滤器有着绝对的优势，能够很轻松的解决。 防止缓存穿透 缓存穿透是指查询一条数据库和缓存都没有的一条数据，就会一直查询数据库，对数据库的访问压力会一直增大。 布隆过滤器在解决缓存穿透的问题效果也是很好，这里不再细说，后续文章会写。 如何实现布隆过滤器？ 了解布隆过滤器的设计思想之后，想要实现一个布隆过滤器其实很简单，陈某这里就不再搬门弄斧了，介绍一下现成的实现方式吧。 Redis 实现 Redis4.0 之后推出了插件的功能，下面用 docker 安装： 12docker pull redislabs/rebloomdocker run -p6379:6379 redislabs/rebloom 安装完成后连接 redis 即可，运行命令： 1redis-cli 至于具体的使用这里不再演示了，直接看官方文档和教程，使用起来还是很简单的。 Guava 实现 guava 对应布隆过滤器的实现做出了支持，使用 guava 可以很轻松的实现一个布隆过滤器。 1. 创建布隆过滤器 创建布隆过滤器，如下： 12345678BloomFilter&lt;Integer&gt; filter = BloomFilter.create( Funnels.integerFunnel(), 5000, 0.01);//插入IntStream.range(0, 100_000).forEach(filter::put);//判断是否存在boolean b = filter.mightContain(1); arg1：用于将任意类型 T 的输入数据转化为 Java 基本类型的数据，这里转换为 byte arg2：byte 字节数组的基数 arg3：期望的假阳性概率 2.估计最优 m 值和 k 值 guava 在底层对 byte 数组的基数(m)和哈希函数的个数 k 做了自己的算法，源码如下： 12345678910111213//m值的计算static long optimalNumOfBits(long n, double p) &#123; if (p == 0) &#123; p = Double.MIN_VALUE; &#125; return (long) (-n * Math.log(p) / (Math.log(2) * Math.log(2)));&#125;//k值的计算static int optimalNumOfHashFunctions(long n, long m) &#123; // (m / n) * log(2), but avoid truncation due to division! return Math.max(1, (int) Math.round((double) m / n * Math.log(2)));&#125; 想要理解 guava 的计算原理，还要从的上面推导的过程继续。 由假阳性率的近似计算方法可知，如果要使假阳性率尽量小，在 m 和 n 给定的情况下，k值应为： 将 k 代入上一节的式子并化简，我们可以整理出期望假阳性率 p 与 m、n 的关系： 换算而得： 根据以上分析得出以下的结论： 如果指定期望假阳性率 p，那么最优的 m 值与期望元素数 n 呈线性关系。 最优的 k 值实际上只与 p 有关，与 m 和 n 都无关，即： 综上两个结论，在创建布隆过滤器的时候，确定p值和m值很重要。 总结 至此，布隆过滤器的知识介绍到这里，如果觉得陈某写得不错的，转发在看点一波，读者的一份支持将会是我莫大的鼓励。 另外想和陈某私聊或者想要加交流群的朋友，公众号回复关键词加群加陈某微信，陈某会第一时间拉你进群。 巨人的肩膀 https://blog.csdn.net/u012422440/article/details/94088166 https://blog.csdn.net/Revivedsun/article/details/94992323]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[看完这篇缓存穿透的文章，保证你能和面试官互扯！！！]]></title>
      <url>%2F2020%2F04%2F26%2F%E7%9C%8B%E5%AE%8C%E8%BF%99%E7%AF%87%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E7%9A%84%E6%96%87%E7%AB%A0%EF%BC%8C%E4%BF%9D%E8%AF%81%E4%BD%A0%E8%83%BD%E5%92%8C%E9%9D%A2%E8%AF%95%E5%AE%98%E4%BA%92%E6%89%AF%EF%BC%81%EF%BC%81%EF%BC%81%2F</url>
      <content type="text"><![CDATA[前言 昨天有读者朋友留言，想要陈某写一篇防止缓存穿透的文章，今天特意写了一篇。 文章目录如下： 什么是缓存穿透？ 缓存穿透其实是指从缓存中没有查到数据，而不得不从后端系统（比如数据库）中查询的情况。 缓存毕竟是在内存中，不可能所有的数据都存储在 Redis 中，因此少量的缓存穿透是不可避免的，也是系统能够承受的，但是一旦在瞬间发生大量的缓存穿透，数据库的压力会瞬间增大，后果可想而知。 在开发中使用缓存的方案如下图，在查询数据库之前会先查询 Redis： 缓存穿透的整个过程分为如下几个步骤： 应用查询缓存，缓存不命中 DB 层查询不命中，不将空结果缓存 返回空结果 下一个请求继续重复1,2,3步。 解决方案 万事万物都是相生相克，既然出现了缓存穿透，就一定有避免的方案。 下面介绍两种缓存的方案，分别是缓存空值、布隆过滤器。 缓存空值 回顾缓存穿透的定义知道，大量空值没有缓存导致重复的访问 DB 层，由此解决方案也是很明显了，直接将返回的空值也缓存即可。此时的执行步骤如下图： 如上图所示，如果缓存不命中，查询 DB 层之后，直接将空值缓存在 Redis 中。伪代码如下： 123456789101112Object nullValue = new Object();try &#123; Object valueFromDB = getFromDB(uid); //从数据库中查询数据 if (valueFromDB == null) &#123; cache.set(uid, nullValue, 10); //如果从数据库中查询到空值，就把空值写入缓存，设置较短的超时时间 &#125; else &#123; cache.set(uid, valueFromDB, 1000); &#125;&#125; catch(Exception e) &#123; // 出现异常也要写入缓存 cache.set(uid, nullValue, 10);&#125; 通过伪代码可以很清楚的了解了缓存空值的流程，但是需要注意以下问题： 缓存一定要设置过期时间：因为空值并不是准确的业务数据，并且会占用缓存空间，所以要给空值加上一个过期时间，使得能够在短期之内被淘汰。但是随之而来的一个问题就是在一定的时间窗口内缓存的数据和实际数据不一致，比如设置 10 秒钟过期时间，但是在这 10 秒之内业务又写入了数据，那么返回就不应该为空值了，所以还要考虑数据一致的问题，解决方法很简单，利用消息系统或者主动更新的方式清除掉缓存中的数据即可。 布隆过滤器 1970 年布隆提出了一种布隆过滤器的算法，用来判断一个元素是否在一个集合中。这种算法由一个二进制数组和一个 Hash 算法组成。 具体的算法思想这里不再详细解释了，如有不了解的可以看陈某上一篇文章大白话布隆过滤器，又能和面试官扯皮了~。 解决缓存穿透的大致思想：在访问缓存层和存储层之前，可以通过定时任务或者系统任务来初始化布隆过滤器，将存在的 key 用布隆过滤器提前保存起来，做第一层的拦截。例如：一个推荐系统有 4 亿个用户 id， 每个小时算法工程师会根据每个用户之前历史行为计算出推荐数据放到存储层中， 但是最新的用户由于没有历史行为， 就会发生缓存穿透的行为， 为此可以将所有推荐数据的用户做成布隆过滤器。 如果布隆过滤器认为该用户 id 不存在， 那么就不会访问存储层， 在一定程度保护了存储层。此时的结构如下图： 当然布隆过滤器的假阳性的存在导致了误判率，但是我们可以尽量的降低误判率，一个解决方案就是：使用多个 Hash 算法为元素计算出多个 Hash 值，只有所有 Hash 值对应的数组中的值都为 1 时，才会认为这个元素在集合中。 这种方法适用于数据命中不高、 数据相对固定、 实时性低（通常是数据 集较大）的应用场景，代码维护较为复杂，但是缓存空间占用少。为什么呢？因为布隆过滤器不支持删除元素，一旦数据变化，并不能及时的更新布隆过滤器。 两种方案对比 两种方案各有优缺点，具体使用哪种方案还是要根据业务场景和系统体量来定。具体的区别如下表： 方案 适用场景 维护成本 缓存对象 1. 数据命中不高 2. 数据频繁变化，实时性高 代码维护点单、需要过多的缓存空间，数据一致性需要自己实现 布隆过滤器 1. 数据命中不高 2.数据相对固定，实时性低 代码维护复杂、缓存空间占用少 总结 至此，如何解决缓存穿透的问题已经介绍完了，觉得写得不错的，有所收获的朋友，点点在看，分享关注一波。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[explain执行计划]]></title>
      <url>%2F2020%2F04%2F20%2Fexplain%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%2F</url>
      <content type="text"><![CDATA[前言如何写出效率高的SQL语句，提到这必然离不开Explain执行计划的分析，至于什么是执行计划，如何写出高效率的SQL，本篇文章将会一一介绍。执行计划执行计划是数据库根据 SQL 语句和相关表的统计信息作出的一个查询方案，这个方案是由查询优化器自动分析产生的。使用explain关键字可以模拟优化器执行 SQL 查询语句，从而知道 MySQL 是如何处理你的 SQL 语句的，分析你的 select 语句或是表结构的性能瓶颈，让我们知道 select 效率低下的原因，从而改进我们的查询。explain 的结果如下：下面是有关各列的详细介绍，重要的有id、type、key、rows、extra。idid 列的编号就是 select 的序列号，也可以理解为 SQL 执行顺序的标识，有几个 select 就有几个 id。id 值不同：如果是只查询，id 的序号会递增，id 值越大优先级越高，越先被执行；id 值相同：从上往下依次执行；id 列为 null：表示这是一个结果集，不需要使用它来进行查询。select_type查询的类型，主要用于区分普通查询、联合查询、子查询等复杂的查询；simple：表示查询中不包括 union 操作或者子查询，位于最外层的查询的 select_type 即为 simple，且只有一个； explain select from t3 where id=3952602;primary：需要 union 操作或者含有子查询的 select，位于最外层的查询的 select_type 即为 primary，且只有一个；explain select from (select from t3 where id=3952602) a ;derived：from 列表中出现的子查询，也叫做衍生表；mysql 或者递归执行这些子查询，把结果放在临时表里。 explain select from (select from t3 where id=3952602) a ;subquery：除了 from 子句中包含的子查询外，其他地方出现的子查询都可能是 subquery。explain select from t3 where id = (select id from t3 whereid=3952602 ) ;union：若第二个 select 出现在 union 之后，则被标记为 union；若 union 包含在 from 子句的子查询中，外层 select 将被标记为 derived。explain select from t3 where id=3952602 union all select from t3;union result：从 union 表获取结果的 select ，因为它不需要参与查询，所以 id 字段为 null。 explain select from t3 where id=3952602 union all select from t3;dependent union：与 union 一样，出现在 union 或 union all 语句中，但是这个查询要受到外部查询的影响；dependent subquery：与 dependent union 类似，子查询中的第一个 SELECT，这个 subquery 的查询要受到外部表查询的影响。table表示 explain 的一行正在访问哪个表。如果查询使用了别名，那么这里显示的是别名;如果不涉及对数据表的操作，那么这显示为 null;如果显示为尖括号括起来的就表示这个是临时表，后边的 N 就是执行计划中的 id，表示结果来自于这个查询产生;如果是尖括号括起来的&lt;union M,N&gt;，与类似，也是一个临时表，表示这个结果来自于 union 查询的 id 为 M,N 的结果集。type访问类型，即 MySQL 决定如何查找表中的行。依次从好到差：system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL，除了 all 之外，其他的 type 都可以使用到索引，除了 index_merge 之外，其他的 type 只可以用到一个索引。一般来说，得保证查询至少达到 range 级别，最好能达到 ref。system：表中只有一行数据（等于系统表），这是 const 类型的特例，平时不会出现，可以忽略不计。const：使用唯一索引或者主键，表示通过索引一次就找到了，const 用于比较 primary key 或者 unique 索引。因为只需匹配一行数据，所有很快。如果将主键置于 where 列表中，mysql 就能将该查询转换为一个 const。eq_ref：唯一性索引扫描，对于每个索引键，表中只有一行数据与之匹配。常见于主键或唯一索引扫描。ref：非唯一性索引扫描，返回匹配某个单独值的所有行。本质也是一种索引。fulltext：全文索引检索，全文索引的优先级很高，若全文索引和普通索引同时存在时，mysql 不管代价，优先选择使用全文索引。ref_or_null：与 ref 方法类似，只是增加了 null 值的比较。index_merge：表示查询使用了两个以上的索引，索引合并的优化方法，最后取交集或者并集，常见 and ，or 的条件使用了不同的索引。unique_subquery：用于 where 中的 in 形式子查询，子查询返回不重复值唯一值；index_subquery：用于 in 形式子查询使用到了辅助索引或者 in 常数列表，子查询可能返回重复值，可以使用索引将子查询去重。range：索引范围扫描，常见于使用&gt;,&lt;,between ,in ,like等运算符的查询中。index：索引全表扫描，把索引树从头到尾扫一遍；all：遍历全表以找到匹配的行（Index 与 ALL 虽然都是读全表，但 index 是从索引中读取，而 ALL 是从硬盘读取）NULL: MySQL 在优化过程中分解语句，执行时甚至不用访问表或索引。possible_keys显示查询可能使用到的索引。key显示查询实际使用哪个索引来优化对该表的访问；select_type 为 index_merge 时，这里可能出现两个以上的索引，其他的 select_type 这里只会出现一个。key_len用于处理查询的索引长度，表示索引中使用的字节数。通过这个值，可以得出一个多列索引里实际使用了哪一部分。注：key_len 显示的值为索引字段的最大可能长度，并非实际使用长度，即 key_len 是根据表定义计算而得，不是通过表内检索出的。另外，key_len 只计算 where 条件用到的索引长度，而排序和分组就算用到了索引，也不会计算到 key_len 中。ref显示哪个字段或者常数与 key 一起被使用。如果是使用的常数等值查询，这里会显示 const。如果是连接查询，被驱动表的执行计划这里会显示驱动表的关联字段。如果是条件使用了表达式或者函数，或者条件列发生了内部隐式转换，这里可能显示为 func。rows表示 MySQL 根据表统计信息及索引选用情况，大致估算的找到所需的目标记录所需要读取的行数，不是精确值。extra不适合在其他列中显示但十分重要的额外信息。这个列可以显示的信息非常多，有几十种，常用的有：类型说明Using filesortMySQL 有两种方式可以生成有序的结果，通过排序操作或者使用索引，当 Extra 中出现了 Using filesort 说明 MySQL 使用了后者，但注意虽然叫 filesort 但并不是说明就是用了文件来进行排序，只要可能排序都是在内存里完成的。大部分情况下利用索引排序更快，所以一般这时也要考虑优化查询了。使用文件完成排序操作，这是可能是 ordery by，group by 语句的结果，这可能是一个 CPU 密集型的过程，可以通过选择合适的索引来改进性能，用索引来为查询结果排序。Using temporary用临时表保存中间结果，常用于 GROUP BY 和 ORDER BY 操作中，一般看到它说明查询需要优化了，就算避免不了临时表的使用也要尽量避免硬盘临时表的使用。Not existsMYSQL 优化了 LEFT JOIN，一旦它找到了匹配 LEFT JOIN 标准的行， 就不再搜索了。Using index说明查询是覆盖了索引的，不需要读取数据文件，从索引树（索引文件）中即可获得信息。如果同时出现 using where，表明索引被用来执行索引键值的查找，没有 using where，表明索引用来读取数据而非执行查找动作。这是 MySQL 服务层完成的，但无需再回表查询记录。Using index condition这是 MySQL 5.6 出来的新特性，叫做“索引条件推送”。简单说一点就是 MySQL 原来在索引上是不能执行如 like 这样的操作的，但是现在可以了，这样减少了不必要的 IO 操作，但是只能用在二级索引上。Using where使用了 WHERE 从句来限制哪些行将与下一张表匹配或者是返回给用户。注意：Extra 列出现 Using where 表示 MySQL 服务器将存储引擎返回服务层以后再应用 WHERE 条件过滤。Using join buffer使用了连接缓存：Block Nested Loop，连接算法是块嵌套循环连接;Batched Key Access，连接算法是批量索引连接impossible wherewhere 子句的值总是 false，不能用来获取任何元组select tables optimized away在没有 GROUP BY 子句的情况下，基于索引优化 MIN/MAX 操作，或者对于 MyISAM 存储引擎优化 COUNT(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。distinct优化 distinct 操作，在找到第一匹配的元组后即停止找同样值的动作filtered使用 explain extended 时会出现这个列，5.7 之后的版本默认就有这个字段，不需要使用 explain extended 了。这个字段表示存储引擎返回的数据在 server 层过滤后，剩下多少满足查询的记录数量的比例，注意是百分比，不是具体记录数。关于 MySQL 执行计划的局限性EXPLAIN 不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况；EXPLAIN 不考虑各种 Cache；EXPLAIN 不能显示 MySQL 在执行查询时所作的优化工作；部分统计信息是估算的，并非精确值；EXPALIN 只能解释 SELECT 操作，其他操作要重写为 SELECT 后查看。查询计划案例分析执行顺序（id = 4）：【select id, name from t2】：select_type 为 union，说明 id=4 的 select 是 union 里面的第二个 select。（id = 3）：【select id, name from t1 where address = ‘11’】：因为是在 from 语句中包含的子查询所以被标记为 DERIVED（衍生），where address = ‘11’ 通过复合索引 idx_name_email_address 就能检索到，所以 type 为 index。（id = 2）：【select id from t3】：因为是在 select 中包含的子查询所以被标记为 SUBQUERY。（id = 1）：【select d1.name, … d2 from … d1】：select_type 为 PRIMARY 表示该查询为最外层查询，table 列被标记为 “derived3”表示查询结果来自于一个衍生表（id = 3 的 select 结果）。（id = NULL）：【 … union … 】：代表从 union 的临时表中读取行的阶段，table 列的 “union 1, 4”表示用 id=1 和 id=4 的 select 结果进行 union 操作。本文使用 mdnice 排版]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mysql最全面试题]]></title>
      <url>%2F2020%2F04%2F20%2FMysql%E6%9C%80%E5%85%A8%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
      <content type="text"><![CDATA[前言前几天有读者找到我，说想要一套全面的Mysql面试题，今天陈某特地为她写了一篇。由于篇幅较长，陈某已经将此文章转换为PDF，公众号回复关键词Mysql面试题即可获取。Mysql什么是SQL？结构化查询语言(Structured Query Language)简称SQL，是一种数据库查询语言。作用：用于存取数据、查询、更新和管理关系数据库系统。什么是MySQL?MySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一。在Java企业级开发中非常常用，因为 MySQL 是开源免费的，并且方便扩展。数据库三大范式是什么？第一范式：每个列都不可以再拆分。第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。mysql有关权限的表都有哪几个？MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。下面分别介绍一下这些表的结构和内容：user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。db权限表：记录各个帐号在各个数据库上的操作权限。table_priv权限表：记录数据表级的操作权限。columns_priv权限表：记录数据列级的操作权限。host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。MySQL的binlog有有几种录入格式？分别有什么区别？有三种格式，statement，row和mixed。statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。mysql有哪些数据类型？1、整数类型，包括TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT，分别表示1字节、2字节、3字节、4字节、8字节整数。任何整数类型都可以加上UNSIGNED属性，表示数据是无符号的，即非负整数。长度：整数类型可以被指定长度，例如：INT(11)表示长度为11的INT类型。长度在大多数场景是没有意义的，它不会限制值的合法范围，只会影响显示字符的个数，而且需要和UNSIGNED ZEROFILL属性配合使用才有意义。例子：假定类型设定为INT(5)，属性为UNSIGNED ZEROFILL，如果用户插入的数据为12的话，那么数据库实际存储数据为00012。2、实数类型，包括FLOAT、DOUBLE、DECIMAL。DECIMAL可以用于存储比BIGINT还大的整型，能存储精确的小数。而FLOAT和DOUBLE是有取值范围的，并支持使用标准的浮点进行近似计算。计算时FLOAT和DOUBLE相比DECIMAL效率更高一些，DECIMAL你可以理解成是用字符串进行处理。3、字符串类型，包括VARCHAR、CHAR、TEXT、BLOBVARCHAR用于存储可变长字符串，它比定长类型更节省空间。VARCHAR使用额外1或2个字节存储字符串长度。列长度小于255字节时，使用1字节表示，否则使用2字节表示。VARCHAR存储的内容超出设置的长度时，内容会被截断。CHAR是定长的，根据定义的字符串长度分配足够的空间。CHAR会根据需要使用空格进行填充方便比较。CHAR适合存储很短的字符串，或者所有值都接近同一个长度。CHAR存储的内容超出设置的长度时，内容同样会被截断。4、枚举类型（ENUM），把不重复的数据存储为一个预定义的集合。有时可以使用ENUM代替常用的字符串类型。ENUM存储非常紧凑，会把列表值压缩到一个或两个字节。ENUM在内部存储时，其实存的是整数。尽量避免使用数字作为ENUM枚举的常量，因为容易混乱。排序是按照内部存储的整数5、日期和时间类型，尽量使用timestamp，空间效率高于datetime，用整数保存时间戳通常不方便处理。如果需要存储微妙，可以使用bigint存储。看到这里，这道真题是不是就比较容易回答了。MyISAM索引与InnoDB索引的区别？InnoDB索引是聚簇索引，MyISAM索引是非聚簇索引。InnoDB的主键索引的叶子节点存储着行数据，因此主键索引非常高效。MyISAM索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。InnoDB引擎的4大特性插入缓冲（insert buffer)二次写(double write)自适应哈希索引(ahi)预读(read ahead)什么是索引？索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。索引有哪些优缺点？索引的优点：可以大大加快数据的检索速度，这也是创建索引的最主要的原因。通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。索引的缺点：时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率；空间方面：索引需要占物理空间。索引有哪几种类型？主键索引: 数据列不允许重复，不允许为NULL，一个表只能有一个主键。唯一索引: 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引普通索引: 基本的索引类型，没有唯一性的限制，允许为NULL值。可以通过ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引可以通过ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引。全文索引： 是目前搜索引擎使用的一种关键技术。可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引索引的数据结构（b树，hash）索引的数据结构和具体存储引擎的实现有关，在MySQL中使用较多的索引有Hash索引，B+树索引等，而我们经常使用的InnoDB存储引擎的默认索引实现为：B+树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。1. B树索引mysql通过存储引擎取数据，基本上90%的人用的就是InnoDB了，按照实现方式分，InnoDB的索引类型目前只有两种：BTREE（B树）索引和HASH索引。B树索引是Mysql数据库中使用最频繁的索引类型，基本所有存储引擎都支持BTree索引。通常我们说的索引不出意外指的就是（B树）索引（实际是用B+树实现的，因为在查看表索引时，mysql一律打印BTREE，所以简称为B树索引）2. B+tree性质n棵子tree的节点包含n个关键字，不用来保存数据而是保存数据的索引。所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。B+ 树中，数据对象的插入和删除仅在叶节点上进行。B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。3. 哈希索引简要说下，类似于数据结构中简单实现的HASH表（散列表）一样，当我们在mysql中用哈希索引时，主要就是通过Hash算法（常见的Hash算法有直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的Hash值，与这条数据的行指针一并存入Hash表的对应位置；如果发生Hash碰撞（两个不同关键字的Hash值相同），则在对应Hash键下以链表形式存储。当然这只是简略模拟图。索引的基本原理索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。索引的原理很简单，就是把无序的数据变成有序的查询把创建了索引的列的内容进行排序对排序结果生成倒排表在倒排表内容上拼上数据地址链在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据索引算法有哪些？索引算法有 BTree算法和Hash算法1. BTree算法BTree是最常用的mysql数据库索引算法，也是mysql默认的算法。因为它不仅可以被用在=,&gt;,&gt;=,&lt;,&lt;=和between这些比较操作符上，而且还可以用于like操作符，只要它的查询条件是一个不以通配符开头的常量。2. Hash算法Hash Hash索引只能用于对等比较，例如=,&lt;=&gt;（相当于=）操作符。由于是一次定位数据，不像BTree索引需要从根节点到枝节点，最后才能访问到页节点这样多次IO访问，所以检索效率远高于BTree索引。索引设计的原则？适合索引的列是出现在where子句中的列，或者连接子句中指定的列。基数较小的类，索引效果较差，没有必要在此列建立索引使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。创建索引的原则索引虽好，但也不是无限制的使用，最好符合一下几个原则最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。较频繁作为查询条件的字段才去创建索引更新频繁字段不适合创建索引若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低)尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。定义有外键的数据列一定要建立索引。对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。对于定义为text、image和bit的数据类型的列不要建立索引。创建索引时需要注意什么？非空字段：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值；取值离散大的字段：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高；索引字段越小越好：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。使用索引查询一定能提高查询的性能吗？通常，通过索引查询数据比全表扫描要快。但是我们也必须注意到它的代价。索引需要空间来存储，也需要定期维护， 每当有记录在表中增减或索引列被修改时，索引本身也会被修改。 这意味着每条记录的INSERT，DELETE，UPDATE将为此多付出4，5 次的磁盘I/O。 因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢。使用索引查询不一定能提高查询性能，索引范围查询(INDEX RANGE SCAN)适用于两种情况:基于一个范围的检索，一般查询返回结果集小于表中记录数的30%基于非唯一性索引的检索百万级别或以上的数据如何删除？关于索引：由于索引需要额外的维护成本，因为索引文件是单独存在的文件,所以当我们对数据的增加,修改,删除,都会产生额外的对索引文件的操作,这些操作需要消耗额外的IO,会降低增/改/删的执行效率。所以，在我们删除数据库百万级别数据的时候，查询MySQL官方手册得知删除数据的速度和创建的索引数量是成正比的。所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟）然后删除其中无用数据（此过程需要不到两分钟）删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。什么是最左前缀原则？什么是最左匹配原则？顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式B树和B+树的区别在B树中，你可以将键和值存放在内部节点和叶子节点；但在B+树中，内部节点都是键，没有值，叶子节点同时存放键和值。B+树的叶子节点有一条链相连，而B树的叶子节点各自独立。使用B树的好处B树可以在内部节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。这种特性使得B树在特定数据重复多次查询的场景中更加高效。使用B+树的好处由于B+树的内部节点只存放键，不存放值，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。 B+树的叶节点由一条链相连，因此，当需要进行一次全数据遍历的时候，B+树只需要使用O(logN)时间找到最小的一个节点，然后通过链进行O(N)的顺序遍历即可。而B树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间什么是聚簇索引？何时使用聚簇索引与非聚簇索引？聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因。非聚簇索引一定会回表查询吗？不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行select age from employee where age &lt; 20的查询时，在索引的叶子节点上，已经包含了age信息，不会再次进行回表查询。联合索引是什么？为什么需要注意联合索引中的顺序？MySQL可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。MySQL使用索引时需要索引有序，假设现在建立了”name，age，school”的联合索引，那么索引的排序为: 先按照name排序，如果name相同，则按照age排序，如果age的值也相等，则按照school进行排序。当进行查询时，此时索引仅仅按照name严格有序，因此必须首先使用name字段进行等值查询，之后对于匹配到的列而言，其按照age字段严格有序，此时可以使用age字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。什么是数据库事务？事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。事物的四大特性(ACID)介绍一下?原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的；隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。什么是脏读？幻读？不可重复读？脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。什么是事务的隔离级别？MySQL的默认隔离级别是什么？为了达到事务的四大特性，数据库定义了4种不同的事务隔离级别，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。SQL 标准定义了四个隔离级别：READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别隔离级别与锁的关系在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁；在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。按照锁的粒度分数据库锁有哪些？行级锁:行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。表级锁: 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。页级锁:页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。从锁的类别上分MySQL都有哪些锁呢？从锁的类别上来讲，有共享锁和排他锁。共享锁: 又叫做读锁。 当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。排他锁: 又叫做写锁。 当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，他和其他的排他锁，共享锁都相斥。InnoDB存储引擎的锁的算法有哪三种？Record lock：单个行记录上的锁Gap lock：间隙锁，锁定一个范围，不包括记录本身Next-key lock：record+gap 锁定一个范围，包含记录本身什么是死锁？怎么解决？死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。常见的解决死锁的方法如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率；如果业务处理不好可以用分布式事务锁或者使用乐观锁数据库的乐观锁和悲观锁是什么？怎么实现的？数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：乐一般会使用版本号机制或CAS算法实现。大表数据查询，怎么优化？优化shema、sql语句+索引；第二加缓存，memcached, redis；主从复制，读写分离；垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表超大分页怎么处理？超大的分页一般从两个方向上来解决:数据库层面,这也是我们主要集中关注的(虽然收效没那么大),类似于select from table where age &gt; 20 limit 1000000,10这种查询其实也是有可以优化的余地的. 这条语句需要load1000000数据然后基本上全部丢弃,只取10条当然比较慢. 当时我们可以修改为select from table where id in (select id from table where age &gt; 20 limit 1000000,10).这样虽然也load了一百万的数据,但是由于索引覆盖,要查询的所有字段都在索引中,所以速度会很快. 同时如果ID连续的好,我们还可以select * from table where id &gt; 1000000 limit 10,效率也是不错的,优化的可能性有许多种,但是核心思想都一样,就是减少load的数据从需求的角度减少这种请求…主要是不做类似的需求(直接跳转到几百万页之后的具体某一页.只允许逐页查看或者按照给定的路线走,这样可预测,可缓存)以及防止ID泄漏且连续被人恶意攻击为什么要尽量设定一个主键？主键是数据库确保数据行在整张表唯一性的保障，即使业务上本张表没有主键，也建议添加一个自增长的ID列作为主键。设定了主键之后，在后续的删改查的时候可能更加快速以及确保操作数据范围安全。主键使用自增ID还是UUID？推荐使用自增ID，不要使用UUID。因为在InnoDB存储引擎中，主键索引是作为聚簇索引存在的，也就是说，主键索引的B+树叶子节点上存储了主键索引以及全部的数据(按照顺序)，如果主键索引是自增ID，那么只需要不断向后排列即可，如果是UUID，由于到来的ID与原来的大小不确定，会造成非常多的数据插入，数据移动，然后导致产生很多的内存碎片，进而造成插入性能的下降。总之，在数据量大一些的情况下，用自增主键性能会好一些。关于主键是聚簇索引，如果没有主键，InnoDB会选择一个唯一键来作为聚簇索引，如果没有唯一键，会生成一个隐式的主键。字段为什么要求定义为not null？null值会占用更多的字节，且会在程序中造成很多与预期不符的情况。如果要存储用户的密码散列，应该使用什么字段进行存储？密码散列，盐，用户身份证号等固定长度的字符串应该使用char而不是varchar来存储，这样可以节省空间且提高检索效率。数据库结构优化？一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。将字段很多的表分解成多个表：对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。增加中间表：对于需要经常联合查询的表，可以建立中间表以提高查询效率。通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。增加冗余字段：设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。MySQL数据库cpu飙升到500%的话他怎么处理？当 cpu 飙升到 500%时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。如果是 mysqld 造成的， show processlist，看看里面跑的 session 情况，是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等。主从复制的作用？主数据库出现问题，可以切换到从数据库。可以进行数据库层面的读写分离。可以在从数据库上进行日常备份。MySQL主从复制解决的问题？数据分布：随意开始或停止复制，并在不同地理位置分布数据备份负载均衡：降低单个服务器的压力高可用和故障切换：帮助应用程序避免单点失败升级测试：可以用更高版本的MySQL作为从库MySQL主从复制工作原理？在主库上把数据更高记录到二进制日志从库将主库的日志复制到自己的中继日志从库读取中继日志的事件，将其重放到从库数据中。小福利由于文章篇幅较长，陈某将其转换为PDF文档，老规矩，回复关键词Mysql面试题即可获取。巨人的肩膀https://www.cnblogs.com/hsmwlyl/p/10719152.htmlhttps://www.cnblogs.com/caomusheng/p/12586895.htmlhttps://article.itxueyuan.com/eoJEMjhttps://blog.csdn.net/thinkwon/article/details/104778621#comments本文使用 mdnice 排版]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[order by 如何工作]]></title>
      <url>%2F2020%2F04%2F20%2Forder%20by%20%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%2F</url>
      <content type="text"><![CDATA[前言在实际的开发中一定会碰到根据某个字段进行排序后来显示结果的需求，但是你真的理解order by在 Mysql 底层是如何执行的吗？假设你要查询城市是苏州的所有人名字，并且按照姓名进行排序返回前 1000 个人的姓名、年龄，这条 sql 语句应该如何写？首先创建一张用户表，sql 语句如下：CREATE TABLE user ( id int(11) NOT NULL, city varchar(16) NOT NULL, name varchar(16) NOT NULL, age int(11) NOT NULL, PRIMARY KEY (id), KEY city (city)) ENGINE=InnoDB;则上述需求的 sql 查询语句如下：select city,name,age from user where city=‘苏州’ order by name limit 1000;这条 sql 查询语句相信大家都能写出来，但是你了解它在 Mysql 底层的执行流程吗？今天陈某来大家聊一聊这条 sql 语句是如何执行的以及有什么参数会影响执行的流程。本篇文章分为如下几个部分进行详细的阐述：全字段排序rowid 排序全字段排序 VS rowid 排序如何避免排序全字段排序前面聊过索引能够避免全表扫描，因此我们给city这个字段上添加了索引，当然城市的字段很小，不用考虑字符串的索引问题，之前有写过一篇关于如何给字符串的加索引的文章，有不了解朋友看一下这篇文章:Mysql 性能优化：如何给字符串加索引？此时用Explain来分析一下的这条查询语句的执行情况，结果如下图：Extra这个字段中的Using filesort表示的就是需要排序，MySQL 会给每个线程分配一块内存用于排序，称为sort_buffer。既然使用了索引进行查询，我们来简单的画一下city这棵索引树的结构，如下图：从上图可以看出，满足city=’苏州’是从ID3到IDX这些记录。通常情况下，此条 sql 语句执行流程如下：初始化 sort_buffer，确定放入 name、city、age 这三个字段。从索引 city 找到第一个满足city=’苏州’条件的主键id，也就是图中的ID3。到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中。从索引city取下一个记录的主键 id。重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的IDX。对sort_buffer中的数据按照字段name做快速排序。按照排序结果取前 1000 行返回给客户端。我们称这个排序过程为全字段排序，执行的流程图如下：图中按name排序这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数sort_buffer_size。sort_buffer_size：就是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。rowid 排序在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在sort_buffer和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么sort_buffer里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。所以如果单行很大，这个方法效率不够好。我们可以修改一个max_length_for_sort_data这个参数使其使用另外一种算法。max_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。city、name、age 这三个字段的定义总长度是36，我把max_length_for_sort_data设置为 16，我们再来看看计算过程有什么改变。设置的 sql 语句如下：SET max_length_for_sort_data = 16;新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。但这时，排序的结果就因为少了 city 和 age 字段的值，不能直接返回了，整个执行流程就变成如下所示的样子：初始化sort_buffer，确定放入两个字段，即name和id。从索引 city 找到第一个满足city=’苏州’条件的主键id，也就是图中的ID3。到主键id索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中。从索引city取下一个记录的主键 id。重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的IDX。对sort_buffer中的数据按照字段name做快速排序。遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。这个执行流程的示意图如下，我把它称为rowid排序。对比全字段排序，rowid排序多了一次回表查询，即是多了第7步的查询主键索引树。全字段排序 VS rowid 排序如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。这也就体现了 MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。如何避免排序其实，并不是所有的order by语句，都需要排序操作的。从上面分析的执行过程，我们可以看到，MySQL 之所以需要生成临时表，并且在临时表上做排序操作，其原因是原来的数据都是无序的。如果能够保证从city这个索引上取出来的行，天然就是按照 name 递增排序的话，是不是就可以不用再排序了呢？因此想到了联合索引，创建(city,name)联合索引，sql 语句如下：alter table user add index city_user(city, name);此时的索引树如下：在这个索引里面，我们依然可以用树搜索的方式定位到第一个满足city=’苏州’的记录，并且额外确保了，接下来按顺序取“下一条记录”的遍历过程中，只要 city 的值是苏州，name 的值就一定是有序的。按照上图，整个查询的流程如下：从索引(city,name)找到第一个满足 city=’苏州’条件的主键 id。到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回。从索引(city,name)取下一个记录主键 id。重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city=’苏州’条件时循环结束。对应的流程图如下：可以看到，这个查询过程不需要临时表，也不需要排序。接下来，我们用 explain 的结果来印证一下。从图中可以看到，Extra字段中没有Using filesort了，也就是不需要排序了。而且由于(city,name)这个联合索引本身有序，所以这个查询也不用把 4000 行全都读一遍，只要找到满足条件的前 1000 条记录就可以退出了。也就是说，在我们这个例子里，只需要扫描 1000 次。难道仅仅这样就能满足了？此条查询语句是否能再优化呢？朋友们还记得覆盖索引吗？覆盖索引的好处就是能够避免再次回表查询，不了解的朋友们可以看一下陈某之前写的文章：Mysql 性能优化：如何使用覆盖索引？。我们创建(city,name,age)联合索引，这样在执行上面的查询语句就能使用覆盖索引了，避免了回表查询了，sql 语句如下：alter table user add index city_user_age(city, name, age);此时执行流程图如下：当然，覆盖索引能够提升效率，但是维护索引也是需要代价的，因此还需要权衡使用。总结今天这篇文章，我和你介绍了 MySQL 里面order by语句的几种算法流程。在开发系统的时候，你总是不可避免地会使用到 order by 语句。心里要清楚每个语句的排序逻辑是怎么实现的，还要能够分析出在最坏情况下，每个语句的执行对系统资源的消耗，这样才能做到下笔如有神，不犯低级错误。本文使用 mdnice 排版]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[一文搞懂Redis持久化]]></title>
      <url>%2F2020%2F04%2F20%2F%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82Redis%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
      <content type="text"><![CDATA[前言 Redis目前已经成为主流的内存数据库了，但是大部分人仅仅是停留在会用的阶段，你真的了解Redis内部的工作原理吗？ 今天这篇文章将为大家介绍Redis持久化的两种方案，文章将会从以下五个方面介绍： 什么是RDB，RDB如何实现持久化？ 什么是AOF，AOF如何实现持久化？ AOF和RDB的区别。 如何重启恢复数据？ 持久化性能问题和解决方案RDB RDB持久化是把当前进程数据生成快照保存到硬盘的过程， 触发RDB持久化过程分为手动触发和自动触发。 RDB完成后会自动生成一个文件，保存在dir配置的指定目录下，文件名是dbfileName指定。 Redis默认会采用LZF算法对生成的RDB文件做压缩处理，压缩后的文件远远小于内存大小，默认开启。 手动触发 手动触发的命令有save和bgsave。 save：该命令会阻塞Redis服务器，直到RDB的过程完成，已经被废弃，因此线上不建议使用。 bgsave：每次进行RDB过程都会fork一个子进程，由子进程完成RDB的操作，因此阻塞只会发生在fork阶段，一般时间很短。 自动触发 除了手动触发RDB，Redis服务器内部还有如下几个场景能够自动触发RDB： 根据我们的 save m n 配置规则自动触发。 如果从节点执行全量复制操作， 主节点自动执行bgsave生成RDB文件并发送给从节点。 执行debug reload命令重新加载Redis时， 也会自动触发save操作。 默认情况下执行shutdown命令时， 如果没有开启AOF持久化功能则自动执行bgsave。 RDB执行流程 RDB的主流方式就是bgsave，通过下图我们来看看RDB的执行流程： 通过上图可以很清楚RDB的执行流程，如下： 执行bgsave命令后，会先判断是否存在AOF或者RDB的子进程，如果存在，直接返回。 父进程fork操作创建一个子进程，fork操作中父进程会被阻塞。 fork完成后，子进程开始根据父进程的内存生成临时快照文件，完成后对原有的RDB文件进行替换。执行lastsave命令可以查看最近一次的RDB时间。 子进程完成后发送信号给父进程，父进程更新统计信息。 RDB的优点 RDB是一个紧凑压缩的二进制文件， 代表Redis在某个时间点上的数据快照。 非常适用于备份， 全量复制等场景。 比如每6小时执行bgsave备份，并把RDB文件拷贝到远程机器或者文件系统中，用于灾难恢复。 Redis加载RDB恢复数据远远快于AOF的方式。 RDB的缺点 RDB方式数据没办法做到实时持久化/秒级持久化。 因为bgsave每次运行都要执行fork操作创建子进程，属于重量级操作，频繁执行成本过高。 RDB文件使用特定二进制格式保存， Redis版本演进过程中有多个格式的RDB版本， 存在老版本Redis服务无法兼容新版RDB格式的问题。 AOF AOF（append only file） 持久化： 以独立日志的方式记录每次写命令，重启时再重新执行AOF文件中的命令达到恢复数据的目的。 AOF的主要作用是解决了数据持久化的实时性， 目前已经是Redis持久化的主流方式。 如何开启AOF 开启AOF功能需要设置配置：appendonly yes， 默认不开启。 AOF文件名通过appendfilename配置设置， 默认文件名是appendonly.aof。 保存路径同RDB持久化方式一致，通过dir配置指定。 AOF整体的执行流程 AOF执行的流程大致分为命令写入、文件同步、文件重写、重启加载四个步骤，如下图： 从上图大致了解了AOF的执行流程，下面一一分析上述的四个步骤。 命令写入 AOF命令写入的内容直接是文本协议格式。 例如set hello world这条命令， 在AOF缓冲区会追加如下文本： 1*3\r\n$3\r\nset\r\n$5\r\nhello\r\n$5\r\nworld\r\n 命令写入是直接写入到AOF的缓冲区中，至于为什么？原因很简单，Redis使用单线程响应命令，如果每次写AOF文件命令都直接追加到硬盘， 那么性能完全取决于当前硬盘负载。先写入缓冲区aof_buf中， 还有另一个好处， Redis可以提供多种缓冲区同步硬盘的策略，在性能和安全性方面做出平衡。 文件同步 Redis提供了多种AOF缓冲区同步文件策略， 由参数appendfsync控制，如下： 配置为always时， 每次写入都要同步AOF文件， 在一般的SATA硬盘上，Redis只能支持大约几百TPS写入， 显然跟Redis高性能特性背道而驰，不建议配置。 配置为no，由于操作系统每次同步AOF文件的周期不可控，而且会加大每次同步硬盘的数据量，虽然提升了性能，但数据安全性无法保证。 配置为everysec（默认的配置），是建议的同步策略， 也是默认配置，做到兼顾性能和数据安全性。理论上只有在系统突然宕机的情况下丢失1秒的数据（当然，这是不太准确的）。 文件重写机制 随着命令不断写入AOF， 文件会越来越大， 为了解决这个问题， Redis引入AOF重写机制压缩文件体积。 AOF文件重写是把Redis进程内的数据转化为写命令同步到新AOF文件的过程。 为什么要文件重写呢？ 因为文件重写能够使得AOF文件的体积变得更小，从而使得可以更快的被Redis加载。 重写过程分为手动触发和自动触发。 手动触发直接使用bgrewriteaof命令。 根据auto-aof-rewrite-min-size和auto-aof-rewrite-percentage参数确定自动触发时机。 auto-aof-rewrite-min-size：表示运行AOF重写时文件最小体积， 默认为64MB。 auto-aof-rewrite-percentage：代表当前AOF文件空间（aof_current_size） 和上一次重写后AOF文件空间（aof_base_size） 的比值。 自动触发时机相当于aof_current_size&gt;auto-aof-rewrite-minsize&amp;&amp;（aof_current_size-aof_base_size） /aof_base_size&gt;=auto-aof-rewritepercentage。其中aof_current_size和aof_base_size可以在info Persistence统计信息中查看。 那么文件重写后的AOF文件为什么会变小呢？ 有如下几个原因： 进程内已经超时的数据将不会再次写入AOF文件中。 旧的AOF文件含有无效命令，如del key1、 hdel key2等。重写使用进程内数据直接生成，这样新的AOF文件只保留最终数据的写入命令。 多条写命令可以合并为一个， 如：lpush list a、 lpush list b、lpush listc可以转化为：lpush list a b c。为了防止单条命令过大造成客户端缓冲区溢出，对于list、 set、 hash、 zset等类型操作，以64个元素为界拆分为多条。 介绍了文件重写的系列知识，下面来看看Redis内部是如何进行文件重写的，如下图： 看完上图，大致了解了文件重写的流程，对于重写的流程，补充如下： 重写期间，主线程并没有阻塞，而是在执行其他的操作命令，依然会向旧的AOF文件写入数据，这样能够保证备份的最终完整性，如果数据重写失败，也能保证数据不会丢失。 为了把重写期间响应的写入信息也写入到新的文件中，因此也会为子进程保留一个缓冲区，防止新写的文件丢失数据。 重写是直接把当前内存的数据生成对应命令，并不需要读取老的AOF文件进行分析、命令合并。 AOF文件直接采用的文本协议，主要是兼容性好、追加方便、可读性高可认为修改修复。 无论是RDB还是AOF都是先写入一个临时文件，然后通过重命名完成文件的替换。 AOF的优点 使用 AOF 持久化会让 Redis 变得非常耐久：你可以设置不同的 fsync 策略，比如无 fsync ，每秒钟一次 fsync ，或者每次执行写入命令时 fsync 。 AOF 的默认策略为每秒钟 fsync 一次，在这种配置下，Redis 仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（ fsync 会在后台线程执行，所以主线程可以继续努力地处理命令请求）。 AOF的缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间。 数据恢复速度相对于RDB比较慢。 AOF和RDB的区别 RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘，实际操作过程是fork一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。 AOF持久化以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式记录，可以打开文件看到详细的操作记录。 重启加载 无论是RDB还是AOF都可用于服务器重启时的数据恢复，执行流程如下图： 上图很清晰的分析了Redis启动恢复数据的流程，先检查AOF文件是否开启，文件是否存在，再检查RDB是否开启，文件是否存在。 性能问题与解决方案 通过上面的分析，我们都知道RDB的快照、AOF的重写都需要fork，这是一个重量级操作，会对Redis造成阻塞。因此为了不影响Redis主进程响应，我们需要尽可能降低阻塞。 那么如何减少fork操作的阻塞呢？ 优先使用物理机或者高效支持fork操作的虚拟化技术。 控制Redis实例最大可用内存， fork耗时跟内存量成正比， 线上建议每个Redis实例内存控制在10GB以内。 合理配置Linux内存分配策略，避免物理内存不足导致fork失败。 降低fork操作的频率，如适度放宽AOF自动触发时机，避免不必要的全量复制等。 总结 本文介绍了Redis持久化的两种不同的策略，大部分内容是运维人员需要掌握的，当然作为后端人员也是需要了解一下，毕竟小公司都是一人搞全栈，哈哈。 如果觉得陈某写的不错，有所收获的话，关注分享一波，你的关注将是陈某写作的最大动力，谢谢支持！！！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mysql最全面试指南]]></title>
      <url>%2F2020%2F04%2F20%2FMysql%E6%9C%80%E5%85%A8%E9%9D%A2%E8%AF%95%E6%8C%87%E5%8D%97%2F</url>
      <content type="text"><![CDATA[前言 前几天有读者找到我，说想要一套全面的Mysql面试题，今天陈某特地为她写了一篇。 由于篇幅较长，陈某已经将此文章转换为PDF，公众号回复关键词Mysql面试题即可获取。 Mysql什么是SQL？ 结构化查询语言(Structured Query Language)简称SQL，是一种数据库查询语言。 作用：用于存取数据、查询、更新和管理关系数据库系统。 什么是MySQL? MySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一。在Java企业级开发中非常常用，因为 MySQL 是开源免费的，并且方便扩展。 数据库三大范式是什么？ 第一范式：每个列都不可以再拆分。 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。 在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。 mysql有关权限的表都有哪几个？ MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。下面分别介绍一下这些表的结构和内容： user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。 db权限表：记录各个帐号在各个数据库上的操作权限。 table_priv权限表：记录数据表级的操作权限。 columns_priv权限表：记录数据列级的操作权限。 host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。 MySQL的binlog有有几种录入格式？分别有什么区别？ 有三种格式，statement，row和mixed。 statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。 row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。 mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。 此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。 mysql有哪些数据类型？ 1、整数类型，包括TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT，分别表示1字节、2字节、3字节、4字节、8字节整数。任何整数类型都可以加上UNSIGNED属性，表示数据是无符号的，即非负整数。 长度：整数类型可以被指定长度，例如：INT(11)表示长度为11的INT类型。长度在大多数场景是没有意义的，它不会限制值的合法范围，只会影响显示字符的个数，而且需要和UNSIGNED ZEROFILL属性配合使用才有意义。 例子：假定类型设定为INT(5)，属性为UNSIGNED ZEROFILL，如果用户插入的数据为12的话，那么数据库实际存储数据为00012。 2、实数类型，包括FLOAT、DOUBLE、DECIMAL。DECIMAL可以用于存储比BIGINT还大的整型，能存储精确的小数。而FLOAT和DOUBLE是有取值范围的，并支持使用标准的浮点进行近似计算。计算时FLOAT和DOUBLE相比DECIMAL效率更高一些，DECIMAL你可以理解成是用字符串进行处理。 3、字符串类型，包括VARCHAR、CHAR、TEXT、BLOB VARCHAR用于存储可变长字符串，它比定长类型更节省空间。 VARCHAR使用额外1或2个字节存储字符串长度。列长度小于255字节时，使用1字节表示，否则使用2字节表示。 VARCHAR存储的内容超出设置的长度时，内容会被截断。 CHAR是定长的，根据定义的字符串长度分配足够的空间。 CHAR会根据需要使用空格进行填充方便比较。 CHAR适合存储很短的字符串，或者所有值都接近同一个长度。 CHAR存储的内容超出设置的长度时，内容同样会被截断。 4、枚举类型（ENUM），把不重复的数据存储为一个预定义的集合。 有时可以使用ENUM代替常用的字符串类型。 ENUM存储非常紧凑，会把列表值压缩到一个或两个字节。 ENUM在内部存储时，其实存的是整数。 尽量避免使用数字作为ENUM枚举的常量，因为容易混乱。 排序是按照内部存储的整数 5、日期和时间类型，尽量使用timestamp，空间效率高于datetime， 用整数保存时间戳通常不方便处理。 如果需要存储微妙，可以使用bigint存储。 看到这里，这道真题是不是就比较容易回答了。 MyISAM索引与InnoDB索引的区别？ InnoDB索引是聚簇索引，MyISAM索引是非聚簇索引。 InnoDB的主键索引的叶子节点存储着行数据，因此主键索引非常高效。 MyISAM索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。 InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。 InnoDB引擎的4大特性 插入缓冲（insert buffer) 二次写(double write) 自适应哈希索引(ahi) 预读(read ahead) 什么是索引？ 索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。 索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。 更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。 索引有哪些优缺点？ 索引的优点： 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 索引的缺点： 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率； 空间方面：索引需要占物理空间。 索引有哪几种类型？ 主键索引: 数据列不允许重复，不允许为NULL，一个表只能有一个主键。 唯一索引: 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。 可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引 可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引 普通索引: 基本的索引类型，没有唯一性的限制，允许为NULL值。 可以通过ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引 可以通过ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引。 全文索引： 是目前搜索引擎使用的一种关键技术。 可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引 索引的数据结构（b树，hash） 索引的数据结构和具体存储引擎的实现有关，在MySQL中使用较多的索引有Hash索引，B+树索引等，而我们经常使用的InnoDB存储引擎的默认索引实现为：B+树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引。 1. B树索引 mysql通过存储引擎取数据，基本上90%的人用的就是InnoDB了，按照实现方式分，InnoDB的索引类型目前只有两种：BTREE（B树）索引和HASH索引。B树索引是Mysql数据库中使用最频繁的索引类型，基本所有存储引擎都支持BTree索引。通常我们说的索引不出意外指的就是（B树）索引（实际是用B+树实现的，因为在查看表索引时，mysql一律打印BTREE，所以简称为B树索引） 2. B+tree性质 n棵子tree的节点包含n个关键字，不用来保存数据而是保存数据的索引。 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 B+ 树中，数据对象的插入和删除仅在叶节点上进行。 B+树有2个头指针，一个是树的根节点，一个是最小关键码的叶节点。 3. 哈希索引 简要说下，类似于数据结构中简单实现的HASH表（散列表）一样，当我们在mysql中用哈希索引时，主要就是通过Hash算法（常见的Hash算法有直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的Hash值，与这条数据的行指针一并存入Hash表的对应位置；如果发生Hash碰撞（两个不同关键字的Hash值相同），则在对应Hash键下以链表形式存储。当然这只是简略模拟图。 索引的基本原理 索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。 索引的原理很简单，就是把无序的数据变成有序的查询 把创建了索引的列的内容进行排序 对排序结果生成倒排表 在倒排表内容上拼上数据地址链 在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据 索引算法有哪些？ 索引算法有 BTree算法和Hash算法 1. BTree算法 BTree是最常用的mysql数据库索引算法，也是mysql默认的算法。因为它不仅可以被用在=,&gt;,&gt;=,&lt;,&lt;=和between这些比较操作符上，而且还可以用于like操作符，只要它的查询条件是一个不以通配符开头的常量。 2. Hash算法 Hash Hash索引只能用于对等比较，例如=,&lt;=&gt;（相当于=）操作符。由于是一次定位数据，不像BTree索引需要从根节点到枝节点，最后才能访问到页节点这样多次IO访问，所以检索效率远高于BTree索引。 索引设计的原则？ 适合索引的列是出现在where子句中的列，或者连接子句中指定的列。 基数较小的类，索引效果较差，没有必要在此列建立索引 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。 创建索引的原则 索引虽好，但也不是无限制的使用，最好符合一下几个原则 最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 较频繁作为查询条件的字段才去创建索引 更新频繁字段不适合创建索引 若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低) 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 定义有外键的数据列一定要建立索引。 对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。 对于定义为text、image和bit的数据类型的列不要建立索引。 创建索引时需要注意什么？ 非空字段：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值； 取值离散大的字段：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高； 索引字段越小越好：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。 使用索引查询一定能提高查询的性能吗？ 通常，通过索引查询数据比全表扫描要快。但是我们也必须注意到它的代价。 索引需要空间来存储，也需要定期维护， 每当有记录在表中增减或索引列被修改时，索引本身也会被修改。 这意味着每条记录的INSERT，DELETE，UPDATE将为此多付出4，5 次的磁盘I/O。 因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢。使用索引查询不一定能提高查询性能，索引范围查询(INDEX RANGE SCAN)适用于两种情况: 基于一个范围的检索，一般查询返回结果集小于表中记录数的30% 基于非唯一性索引的检索 百万级别或以上的数据如何删除？ 关于索引：由于索引需要额外的维护成本，因为索引文件是单独存在的文件,所以当我们对数据的增加,修改,删除,都会产生额外的对索引文件的操作,这些操作需要消耗额外的IO,会降低增/改/删的执行效率。所以，在我们删除数据库百万级别数据的时候，查询MySQL官方手册得知删除数据的速度和创建的索引数量是成正比的。 所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟） 然后删除其中无用数据（此过程需要不到两分钟） 删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。 与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。 什么是最左前缀原则？什么是最左匹配原则？ 顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 B树和B+树的区别 在B树中，你可以将键和值存放在内部节点和叶子节点；但在B+树中，内部节点都是键，没有值，叶子节点同时存放键和值。 B+树的叶子节点有一条链相连，而B树的叶子节点各自独立。 使用B树的好处 B树可以在内部节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。这种特性使得B树在特定数据重复多次查询的场景中更加高效。 使用B+树的好处 由于B+树的内部节点只存放键，不存放值，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。 B+树的叶节点由一条链相连，因此，当需要进行一次全数据遍历的时候，B+树只需要使用O(logN)时间找到最小的一个节点，然后通过链进行O(N)的顺序遍历即可。而B树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间 什么是聚簇索引？何时使用聚簇索引与非聚簇索引？ 聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据 非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因。 非聚簇索引一定会回表查询吗？ 不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。 举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行select age from employee where age &lt; 20的查询时，在索引的叶子节点上，已经包含了age信息，不会再次进行回表查询。 联合索引是什么？为什么需要注意联合索引中的顺序？ MySQL可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。 MySQL使用索引时需要索引有序，假设现在建立了”name，age，school”的联合索引，那么索引的排序为: 先按照name排序，如果name相同，则按照age排序，如果age的值也相等，则按照school进行排序。 当进行查询时，此时索引仅仅按照name严格有序，因此必须首先使用name字段进行等值查询，之后对于匹配到的列而言，其按照age字段严格有序，此时可以使用age字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。 什么是数据库事务？ 事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。 事物的四大特性(ACID)介绍一下? 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； 隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 什么是脏读？幻读？不可重复读？ 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。 不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。 幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。 什么是事务的隔离级别？MySQL的默认隔离级别是什么？ 为了达到事务的四大特性，数据库定义了4种不同的事务隔离级别，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。 SQL 标准定义了四个隔离级别： READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别 隔离级别与锁的关系 在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突 在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁； 在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。 SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。 按照锁的粒度分数据库锁有哪些？ 行级锁:行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 表级锁: 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 页级锁:页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。 从锁的类别上分MySQL都有哪些锁呢？ 从锁的类别上来讲，有共享锁和排他锁。 共享锁: 又叫做读锁。 当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。 排他锁: 又叫做写锁。 当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，他和其他的排他锁，共享锁都相斥。 InnoDB存储引擎的锁的算法有哪三种？ Record lock：单个行记录上的锁 Gap lock：间隙锁，锁定一个范围，不包括记录本身 Next-key lock：record+gap 锁定一个范围，包含记录本身 什么是死锁？怎么解决？ 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。 常见的解决死锁的方法 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率； 如果业务处理不好可以用分布式事务锁或者使用乐观锁 数据库的乐观锁和悲观锁是什么？怎么实现的？ 数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：乐一般会使用版本号机制或CAS算法实现。 大表数据查询，怎么优化？ 优化shema、sql语句+索引； 第二加缓存，memcached, redis； 主从复制，读写分离； 垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统 水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表 超大分页怎么处理？ 超大的分页一般从两个方向上来解决: 数据库层面,这也是我们主要集中关注的(虽然收效没那么大),类似于select from table where age &gt; 20 limit 1000000,10这种查询其实也是有可以优化的余地的. 这条语句需要load1000000数据然后基本上全部丢弃,只取10条当然比较慢. 当时我们可以修改为select from table where id in (select id from table where age &gt; 20 limit 1000000,10).这样虽然也load了一百万的数据,但是由于索引覆盖,要查询的所有字段都在索引中,所以速度会很快. 同时如果ID连续的好,我们还可以select * from table where id &gt; 1000000 limit 10,效率也是不错的,优化的可能性有许多种,但是核心思想都一样,就是减少load的数据 从需求的角度减少这种请求…主要是不做类似的需求(直接跳转到几百万页之后的具体某一页.只允许逐页查看或者按照给定的路线走,这样可预测,可缓存)以及防止ID泄漏且连续被人恶意攻击 为什么要尽量设定一个主键？ 主键是数据库确保数据行在整张表唯一性的保障，即使业务上本张表没有主键，也建议添加一个自增长的ID列作为主键。设定了主键之后，在后续的删改查的时候可能更加快速以及确保操作数据范围安全。 主键使用自增ID还是UUID？ 推荐使用自增ID，不要使用UUID。 因为在InnoDB存储引擎中，主键索引是作为聚簇索引存在的，也就是说，主键索引的B+树叶子节点上存储了主键索引以及全部的数据(按照顺序)，如果主键索引是自增ID，那么只需要不断向后排列即可，如果是UUID，由于到来的ID与原来的大小不确定，会造成非常多的数据插入，数据移动，然后导致产生很多的内存碎片，进而造成插入性能的下降。 总之，在数据量大一些的情况下，用自增主键性能会好一些。 关于主键是聚簇索引，如果没有主键，InnoDB会选择一个唯一键来作为聚簇索引，如果没有唯一键，会生成一个隐式的主键。 字段为什么要求定义为not null？ null值会占用更多的字节，且会在程序中造成很多与预期不符的情况。 如果要存储用户的密码散列，应该使用什么字段进行存储？ 密码散列，盐，用户身份证号等固定长度的字符串应该使用char而不是varchar来存储，这样可以节省空间且提高检索效率。 数据库结构优化？ 一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。 需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。 将字段很多的表分解成多个表：对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 增加中间表：对于需要经常联合查询的表，可以建立中间表以提高查询效率。通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段：设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。 MySQL数据库cpu飙升到500%的话他怎么处理？ 当 cpu 飙升到 500%时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。 如果是 mysqld 造成的， show processlist，看看里面跑的 session 情况，是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。 一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。 也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等。 主从复制的作用？ 主数据库出现问题，可以切换到从数据库。 可以进行数据库层面的读写分离。 可以在从数据库上进行日常备份。 MySQL主从复制解决的问题？ 数据分布：随意开始或停止复制，并在不同地理位置分布数据备份 负载均衡：降低单个服务器的压力 高可用和故障切换：帮助应用程序避免单点失败 升级测试：可以用更高版本的MySQL作为从库 MySQL主从复制工作原理？ 在主库上把数据更高记录到二进制日志 从库将主库的日志复制到自己的中继日志 从库读取中继日志的事件，将其重放到从库数据中。 小福利 由于文章篇幅较长，陈某将其转换为PDF文档，老规矩，回复关键词Mysql面试题即可获取。 巨人的肩膀 https://www.cnblogs.com/hsmwlyl/p/10719152.html https://www.cnblogs.com/caomusheng/p/12586895.html https://article.itxueyuan.com/eoJEMj https://blog.csdn.net/thinkwon/article/details/104778621#comments]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Zookeeper实现分布式锁]]></title>
      <url>%2F2020%2F04%2F19%2FZookeeper%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
      <content type="text"><![CDATA[导读 真是有人(锁)的地方就有江湖(事务)，今天不谈江湖，来撩撩人。 分布式锁的概念、为什么使用分布式锁，想必大家已经很清楚了。前段时间作者写过Redis是如何实现分布式锁，今天这篇文章来谈谈Zookeeper是如何实现分布式锁的。 陈某今天分别从如下几个方面来详细讲讲ZK如何实现分布式锁： ZK的四种节点 排它锁的实现 读写锁的实现 Curator实现分步式锁 ZK的四种节点 持久性节点：节点创建后将会一直存在 临时节点：临时节点的生命周期和当前会话绑定，一旦当前会话断开临时节点也会删除，当然可以主动删除。 持久有序节点：节点创建一直存在，并且zk会自动为节点加上一个自增的后缀作为新的节点名称。 临时有序节点：保留临时节点的特性，并且zk会自动为节点加上一个自增的后缀作为新的节点名称。 排它锁的实现 排他锁的实现相对简单一点，利用了zk的创建节点不能重名的特性。如下图： 根据上图分析大致分为如下步骤： 尝试获取锁：创建临时节点，zk会保证只有一个客户端创建成功。 创建临时节点成功，获取锁成功，执行业务逻辑，业务执行完成后删除锁。 创建临时节点失败，阻塞等待。 监听删除事件，一旦临时节点删除了，表示互斥操作完成了，可以再次尝试获取锁。 递归：获取锁的过程是一个递归的操作，获取锁-&gt;监听-&gt;获取锁。 如何避免死锁：创建的是临时节点，当服务宕机会话关闭后临时节点将会被删除，锁自动释放。 代码实现 作者参照JDK锁的实现方式加上模板方法模式的封装，封装接口如下： 12345678910111213141516/** * @Description ZK分布式锁的接口 * @Author 陈某 * @Date 2020/4/7 22:52 */public interface ZKLock &#123; /** * 获取锁 */ void lock() throws Exception; /** * 解锁 */ void unlock() throws Exception;&#125; 模板抽象类如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * @Description 排他锁，模板类 * @Author 陈某 * @Date 2020/4/7 22:55 */public abstract class AbstractZKLockMutex implements ZKLock &#123; /** * 节点路径 */ protected String lockPath; /** * zk客户端 */ protected CuratorFramework zkClient; private AbstractZKLockMutex()&#123;&#125; public AbstractZKLockMutex(String lockPath,CuratorFramework client)&#123; this.lockPath=lockPath; this.zkClient=client; &#125; /** * 模板方法，搭建的获取锁的框架，具体逻辑交于子类实现 * @throws Exception */ @Override public final void lock() throws Exception &#123; //获取锁成功 if (tryLock())&#123; System.out.println(Thread.currentThread().getName()+"获取锁成功"); &#125;else&#123; //获取锁失败 //阻塞一直等待 waitLock(); //递归，再次获取锁 lock(); &#125; &#125; /** * 尝试获取锁，子类实现 */ protected abstract boolean tryLock() ; /** * 等待获取锁，子类实现 */ protected abstract void waitLock() throws Exception; /** * 解锁：删除节点或者直接断开连接 */ @Override public abstract void unlock() throws Exception;&#125; 排他锁的具体实现类如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/** * @Description 排他锁的实现类，继承模板类 AbstractZKLockMutex * @Author 陈某 * @Date 2020/4/7 23:23 */@Datapublic class ZKLockMutex extends AbstractZKLockMutex &#123; /** * 用于实现线程阻塞 */ private CountDownLatch countDownLatch; public ZKLockMutex(String lockPath,CuratorFramework zkClient)&#123; super(lockPath,zkClient); &#125; /** * 尝试获取锁：直接创建一个临时节点，如果这个节点存在创建失败抛出异常，表示已经互斥了， * 反之创建成功 * @throws Exception */ @Override protected boolean tryLock() &#123; try &#123; zkClient.create() //临时节点 .withMode(CreateMode.EPHEMERAL) //权限列表 world:anyone:crdwa .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) .forPath(lockPath,"lock".getBytes()); return true; &#125;catch (Exception ex)&#123; return false; &#125; &#125; /** * 等待锁，一直阻塞监听 * @return 成功获取锁返回true，反之返回false */ @Override protected void waitLock() throws Exception &#123; //监听节点的新增、更新、删除 final NodeCache nodeCache = new NodeCache(zkClient, lockPath); //启动监听 nodeCache.start(); ListenerContainer&lt;NodeCacheListener&gt; listenable = nodeCache.getListenable(); //监听器 NodeCacheListener listener=()-&gt; &#123; //节点被删除，此时获取锁 if (nodeCache.getCurrentData() == null) &#123; //countDownLatch不为null，表示节点存在，此时监听到节点删除了，因此-1 if (countDownLatch != null) countDownLatch.countDown(); &#125; &#125;; //添加监听器 listenable.addListener(listener); //判断节点是否存在 Stat stat = zkClient.checkExists().forPath(lockPath); //节点存在 if (stat!=null)&#123; countDownLatch=new CountDownLatch(1); //阻塞主线程，监听 countDownLatch.await(); &#125; //移除监听器 listenable.removeListener(listener); &#125; /** * 解锁，直接删除节点 * @throws Exception */ @Override public void unlock() throws Exception &#123; zkClient.delete().forPath(lockPath); &#125;&#125; 可重入性排他锁如何设计 可重入的逻辑很简单，在本地保存一个ConcurrentMap，key是当前线程，value是定义的数据，结构如下： 1private final ConcurrentMap&lt;Thread, LockData&gt; threadData = Maps.newConcurrentMap(); 重入的伪代码如下： 123456public boolean tryLock()&#123; //判断当前线程是否在threadData保存过 //存在，直接return true //不存在执行获取锁的逻辑 //获取成功保存在threadData中&#125; 读写锁的实现 读写锁分为读锁和写锁，区别如下： 读锁允许多个线程同时读数据，但是在读的同时不允许写线程修改。 写锁在获取后，不允许多个线程同时写或者读。 如何实现读写锁？ZK中有一类节点叫临时有序节点，上文有介绍。下面我们来利用临时有序节点来实现读写锁的功能。 读锁的设计 读锁允许多个线程同时进行读，并且在读的同时不允许线程进行写操作，实现原理如下图： 根据上图，获取一个读锁分为以下步骤： 创建临时有序节点（当前线程拥有的读锁或称作读节点）。 获取路径下所有的子节点，并进行从小到大排序 获取当前节点前的临近写节点(写锁)。 如果不存在的临近写节点，则成功获取读锁。 如果存在临近写节点，对其监听删除事件。 一旦监听到删除事件，重复2,3,4,5的步骤(递归)。 写锁的设计 线程一旦获取了写锁，不允许其他线程读和写。实现原理如下： 从上图可以看出唯一和写锁不同的就是监听的节点，这里是监听临近节点(读节点或者写节点)，读锁只需要监听写节点，步骤如下： 创建临时有序节点（当前线程拥有的写锁或称作写节点）。 获取路径下的所有子节点，并进行从小到大排序。 获取当前节点的临近节点(读节点和写节点)。 如果不存在临近节点，则成功获取锁。 如果存在临近节点，对其进行监听删除事件。 一旦监听到删除事件，重复2,3,4,5的步骤(递归)。 如何监听 无论是写锁还是读锁都需要监听前面的节点，不同的是读锁只监听临近的写节点，写锁是监听临近的所有节点，抽象出来看其实是一种链式的监听，如下图： 每一个节点都在监听前面的临近节点，一旦前面一个节点删除了，再从新排序后监听前面的节点，这样递归下去。 代码实现 作者简单的写了读写锁的实现，先造出来再优化，不建议用在生产环境。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140public class ZKLockRW &#123; /** * 节点路径 */ protected String lockPath; /** * zk客户端 */ protected CuratorFramework zkClient; /** * 用于阻塞线程 */ private CountDownLatch countDownLatch=new CountDownLatch(1); private final static String WRITE_NAME="_W_LOCK"; private final static String READ_NAME="_R_LOCK"; public ZKLockRW(String lockPath, CuratorFramework client) &#123; this.lockPath=lockPath; this.zkClient=client; &#125; /** * 获取锁，如果获取失败一直阻塞 * @throws Exception */ public void lock() throws Exception &#123; //创建节点 String node = createNode(); //阻塞等待获取锁 tryLock(node); countDownLatch.await(); &#125; /** * 创建临时有序节点 * @return * @throws Exception */ private String createNode() throws Exception &#123; //创建临时有序节点 return zkClient.create() .withMode(CreateMode.EPHEMERAL_SEQUENTIAL) .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) .forPath(lockPath); &#125; /** * 获取写锁 * @return */ public ZKLockRW writeLock()&#123; return new ZKLockRW(lockPath+WRITE_NAME,zkClient); &#125; /** * 获取读锁 * @return */ public ZKLockRW readLock()&#123; return new ZKLockRW(lockPath+READ_NAME,zkClient); &#125; private void tryLock(String nodePath) throws Exception &#123; //获取所有的子节点 List&lt;String&gt; childPaths = zkClient.getChildren() .forPath("/") .stream().sorted().map(o-&gt;"/"+o).collect(Collectors.toList()); //第一个节点就是当前的锁，直接获取锁。递归结束的条件 if (nodePath.equals(childPaths.get(0)))&#123; countDownLatch.countDown(); return; &#125; //1. 读锁：监听最前面的写锁，写锁释放了，自然能够读了 if (nodePath.contains(READ_NAME))&#123; //查找临近的写锁 String preNode = getNearWriteNode(childPaths, childPaths.indexOf(nodePath)); if (preNode==null)&#123; countDownLatch.countDown(); return; &#125; NodeCache nodeCache=new NodeCache(zkClient,preNode); nodeCache.start(); ListenerContainer&lt;NodeCacheListener&gt; listenable = nodeCache.getListenable(); listenable.addListener(() -&gt; &#123; //节点删除事件 if (nodeCache.getCurrentData()==null)&#123; //继续监听前一个节点 String nearWriteNode = getNearWriteNode(childPaths, childPaths.indexOf(preNode)); if (nearWriteNode==null)&#123; countDownLatch.countDown(); return; &#125; tryLock(nearWriteNode); &#125; &#125;); &#125; //如果是写锁，前面无论是什么锁都不能读，直接循环监听上一个节点即可，直到前面无锁 if (nodePath.contains(WRITE_NAME))&#123; String preNode = childPaths.get(childPaths.indexOf(nodePath) - 1); NodeCache nodeCache=new NodeCache(zkClient,preNode); nodeCache.start(); ListenerContainer&lt;NodeCacheListener&gt; listenable = nodeCache.getListenable(); listenable.addListener(() -&gt; &#123; //节点删除事件 if (nodeCache.getCurrentData()==null)&#123; //继续监听前一个节点 tryLock(childPaths.get(childPaths.indexOf(preNode) - 1&lt;0?0:childPaths.indexOf(preNode) - 1)); &#125; &#125;); &#125; &#125; /** * 查找临近的写节点 * @param childPath 全部的子节点 * @param index 右边界 * @return */ private String getNearWriteNode(List&lt;String&gt; childPath,Integer index)&#123; for (int i = 0; i &lt; index; i++) &#123; String node = childPath.get(i); if (node.contains(WRITE_NAME)) return node; &#125; return null; &#125;&#125; Curator实现分步式锁 Curator是Netflix公司开源的一个Zookeeper客户端，与Zookeeper提供的原生客户端相比，Curator的抽象层次更高，简化了Zookeeper客户端的开发量。 Curator在分布式锁方面已经为我们封装好了，大致实现的思路就是按照作者上述的思路实现的。中小型互联网公司还是建议直接使用框架封装好的，毕竟稳定，有些大型的互联公司都是手写的，牛逼啊。 创建一个排他锁很简单，如下： 123456//arg1：CuratorFramework连接对象，arg2：节点路径lock=new InterProcessMutex(client,path);//获取锁lock.acquire();//释放锁lock.release(); 更多的API请参照官方文档，不是此篇文章重点。 至此ZK实现分布式锁就介绍完了，如有想要源码的朋友，老规矩，回复关键词分布式锁获取。 一点小福利 对于Zookeeper不太熟悉的朋友，陈某特地花费两天时间总结了ZK的常用知识点，包括ZK常用shell命令、ZK权限控制、Curator的基本操作API。目录如下： 需要上面PDF文件的朋友，老规矩，回复关键词ZK总结。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Zookeeper入门]]></title>
      <url>%2F2020%2F04%2F19%2FZookeeper%E5%85%A5%E9%97%A8%2F</url>
      <content type="text"><![CDATA[导读 Zookeeper 相信大家都听说过，最典型的使用就是作为服务注册中心。今天陈某带大家从零基础入门 Zookeeper，看了本文，你将会对 Zookeeper 有了初步的了解和认识。 注意：本文基于 Zookeeper 的版本是 3.4.14，最新版本的在使用上会有一些出入，但是企业现在使用的大部分都是 3.4x 版本的。 Zookeeper 概述 Zookeeper 是一个分布式协调服务的开源框架。主要用来解决分布式集群中应用系统的一致性问题，例如怎样避免同时操作同一数据造成脏读的问题。 ZooKeeper 本质上是一个分布式的小文件存储系统。提供基于类似于文件系 统的目录树方式的数据存储，并且可以对树中的节点进行有效管理。从而用来维护和监控你存储的数据的状态变化。通过监控这些数据状态的变化，从而可以达 到基于数据的集群管理。诸如：统一命名服务、分布式配置管理、分布式消息队列、分布式锁、分布式协调等功能。 Zookeeper 特性 全局数据一致：每个 server 保存一份相同的数据副本，client 无论连 接到哪个 server，展示的数据都是一致的，这是最重要的特征； 可靠性：如果消息被其中一台服务器接受，那么将被所有的服务器接受。 顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上 消息 a 在消息 b 前发布，则在所有 Server 上消息 a 都将在消息 b 前被 发布；偏序是指如果一个消息 b 在消息 a 后被同一个发送者发布，a 必将排在 b 前面。 数据更新原子性：一次数据更新要么成功（半数以上节点成功），要么失 败，不存在中间状态； 实时性：Zookeeper 保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。 Zookeeper 节点类型 Znode 有两种，分别为临时节点和永久节点。 临时节点：该节点的生命周期依赖于创建它们的会话。一旦会话结束，临时节点将被自动删除，当然可以也可以手动删除。临时节点不允许拥有子节点。 永久节点：该节点的生命周期不依赖于会话，并且只有在客户端显示执行删除操作的时候，他们才能被删除。 节点的类型在创建时即被确定，并且不能改变。 Znode 还有一个序列化的特性，如果创建的时候指定的话，该 Znode 的名字后面会自动追加一个不断增加的序列号。序列号对于此节点的父节点来说是唯一的，这样便会记录每个子节点创建的先后顺序。它的格式为&quot;%10d&quot;(10 位数字,没有数值的数位用 0 补充，例如“0000000001”)。 这样便会存在四种类型的 Znode 节点，分类如下： PERSISTENT：永久节点 EPHEMERAL：临时节点 PERSISTENT_SEQUENTIAL：永久节点、序列化 EPHEMERAL_SEQUENTIAL：临时节点、序列化 ZooKeeper Watcher ZooKeeper 提供了分布式数据发布/订阅功能，一个典型的发布/订阅模型系统定义了一种一对多的订阅关系，能让多个订阅者同时监听某一个主题对象，当这个主题对象自身状态变化时，会通知所有订阅者，使他们能够做出相应的处理。 触发事件种类很多，如：节点创建，节点删除，节点改变，子节点改变等。 总的来说可以概括 Watcher 为以下三个过程：客户端向服务端注册 Watcher、服务端事件发生触发 Watcher、客户端回调 Watcher 得到触发事件情况。 Watcher 机制特点 一次性触发 ：事件发生触发监听，一个 watcher event 就会被发送到设置监听的客户端，这种效果是一次性的，后续再次发生同样的事件，不会再次触发。 事件封装 ：ZooKeeper 使用 WatchedEvent 对象来封装服务端事件并传递。WatchedEvent 包含了每一个事件的三个基本属性： 通知状态（keeperState），事件类型（EventType）和节点路径（path）。 event 异步发送 ：watcher 的通知事件从服务端发送到客户端是异步的。 先注册再触发 ：Zookeeper 中的 watch 机制，必须客户端先去服务端注册监听，这样事件发送才会触发监听，通知给客户端。 常用 Shell 命令新增节点1create [-s] [-e] path data -s：表示创建有序节点 -e：表示创建临时节点 创建持久化节点： 1234create /test 1234## 子节点create /test/node1 node1 创建持久化有序节点： 1234567## 完整的节点名称是a0000000001create /a aCreated /a0000000001## 完整的节点名称是b0000000002create /b bCreated /b0000000002 创建临时节点： 1create -e /a a 创建临时有序节点： 123## 完整的节点名称是a0000000001create -e -s /a aCreated /a0000000001 更新节点1set [path] [data] [version] path：节点路径 data：数据 version：版本号 修改节点数据： 1234set /test aaa## 修改子节点set /test/node1 bbb 基于数据版本号修改，如果修改的节点的版本号(dataVersion)不正确，拒绝修改 1set /test aaa 1 删除节点1delete [path] [version] path：节点路径 version：版本号，版本号不正确拒绝删除 删除节点 1234delete /test## 版本号删除delete /test 2 递归删除，删除某个节点及后代 1rmr /test 查看节点数据和状态 命令格式如下： 1get path 获取节点详情： 12345678910111213141516## 获取节点详情get /node1## 节点内容aaacZxid = 0x6ctime = Sun Apr 05 14:50:10 CST 2020mZxid = 0x6mtime = Sun Apr 05 14:50:10 CST 2020pZxid = 0x7cversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 1 节点各个属性对应的含义如下： cZxid：数据节点创建时的事务 ID。 ctime：数据节点创建时间。 mZxid：数据节点最后一次更新时的事务 ID。 mtime：数据节点最后一次更新的时间。 pZxid：数据节点的子节点最后一次被修改时的事务 ID。 cversion：子节点的更改次数。 dataVersion：节点数据的更改次数。 aclVersion ：节点 ACL 的更改次数。 ephemeralOwner：如果节点是临时节点，则表示创建该节点的会话的 SessionID。如果节点是持久化节点，值为 0。 dataLength ：节点数据内容的长度。 numChildren：数据节点当前的子节点的个数。 查看节点状态1stat path stat命令和get命令相似，不过这个命令不会返回节点的数据，只返回节点的状态属性。 1234567891011121314stat /node1## 节点状态信息，没有节点数据cZxid = 0x6ctime = Sun Apr 05 14:50:10 CST 2020mZxid = 0x6mtime = Sun Apr 05 14:50:10 CST 2020pZxid = 0x7cversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 1 查看节点列表 查看节点列表有ls path和ls2 path两个命令。后者是前者的增强，不仅会返回节点列表还会返回当前节点的状态信息。 ls path： 1234ls /## 仅仅返回节点列表[zookeeper, node1] ls2 path： 123456789101112131415ls2 /## 返回节点列表和当前节点的状态信息[zookeeper, node1]cZxid = 0x0ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x6cversion = 2dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 2 监听器 get path watch 使用get path watch注册的监听器在节点内容发生改变时，向客户端发送通知，注意 Zookeeper 的触发器是一次性的，触发一次后会立即生效。 12345678get /node1 watch## 改变节点数据set /node1 bbb## 监听到节点内容改变了WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/node1 监听器 stat path watch stat path watch注册的监听器能够在节点状态发生改变时向客户端发出通知。比如节点数据改变、节点被删除等。 12345678stat /node2 watch## 删除节点node2delete /node2## 监听器监听到了节点删除WATCHER::WatchedEvent state:SyncConnected type:NodeDeleted path:/node2 监听器 ls/ls2 path watch 使用ls path watch或者ls2 path watch注册的监听器，能够监听到该节点下的子节点的增加和删除操作。 12345678ls /node1 watch## 创建子节点create /node1/b b## 监听到了子节点的新增WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/node1 Zookeeper 的 ACL 权限控制 zookeeper 类似文件控制系统，client 可以创建，删除，修改，查看节点，那么如何做到权限控制的呢？zookeeper 的access control list 访问控制列表可以做到这一点。 ACL 权限控制，使用scheme:id:permission来标识。 权限模式(scheme)：授权的策略 授权对象(id)：授权的对象 权限(permission)：授予的权限 权限控制是基于每个节点的，需要对每个节点设置权限。 每个节点支持设置多种权限控制方案和多个权限。 子节点不会继承父节点的权限，客户端无权访问某节点，但可能可以访问它的子节点。 例如：根据 IP 地址进行授权，命令如下： 1setACl /node1 ip:192.168.10.1:crdwa 权限模式 权限模式即是采用何种方式授权。 world：只有一个用户，anyone，表示登录 zookeeper 所有人（默认的模式）。 ip：对客户端使用 IP 地址认证。 auth：使用已添加认证的用户认证。 digest：使用用户名:密码方式认证。 授权对象 给谁授权，授权对象的 ID 指的是权限赋予的实体，例如 IP 地址或用户。 授予的权限 授予的权限包括create、delete、read、writer、admin。也就是增、删、改、查、管理的权限，简写cdrwa。 注意：以上 5 种权限中，delete是指对子节点的删除权限，其他 4 种权限是对自身节点的操作权限。 create：简写c，可以创建子节点。 delete：简写d，可以删除子节点（仅下一级节点）。 read：简写r，可以读取节点数据以及显示子节点列表。 write：简写w，可以更改节点数据。 admin：简写a，可以设置节点访问控制列表权限。 授权相关命令 getAcl [path]：读取指定节点的 ACL 权限。 setAcl [path] [acl]：设置 ACL addauth &lt;scheme&gt; &lt;auth&gt;：添加认证用户，和 auth，digest 授权模式相关。 world 授权模式案例 zookeeper 中默认的授权模式，针对登录 zookeeper 的任何用户授予指定的权限。命令如下： 1setAcl [path] world:anyone:[permission] path：节点 permission：授予的权限，比如cdrwa 去掉不能读取节点数据的权限： 12345678910111213141516171819## 获取权限列表（默认的）getAcl /node2'world,'anyone: cdrwa## 去掉读取节点数据的的权限，去掉rsetAcl /node2 world:anyone:cdwa## 再次获取权限列表getAcl /node2'world,'anyone: cdwa## 获取节点数据，没有权限，失败get /node2Authentication is not valid : /node2 IP 授权模式案例 针对登录用户的 ip 进行限制权限。命令如下： 1setAcl [path] ip:[ip]:[acl] 远程登录 zookeeper 的命令如下： 1./zkCli.sh -server ip 设置192.168.10.1这个 ip 的增删改查管理的权限。 1setAcl /node2 ip:192.168.10.1:crdwa Auth 授权模式案例 auth 授权模式需要有一个认证用户，添加命令如下： 1addauth digest [username]:[password] 设置 auth 授权模式命令如下： 1setAcl [path] auth:[user]:[acl] 为chenmou这个账户添加 cdrwa 权限： 12345## 添加一个认证账户addauth digest chenmou:123456## 添加权限setAcl /node2 auth:chenmou:crdwa 多种模式授权 zookeeper 中同一个节点可以使用多种授权模式，多种授权模式用,分隔。 12345678## 创建节点create /node3## 添加认证用户addauth chenmou:123456## 添加多种授权模式setAcl /node3 ip:192.178.10.1:crdwa,auth:chenmou:crdwa ACL 超级管理员 zookeeper 的权限管理模式有一种叫做super，该模式提供一个超管可以方便的访问任何权限的节点。 假设这个超管是super:admin，需要先为超管生成密码的密文： 1234echo -n super:admin | openssl dgst -binary -sha1 |openssl base64## 执行完生成了秘钥xQJmxLMiHGwaqBvst5y6rkB6HQs= 打开zookeeper目录下/bin/zkServer.sh，找到如下一行： 1nohup JAVA&amp;quot;−Dzookeeper.log.dir=JAVA"−Dzookeeper.log.dir=&#123;ZOO_LOG_DIR&#125;" "-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;" 在后面添加一行脚本，如下： 1"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:xQJmxLMiHGwaqBvst5y6rkB6HQs=" 此时完整的脚本如下： 12nohup "$JAVA" "-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;" "-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;" "-Dzookeeper.DigestAuthenticationProvider.superDigest=super:xQJmxLMiHGwaqBvst5y6rkB6HQs=" \ -cp "$CLASSPATH" $JVMFLAGS $ZOOMAIN "$ZOOCFG" &gt; "$_ZOO_DAEMON_OUT" 2&gt;&amp;1 &lt; /dev/null &amp; 重启 zookeeper 重启完成之后此时超管即配置完成，如果需要使用，则使用如下命令： 1addauth digest super:admin Curator 客户端 Curator 是 Netflix 公司开源的一个 Zookeeper 客户端，与 Zookeeper 提供的原生客户端相比，Curator 的抽象层次更高，简化了 Zookeeper 客户端的开发量。 添加依赖1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;4.0.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.10&lt;/version&gt; &lt;/dependency&gt; 建立连接 客户端建立与 Zookeeper 的连接，这里仅仅演示单机版本的连接，如下： 12345678910111213//创建CuratorFramework，用来操作apiCuratorFramework client = CuratorFrameworkFactory.builder() //ip地址+端口号，如果是集群，逗号分隔 .connectString("120.26.101.207:2181") //会话超时时间 .sessionTimeoutMs(5000) //超时重试策略,RetryOneTime：超时重连仅仅一次 .retryPolicy(new RetryOneTime(3000)) //命名空间，父节点，如果不指定是在根节点下 .namespace("node4") .build();//启动client.start(); 重连策略 会话连接策略，即是当客户端与 Zookeeper 断开连接之后，客户端重新连接 Zookeeper 时使用的策略，比如重新连接一次。 RetryOneTime：N 秒后重连一次，仅仅一次，演示如下： 1.retryPolicy(new RetryOneTime(3000)) RetryNTimes：每 n 秒重连一次，重连 m 次。演示如下： 12//每三秒重连一次，重连3次。arg1：多长时间后重连，单位毫秒，arg2：总共重连几次.retryPolicy(new RetryNTimes(3000,3)) RetryUntilElapsed：设置了最大等待时间，如果超过这个最大等待时间将会不再连接。 12//每三秒重连一次，等待时间超过10秒不再重连。arg1：总等待时间，arg2：多长时间重连，单位毫秒.retryPolicy(new RetryUntilElapsed(10000,3000)) 新增节点 新增节点 1234567client.create() //指定节点的类型。PERSISTENT：持久化节点，PERSISTENT_SEQUENTIAL：持久化有序节点，EPHEMERAL：临时节点，EPHEMERAL_SEQUENTIAL临时有序节点 .withMode(CreateMode.PERSISTENT) //指定权限列表，OPEN_ACL_UNSAFE：world:anyone:crdwa .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) //写入节点数据，arg1:节点名称 arg2:节点数据 .forPath("/a", "a".getBytes()); 自定义权限列表：withACL(acls)方法中可以设置自定义的权限列表，代码如下： 1234567891011//自定义权限列表List&lt;ACL&gt; acls=new ArrayList&lt;&gt;();//指定授权模式和授权对象 arg1:授权模式，arg2授权对象Id id=new Id("ip","127.0.0.1");//指定授予的权限，ZooDefs.Perms.ALL:crdwaacls.add(new ACL(ZooDefs.Perms.ALL,id));client.create() .withMode(CreateMode.PERSISTENT) //指定自定义权限列表 .withACL(acls) .forPath("/b", "b".getBytes()); 递归创建节点：creatingParentsIfNeeded()方法对于创建多层节点，如果其中一个节点不存在的话会自动创建 12345678//递归创建节点client.create() //递归方法，如果节点不存在，那么创建该节点 .creatingParentsIfNeeded() .withMode(CreateMode.PERSISTENT) .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) //test节点和b节点不存在，递归创建出来 .forPath("/test/a", "a".getBytes()); 异步创建节点：inBackground()方法可以异步回调创建节点，创建完成后会自动回调实现的方法 1234567891011121314151617 //异步创建节点client.create() .withMode(CreateMode.PERSISTENT) .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) //异步创建 .inBackground(new BackgroundCallback() &#123; /** * @param curatorFramework 客户端对象 * @param curatorEvent 事件对象 */ @Override public void processResult(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception &#123; //打印事件类型 System.out.println(curatorEvent.getType()); &#125; &#125;) .forPath("/test1", "a".getBytes()); 更新节点数据 更新节点，当节点不存在会报错，代码如下： 12client.setData() .forPath("/a","a".getBytes()); 携带版本号更新节点，当版本错误拒绝更新 1234client.setData() //指定版本号更新，如果版本号错误则拒绝更新 .withVersion(1) .forPath("/a","a".getBytes()); 异步更新节点数据： 123456789client.setData() //异步更新 .inBackground(new BackgroundCallback() &#123; //回调方法 @Override public void processResult(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception &#123; &#125; &#125;) .forPath("/a","a".getBytes()); 删除节点 删除当前节点，如果有子节点则拒绝删除 123client.delete() //删除节点，如果是该节点包含子节点，那么不能删除 .forPath("/a"); 指定版本号删除，如果版本错误则拒绝删除 12345client.delete() //指定版本号删除 .withVersion(1) //删除节点，如果是该节点包含子节点，那么不能删除 .forPath("/a"); 如果当前节点包含子节点则一并删除，使用deletingChildrenIfNeeded()方法 12345client.delete() //如果删除的节点包含子节点则一起删除 .deletingChildrenIfNeeded() //删除节点，如果是该节点包含子节点，那么不能删除 .forPath("/a"); 异步删除节点，使用inBackground() 1234567891011client.delete() .deletingChildrenIfNeeded() //异步删除节点 .inBackground(new BackgroundCallback() &#123; @Override public void processResult(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception &#123; //回调监听 &#125; &#125;) //删除节点，如果是该节点包含子节点，那么不能删除 .forPath("/a"); 获取节点数据 同步获取节点数据 12byte[] bytes = client.getData().forPath("/node1");System.out.println(new String(bytes)); 获取节点状态和数据 123456789//保存节点状态Stat stat=new Stat();byte[] bytes = client.getData() //获取节点状态存储在stat对象中 .storingStatIn(stat) .forPath("/node1");System.out.println(new String(bytes));//获取节点数据的长度System.out.println(stat.getDataLength()); 异步获取节点数据 1234567client.getData() //异步获取节点数据，回调监听 .inBackground((curatorFramework, curatorEvent) -&gt; &#123; //节点数据 System.out.println(new String(curatorEvent.getData())); &#125;) .forPath("/node1"); 获取子节点 同步获取全部子节点 1234List&lt;String&gt; strs = client.getChildren().forPath("/"); for (String str:strs) &#123; System.out.println(str); &#125; 异步获取全部子节点 123456789client.getChildren()//异步获取.inBackground((curatorFramework, curatorEvent) -&gt; &#123; List&lt;String&gt; strs = curatorEvent.getChildren(); for (String str:strs) &#123; System.out.println(str); &#125; &#125;).forPath("/"); 查看节点是否存在 同步查看 12//如果节点不存在，stat为nullStat stat = client.checkExists().forPath("/node"); 异步查看 1234567//如果节点不存在，stat为nullclient.checkExists() .inBackground((curatorFramework, curatorEvent) -&gt; &#123; //如果为null则不存在 System.out.println(curatorEvent.getStat()); &#125;) .forPath("/node"); Watcher API curator 提供了两种 watcher 来监听节点的变化 NodeCache：监听一个特定的节点，监听新增和修改 PathChildrenCache：监听一个节点的子节点，当一个子节点增加、删除、更新时，path Cache 会改变他的状态，会包含最新的子节点的数据和状态。 NodeCache 演示： 123456789101112131415//arg1:连接对象 arg2:监听的节点路径,/namespace/pathfinal NodeCache nodeCache = new NodeCache(client, "/w1");//启动监听nodeCache.start();//添加监听器nodeCache.getListenable().addListener(() -&gt; &#123; //节点路径 System.out.println(nodeCache.getCurrentData().getPath()); //节点数据 System.out.println(new String(nodeCache.getCurrentData().getData()));&#125;);//睡眠100秒Thread.sleep(1000000);//关闭监听nodeCache.close(); PathChildrenCache演示： 123456789101112//arg1：连接对象 arg2：节点路径 arg3:是否能够获取节点数据PathChildrenCache cache=new PathChildrenCache(client,"/w1", true);cache.start();cache.getListenable().addListener((curatorFramework, pathChildrenCacheEvent) -&gt; &#123; //节点路径 System.out.println(pathChildrenCacheEvent.getData().getPath()); //节点状态 System.out.println(pathChildrenCacheEvent.getData().getStat()); //节点数据 System.out.println(new String(pathChildrenCacheEvent.getData().getData()));&#125;);cache.close(); 小福利 是不是觉得文章太长看得头晕脑胀，为此陈某特地将本篇文章制作成 PDF 文本，需要回去仔细研究的朋友，老规矩，回复关键词ZK入门指南。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mysql中orderby底层执行流程]]></title>
      <url>%2F2020%2F04%2F19%2FMysql%E4%B8%ADorderby%E5%BA%95%E5%B1%82%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%2F</url>
      <content type="text"><![CDATA[前言 在实际的开发中一定会碰到根据某个字段进行排序后来显示结果的需求，但是你真的理解order by在 Mysql 底层是如何执行的吗？ 假设你要查询城市是苏州的所有人名字，并且按照姓名进行排序返回前 1000 个人的姓名、年龄，这条 sql 语句应该如何写？ 首先创建一张用户表，sql 语句如下： 12345678CREATE TABLE user ( id int(11) NOT NULL, city varchar(16) NOT NULL, name varchar(16) NOT NULL, age int(11) NOT NULL, PRIMARY KEY (id), KEY city (city)) ENGINE=InnoDB; 则上述需求的 sql 查询语句如下： 1select city,name,age from user where city='苏州' order by name limit 1000; 这条 sql 查询语句相信大家都能写出来，但是你了解它在 Mysql 底层的执行流程吗？今天陈某来大家聊一聊这条 sql 语句是如何执行的以及有什么参数会影响执行的流程。 本篇文章分为如下几个部分进行详细的阐述： 全字段排序 rowid 排序 全字段排序 VS rowid 排序 如何避免排序 全字段排序 前面聊过索引能够避免全表扫描，因此我们给city这个字段上添加了索引，当然城市的字段很小，不用考虑字符串的索引问题，之前有写过一篇关于如何给字符串的加索引的文章，有不了解朋友看一下这篇文章:Mysql 性能优化：如何给字符串加索引？ 此时用Explain来分析一下的这条查询语句的执行情况，结果如下图： Extra这个字段中的Using filesort表示的就是需要排序，MySQL 会给每个线程分配一块内存用于排序，称为sort_buffer。 既然使用了索引进行查询，我们来简单的画一下city这棵索引树的结构，如下图： 从上图可以看出，满足city=&#39;苏州&#39;是从ID3到IDX这些记录。 通常情况下，此条 sql 语句执行流程如下： 初始化 sort_buffer，确定放入 name、city、age 这三个字段。 从索引 city 找到第一个满足city=&#39;苏州&#39;条件的主键id，也就是图中的ID3。 到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中。 从索引city取下一个记录的主键 id。 重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的IDX。 对sort_buffer中的数据按照字段name做快速排序。 按照排序结果取前 1000 行返回给客户端。 我们称这个排序过程为全字段排序，执行的流程图如下： 图中按name排序这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数sort_buffer_size。 sort_buffer_size：就是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。 rowid 排序 在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在sort_buffer和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么sort_buffer里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。 所以如果单行很大，这个方法效率不够好。 我们可以修改一个max_length_for_sort_data这个参数使其使用另外一种算法。max_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。 city、name、age 这三个字段的定义总长度是36，我把max_length_for_sort_data设置为 16，我们再来看看计算过程有什么改变。设置的 sql 语句如下： 1SET max_length_for_sort_data = 16; 新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。 但这时，排序的结果就因为少了 city 和 age 字段的值，不能直接返回了，整个执行流程就变成如下所示的样子： 初始化sort_buffer，确定放入两个字段，即name和id。 从索引 city 找到第一个满足city=&#39;苏州&#39;条件的主键id，也就是图中的ID3。 到主键id索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中。 从索引city取下一个记录的主键 id。 重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的IDX。 对sort_buffer中的数据按照字段name做快速排序。 遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。 这个执行流程的示意图如下，我把它称为rowid排序。 对比全字段排序，rowid排序多了一次回表查询，即是多了第7步的查询主键索引树。 全字段排序 VS rowid 排序 如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。 如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。 这也就体现了 MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。 对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。 如何避免排序 其实，并不是所有的order by语句，都需要排序操作的。从上面分析的执行过程，我们可以看到，MySQL 之所以需要生成临时表，并且在临时表上做排序操作，其原因是原来的数据都是无序的。 如果能够保证从city这个索引上取出来的行，天然就是按照 name 递增排序的话，是不是就可以不用再排序了呢？ 因此想到了联合索引，创建(city,name)联合索引，sql 语句如下： 1alter table user add index city_user(city, name); 此时的索引树如下： 在这个索引里面，我们依然可以用树搜索的方式定位到第一个满足city=&#39;苏州&#39;的记录，并且额外确保了，接下来按顺序取“下一条记录”的遍历过程中，只要 city 的值是苏州，name 的值就一定是有序的。 按照上图，整个查询的流程如下： 从索引(city,name)找到第一个满足 city=’苏州’条件的主键 id。 到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回。 从索引(city,name)取下一个记录主键 id。 重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city=’苏州’条件时循环结束。 对应的流程图如下： 可以看到，这个查询过程不需要临时表，也不需要排序。接下来，我们用 explain 的结果来印证一下。 从图中可以看到，Extra字段中没有Using filesort了，也就是不需要排序了。而且由于(city,name)这个联合索引本身有序，所以这个查询也不用把 4000 行全都读一遍，只要找到满足条件的前 1000 条记录就可以退出了。也就是说，在我们这个例子里，只需要扫描 1000 次。 难道仅仅这样就能满足了？此条查询语句是否能再优化呢？ 朋友们还记得覆盖索引吗？覆盖索引的好处就是能够避免再次回表查询，不了解的朋友们可以看一下陈某之前写的文章：Mysql 性能优化：如何使用覆盖索引？。 我们创建(city,name,age)联合索引，这样在执行上面的查询语句就能使用覆盖索引了，避免了回表查询了，sql 语句如下： 1alter table user add index city_user_age(city, name, age); 此时执行流程图如下： 当然，覆盖索引能够提升效率，但是维护索引也是需要代价的，因此还需要权衡使用。 总结 今天这篇文章，我和你介绍了 MySQL 里面order by语句的几种算法流程。 在开发系统的时候，你总是不可避免地会使用到 order by 语句。心里要清楚每个语句的排序逻辑是怎么实现的，还要能够分析出在最坏情况下，每个语句的执行对系统资源的消耗，这样才能做到下笔如有神，不犯低级错误。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[面试官：你知道哪些事务失效的场景？]]></title>
      <url>%2F2020%2F04%2F19%2F%E9%9D%A2%E8%AF%95%E5%AE%98%EF%BC%9A%E4%BD%A0%E7%9F%A5%E9%81%93%E5%93%AA%E4%BA%9B%E4%BA%8B%E5%8A%A1%E5%A4%B1%E6%95%88%E7%9A%84%E5%9C%BA%E6%99%AF%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[前言 声明式事务是Spring功能中最爽之一，可是有些时候，我们在使用声明式事务并未生效，这是为什么呢？ 今天陈某带大家来聊一聊声明事务的几种失效场景。本文将会从以下两个方面来说一下事务为什么会失效？ @Transactional介绍 @Transactional失效场景 @Transactional介绍 @Transactional是声明式事务的注解，可以被标记在类上、接口、方法上。 该注解中有很多值得深入了解的几种属性，我们来看一下。 transactionManager 指定事务管理器，值为bean的名称，这个主要用于多事务管理器情况下指定。比如多数据源配置的情况下。 isolation 事务的隔离级别，默认是Isolation.DEFAULT。 几种值的含义如下： Isolation.DEFAULT：事务默认的隔离级别，使用数据库默认的隔离级别。 Isolation.READ_UNCOMMITTED：这是事务最低的隔离级别，它充许别外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻读。 Isolation.READ_COMMITTED：保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。这种事务隔离级别可以避免脏读出现，但是可能会出现不可重复读和幻读。 Isolation.REPEATABLE_READ：这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻读。 Isolation.SERIALIZABLE：这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读外，还避免了幻读。 propagation 代表事务的传播行为，默认值为Propagation.REQUIRED。 Propagation.REQUIRED：如果存在一个事务，则支持当前事务。如果没有事务则开启一个新的事务。比如A方法内部调用了B方法，此时B方法将会使用A方法的事务。 Propagation.MANDATORY：支持当前事务，如果当前没有事务，就抛出异常。 Propagation.NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。 Propagation.NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 Propagation.REQUIRES_NEW：新建事务，如果当前存在事务，把当前事务挂起。比如A方法使用默认的事务传播属性，B方法使用REQUIRES_NEW，此时A方法在内部调用B方法，一旦A方法出现异常，A方法中的事务回滚了，但是B方法并没有回滚，因为A和B方法使用的不是同一个事务，B方法新建了一个事务。 Propagation.NESTED：支持当前事务，新增Savepoint点，也就是在进入子事务之前，父事务建立一个回滚点，与当前事务同步提交或回滚。 子事务是父事务的一部分，在父事务还未提交时，子事务一定没有提交。嵌套事务一个非常重要的概念就是内层事务依赖于外层事务。外层事务失败时，会回滚内层事务所做的动作。而内层事务操作失败并不会引起外层事务的回滚。 timeout 事务的超时时间，单位为秒。 readOnly 该属性用于设置当前事务是否为只读事务，设置为true表示只读，false则表示可读写，默认值为false。如果一个事务只涉及到只读，可以设置为true。 rollbackFor 属性 用于指定能够触发事务回滚的异常类型，可以指定多个异常类型。 默认是在RuntimeException和Error上回滚。 noRollbackFor 抛出指定的异常类型，不回滚事务，也可以指定多个异常类型。 @Transactional失效场景 声明式事务失效的场景有很多，陈某这里只是罗列一下几种常见的场景。 底层数据库引擎不支持事务 如果数据库引擎不支持事务，则Spring自然无法支持事务。 在非public修饰的方法使用 @Transactional注解使用的是AOP，在使用动态代理的时候只能针对public方法进行代理，源码依据在AbstractFallbackTransactionAttributeSource类中的computeTransactionAttribute方法中，如下： 123456protected TransactionAttribute computeTransactionAttribute(Method method, Class&lt;?&gt; targetClass) &#123; // Don't allow no-public methods as required. if (allowPublicMethodsOnly() &amp;&amp; !Modifier.isPublic(method.getModifiers())) &#123; return null;&#125; 此处如果不是标注在public修饰的方法上并不会抛出异常，但是会导致事务失效。 异常被 “ 踹死了 “ 这种情况小白是最容易犯错的，在整个事务的方法中使用try-catch，导致异常无法抛出，自然会导致事务失效。伪代码如下：123456789@Transactionalpublic void method()&#123; try&#123; //插入一条数据 //更改一条数据 &#125;catch(Exception ex)&#123; return; &#125;&#125; 方法中调用同类的方法 简单的说就是一个类中的A方法（未标注声明式事务）在内部调用了B方法(标注了声明式事务)，这样会导致B方法中的事务失效。 代码如下： 123456789101112public class Test&#123; public void A()&#123; //插入一条数据 //调用B方法 B(); &#125; @Transactional public void B()&#123; //插入数据 &#125;&#125; 为什么会失效呢？：其实原因很简单，Spring在扫描Bean的时候会自动为标注了@Transactional注解的类生成一个代理类（proxy）,当有注解的方法被调用的时候，实际上是代理类调用的，代理类在调用之前会开启事务，执行事务的操作，但是同类中的方法互相调用，相当于this.B()，此时的B方法并非是代理类调用，而是直接通过原有的Bean直接调用，所以注解会失效。 如何解决呢？：这就涉及到注解失效的原因了，后续文章会介绍到，这里不过多介绍了。 rollbackFor属性设置错误 很容易理解，指定异常触发回滚，一旦设置错误，导致一些异常不能触发回滚，此时的声明式事务不就失效了吗。 noRollbackFor属性设置错误 这个和rollbackFor属性设置错误类似，一旦设置错误，也会导致异常不能触发回滚，此时的声明式事务会失效。 propagation属性设置错误 事务的传播属性在上面已经介绍了，默认的事务传播属性是Propagation.REQUIRED，但是一旦配置了错误的传播属性，也是会导致事务失效，如下三种配置将会导致事务失效： Propagation.SUPPORTS Propagation.NOT_SUPPORTED Propagation.NEVER 原始SSM项目，重复扫描导致事务失效 在原始的SSM项目中都配置了context:component-scan并且同时扫描了service层，此时事务将会失效。 按照Spring配置文件的加载顺序来说，会先加载Springmvc的配置文件，如果在加载Springmvc配置文件的时候把service也加载了，但是此时事务还没加载，将会导致事务无法成功生效。 解决方法很简单，把扫描service层的配置设置在Spring配置文件或者其他配置文件中即可。 总结 事务失效的原因很多，但是千万不要做到一知半解，只有深入理解了，才能在面试过程中对答如流。 今天的文章就到此结束了，如果觉得陈某写得不错，有所收获的，关注在看来一波，你们的鼓励，将会是我写作的动力，谢谢支持！！！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[设计模式：工厂方法模式]]></title>
      <url>%2F2020%2F04%2F05%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%9A%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[导读 工厂方法模式是所有设计模式中比较常用的一种模式，但是真正能搞懂用好的少之又少，Spring底层大量的使用该设计模式来进行封装，以致开发者阅读源代码的时候晕头转向。 今天陈某分别从以下五个方面详细讲述一下工厂方法模式： 从什么是工厂方法模式 通用框架实现 工厂方法模式的优点 工厂方法模式的升级 Spring底层如何使用工厂方法模式 什么是工厂方法模式？ 定义：定义一个用于创建对象的 接口，让子类决定实例化哪一个类。工厂方法使一个类的实例化延迟到其子类。 工厂方法模式通用类图如下： 在工厂方法模式中，抽象产品Product负责定义产品的特性，实现对事物的抽象定义。 AbstractFactory是抽象工厂类，定义了一个抽象工厂方法。具体的如何创建产品由工厂实现类ConcreteFactory完成。 通用框架实现 工厂方法模式的变种有很多，陈某给出一个比较实用的通用框架。 抽象产品类： 1234567891011public abstract class Product &#123; /** * 公共逻辑方法 */ public void method1()&#123;&#125; /** * 抽象方法：由子类实现，根据业务逻辑定义多个 */ public abstract void method2();&#125; 具体产品类1，继承抽象产品类，如下： 123456789public class Product1 extends Product &#123; /** * 实现抽象产品类的抽象方法 */ @Override public void method2() &#123; &#125;&#125; 具体产品类2，继承抽象产品类，如下： 12345678910public class Product2 extends Product &#123; /** * 实现抽象产品类的抽象方法 */ @Override public void method2() &#123; &#125;&#125; 抽象工厂类，必须定义一个工厂方法来自己实现具体的创建逻辑，如下： 123456789public abstract class AbstractFactory &#123; /** * 工厂方法，需要子类实现 * @param cls * @param &lt;T&gt; * @return */ public abstract &lt;T extends Product&gt; T create(Class&lt;T&gt; cls);&#125; 具体工厂类，使用了反射对具体产品的实例化，如下： 123456789101112public class ConcreteFactory extends AbstractFactory &#123; @Override public &lt;T extends Product&gt; T create(Class&lt;T&gt; cls) &#123; Product product=null; try&#123; product= (Product) Class.forName(cls.getName()).newInstance(); &#125;catch (Exception ex)&#123; ex.printStackTrace(); &#125; return (T) product; &#125;&#125; 测试如下： 1234567public static void main(String[] args) &#123; //创建具体工厂类 ConcreteFactory factory = new ConcreteFactory(); //调用工厂方法获取产品类1的实例 Product1 product1 = factory.create(Product1.class); System.out.println(product1); &#125; 以上是简单的一个通用框架，读者可以根据自己的业务在其上拓展。 工厂方法模式的优点 良好的封装性，代码结构清晰，调用者不用关系具体的实现过程，只需要提供对应的产品类名称即可。 易扩展性，在增加产品类的情况下，只需要适当的修改工厂类逻辑或者重新拓展一个工厂类即可。 屏蔽了产品类，产品类的变化调用者不用关心。比如在使用JDBC连接数据库时，只需要改动一个驱动的名称，数据库就会从Mysql切换到Oracle，极其灵活。 工厂方法模式的升级 在复杂的系统中，一个产品的初始化过程是及其复杂的，仅仅一个具体工厂实现可能有些吃力，此时最好的做法就是为每个产品实现一个工厂，达到一个工厂类只负责生产一个产品。 此时工厂方法模式的类图如下： 如上图，每个产品类都对应了一个工厂，一个工厂只负责生产一个产品，非常符合单一职责原则。 针对上述的升级过程，那么工厂方法中不需要传入抽象产品类了，因为一个工厂只负责一个产品的生产，此时的抽象工厂类如下：123456public abstract class AbstractFactory &#123; /** * 工厂方法，需要子类实现 */ public abstract &lt;T extends Product&gt; T create();&#125; Spring底层如何使用工厂方法模式？ 工厂方法模式在Spring底层被广泛的使用，陈某今天举个最常用的例子就是AbstractFactoryBean。 这个抽象工厂很熟悉了，这里不再讨论具体的作用。其实现了FactoryBean接口，这个接口中getObject()方法返回真正的Bean实例。 AbstractFactoryBean中的getObject()方法如下： 12345678910111213public final T getObject() throws Exception &#123; //单例，从缓存中取，或者暴露一个早期实例解决循环引用 if (isSingleton()) &#123; return (this.initialized ? this.singletonInstance : getEarlySingletonInstance()); &#125; //多实例 else &#123; //调用createInstance return createInstance(); &#125; &#125; //创建对象 protected abstract T createInstance() throws Exception; 从以上代码可以看出，创建对象的职责交给了createInstance这个抽象方法，由其子类去定制自己的创建逻辑。 下图显示了继承了AbstractFactoryBean的具体工厂类，如下： 其实与其说AbstractFactoryBean是抽象工厂类，不如说FactoryBean是真正的抽象工厂类，前者只是对后者的一种增强，完成大部分的可复用的逻辑。比如常用的sqlSessionFactoryBean只是简单的实现了FactoryBean，并未继承AbstractFactoryBean，至于结论如何，具体看你从哪方面看了。 总结 工厂方法模式是一种常见的设计模式，但是真正能够用的高级，用的透彻还是有些难度的，开发者所能做的就是在此模式基础上思考如何优化自己的代码，达到易扩展、封装性强的效果了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[设计模式：模板模式]]></title>
      <url>%2F2020%2F04%2F05%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%9A%E6%A8%A1%E6%9D%BF%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[导读 模板模式在是Spring底层被广泛的应用，比如事务管理器的实现，JDBC模板的实现。 今天就来谈谈什么是模板模式、模板模式的优缺点、模板模式的简单演示、模板模式在Spring底层的实现。 什么是模板模式 模板模式首先要有一个抽象类，这个抽象类公开定义了执行它的方法的方式/模板。它的子类可以按需要重写方法实现，但调用将以抽象类中定义的方式进行。这种类型的设计模式属于行为型模式。 定义：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 比如在造房子一样，地基，铺线，房子户型都是一样的，由开发商决定，但是在交房之后，室内的装修风格和场景布置却是由业主决定，在这个场景中，开发商其实就是一个抽象类，地基，铺线，房子户型都是可以复用的，但是装修却是不可复用的，必须由业主决定，此时的每一个业主的房子就是一个实现的子类。 模板方法的实现条件注意： 必须是一个抽象类。 抽象类有一个模板方法，其中定义了算法骨架。 为了防止恶意操作，模板方法必须加上final关键词。 模板方法中除了复用的代码，其他的关键代码必须是抽象的，子类可以继承实现。 优点 它封装了不变部分，扩展可变部分。它把认为是不变部分的算法封装到父类中实现，而把可变部分算法由子类继承实现，便于子类继续扩展。 它在父类中提取了公共的部分代码，便于代码复用。 部分方法是由子类实现的，因此子类可以通过扩展方式增加相应的功能，符合开闭原则。 缺点 对每个不同的实现都需要定义一个子类，这会导致类的个数增加，系统更加庞大，设计也更加抽象。 父类中的抽象方法由子类实现，子类执行的结果会影响父类的结果，这导致一种反向的控制结构，它提高了代码阅读的难度。 简单演示 比如游戏的运行需要如下几个步骤： 初始化游戏 开始游戏 结束游戏 上述的三个步骤可以是模板类的抽象方法，由具体的子类实现，比如足球游戏。 定义模板类，必须是一个抽象类，模板方法必须是final修饰。 12345678910111213141516171819public abstract class Game &#123; //抽象方法 abstract void initialize(); abstract void startPlay(); abstract void endPlay(); //模板方法 public final void play()&#123; //初始化游戏 initialize(); //开始游戏 startPlay(); //结束游戏 endPlay(); &#125;&#125; 定义实现类，足球游戏，继承模板类，实现其中的三个抽象方法 1234567891011121314151617public class Football extends Game &#123; @Override void endPlay() &#123; System.out.println("足球游戏结束......"); &#125; @Override void initialize() &#123; System.out.println("足球游戏初始化中......"); &#125; @Override void startPlay() &#123; System.out.println("足球游侠开始了......"); &#125;&#125; 此时写一个测试方法，运行足球游戏，如下： 12345678public class TemplatePatternDemo &#123; public static void main(String[] args) &#123; //创建足球游戏实例 Game game = new Football(); //开始游戏 game.play(); &#125;&#125; 输出结果如下： 123足球游戏初始化中......足球游侠开始了......足球游戏结束...... Spring中的模板模式 Spring底层对于模板模式的使用有很多处，今天陈某带大家康康事务管理器是如何使用模板模式的。 模板抽象类 AbstractPlatformTransactionManager是Spring中的模板抽象类，来看看它的继承关系图： 实现了PlatformTransactionManager接口，重载了接口中的方法。 模板方法 事务管理器中抽象类中的模板方法不止一个，比如以下两个方法 12345//提交事务public final void commit()//获取TransactionStatuspublic final TransactionStatus getTransaction() 这两个方法都对于自己要实现的逻辑搭建了一个骨架，主要的功能是由抽象方法完成，由子类来完成。 抽象方法 事务管理器抽象类中的抽象方法定义了多个，分别用于处理不同的业务逻辑，由子类实现其中具体的逻辑，如下： 1234567891011//提交事务protected abstract void doCommit(DefaultTransactionStatus status);//回滚事务protected abstract void doRollback(DefaultTransactionStatus status);//开始事务protected abstract void doBegin(Object transaction, TransactionDefinition definition)//获取当前的事务对象protected abstract Object doGetTransaction() 抽象方法的定义便于子类去扩展，在保证算法逻辑不变的情况下，子类能够定制自己的实现。 具体子类 事务管理器的模板类有很多的具体子类，如下图： 其中我们熟悉的有DataSourceTransactionManager、JtaTransactionManager、RabbitTransactionManager。具体承担什么样的角色和责任不是本节的重点，不再细说。 总结 模板模式是一个很重要，易扩展的模式，提高了代码复用性，在Spring中有着广泛的应用，比如JDBCTemplate,AbstractPlatformTransactionManager，这些实现都用到了模板模式。 如果觉得陈某的文章能够对你有所帮助，有所启发，关注分享一波，点个在看，谢谢支持！！！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mysql性能优化：为什么count(*)这么慢？]]></title>
      <url>%2F2020%2F04%2F05%2FMysql%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88count%E8%BF%99%E4%B9%88%E6%85%A2%2F</url>
      <content type="text"><![CDATA[导读 在开发中一定会用到统计一张表的行数，比如一个交易系统，老板会让你每天生成一个报表，这些统计信息少不了sql中的count函数。 但是随着记录越来越多，查询的速度会越来越慢，为什么会这样呢？Mysql内部到底是怎么处理的？ 今天这篇文章将从Mysql内部对于count函数是怎样处理的？ count的实现方式 在Mysql中的不同的存储引擎对count函数有不同的实现方式。 MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高（没有where查询条件）。 InnoDB引擎并没有直接将总数存在磁盘上，在执行count(*)函数的时候需要一行一行的将数据读出来，然后累计总数。 为什么InnoDB不将总数存起来？ 说道InnoDB相信读者总会想到其支持事务的特性，事务具有隔离性，如果将总数存起来，怎么保证各个事务之间的总数的一致性呢？不明白的看下图： 事务A和事务B中的count(*)的执行结果是不同的，因此InnoDB引擎在每个事务中返回多少行是不确定的，只能一行一行的读出来用来判断总数。 如何提升count效率 在InnoDB对于如何提升count(*)的查询效率，网上有多种解决办法，这里主要介绍三种，并分析可行性。 show table status show table status这个命令能够很快的查询出数据库中每个表的行数，但是真的能够替代count(*)吗？ 答案是不能。原因很简单，这个命令统计出来的值是一个估值，因此是不准确的，官方文档说误差大概在40%-50%。 因此这种方法直接pass，不准确还用它干嘛。 缓存系统存储总数 这种方法也是最容易想到的，增加一行就+1，删除一行就-1，并且缓存系统读取也是很快，既简单又方便的为什么不用？ 缓存系统和Mysql是两个系统，比如redis和Mysql这两个是典型的比较。两个系统最难的就是在高并发下无法保证数据的一致性。通过以下两图我们来理解一下： 通过上面两张图，无论是redis计数+1还是insert into user先执行，最终都会导致数据在逻辑上的不一致。第一张图会出现redis计数少了，第二张图虽然计数正确了但是并没有查询出插入的那一行数据。 在并发系统里面，我们是无法精确控制不同线程的执行时刻的，因为存在图中的这种操作序列，所以，我们说即使Redis正常工作，这个计数值还是逻辑上不精确的。 在数据库保存计数 通过缓存系统保存的分析得知了使用缓存无法保证数据在逻辑上的一致性，因此我们想到了直接使用数据库来保存，有了事务的支持，也就保证了数据的一致性了。 如何使用呢？很简单，直接将计数保存在一张表中（table_name,total）。 至于执行的逻辑只需要将缓存系统中redis计数+1改成total字段+1即可，如下图： 由于在同一个事务中，保证了数据在逻辑上的一致性。 不同count的用法 count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加1，否则不加。最后返回累计值。 count的用法有多种，分别是count(*)、count(字段)、count(1)、count(主键id)。那么多种用法，到底有什么差别呢？当然，前提是没有where条件语句。 count(id)：InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层。server层拿到id后，判断是不可能为空的，就按行累加。 count(1)：InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个数字1进去，判断是不可能为空的，按行累加。 count(字段)： 如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为null，按行累加； 如果这个字段定义允许为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加。 count(*)：不会把全部字段取出来，而是专门做了优化，不取值。count(*)肯定不是null，按行累加。 所以结论很简单：按照效率排序的话，count(字段)&lt;count(主键id)&lt;count(1)≈count(*)，所以建议读者，尽量使用count(*)。 注意：这里肯定有人会问，count(id)不是走的索引吗，为什么查询效率和其他的差不多呢？陈某在这里解释一下，虽然走的索引，但是还是要一行一行的扫描才能统计出来总数。 总结 MyISAM表虽然count(*)很快，但是不支持事务； show table status命令虽然返回很快，但是不准确； InnoDB直接count(*)会遍历全表(没有where条件)，虽然结果准确，但会导致性能问题。 缓存系统的存储计数虽然简单效率高，但是无法保证数据的一致性。 数据库保存计数很简单，也能保证数据的一致性，建议使用。 思考题，读者留言区讨论：在系统高并发的情况下，使用数据库保存计数，是先更新计数+1,还是先插入数据。即是先update total+=1还是先insert into。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[接口幂等性如何设计？]]></title>
      <url>%2F2020%2F04%2F01%2F%E6%8E%A5%E5%8F%A3%E5%B9%82%E7%AD%89%E6%80%A7%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[导读 现在这个时代大家可能最关心的就是钱了，那么有没有想过你银行转账给你没有一次是转多的，要么失败，要么成功，为什么不能失误一下多转一笔呢？醒醒吧年轻人，别做梦了，做银行的能那么傻x吗？ 今天我们就来谈一谈为什么银行转账不能多给我转一笔？关乎到钱的问题，小伙伴们打起精神！！！ 要想要理解上述的疑惑，不得不提的一个概念就是幂等性，至于什么是幂等性，如何通过代码实现幂等性，下面将会详细讲述。 什么是幂等性 所谓幂等性通俗的将就是一次请求和多次请求同一个资源产生相同的副作用。用数学语言表达就是f(x)=f(f(x))。 维基百科的幂等性定义如下： 12幂等（idempotent、idempotence）是一个数学与计算机学概念，常见于抽象代数中。在编程中一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。幂等函数，或幂等方法，是指可以使用相同参数重复执行，并能获得相同结果的函数。这些函数不会影响系统状态，也不用担心重复执行会对系统造成改变。例如，“setTrue()”函数就是一个幂等函数,无论多次执行，其结果都是一样的，更复杂的操作幂等保证是利用唯一交易号(流水号)实现. 为什么需要幂等性 在系统高并发的环境下，很有可能因为网络，阻塞等等问题导致客户端或者调用方并不能及时的收到服务端的反馈甚至是调用超时的问题。总之，就是请求方调用了你的服务，但是没有收到任何的信息，完全懵逼的状态。比如订单的问题，可能会遇到如下的几个问题： 创建订单时，第一次调用服务超时，再次调用是否产生两笔订单？ 订单创建成功去减库存时，第一次减库存超时，是否会多扣一次？ 订单支付时，服务端扣钱成功，但是接口反馈超时，此时再次调用支付，是否会多扣一笔呢？ 作为消费者，前两种能接受，第三种情况就MMP了，哈哈哈！！！这种情况一般有如下两种解决方式 服务方提供一个查询操作是否成功的api，第一次超时之后，调用方调用查询接口，如果查到了就走成功的流程，失败了就走失败的流程。 另一种就是服务方需要使用幂等的方式保证一次和多次的请求结果一致。 HTTP的幂等性 GET：只是获取资源，对资源本身没有任何副作用，天然的幂等性。 HEAD：本质上和GET一样，获取头信息，主要是探活的作用，具有幂等性。 OPTIONS：获取当前URL所支持的方法，因此也是具有幂等性的。 DELETE：用于删除资源，有副作用，但是它应该满足幂等性，比如根据id删除某一个资源，调用方可以调用N次而不用担心引起的错误（根据业务需求而变）。 PUT：用于更新资源，有副作用，但是它应该满足幂等性，比如根据id更新数据，调用多次和N次的作用是相同的（根据业务需求而变）。 POST：用于添加资源，多次提交很可能产生副作用，比如订单提交，多次提交很可能产生多笔订单。 幂等性的实现方式 对于客户端交互的接口，可以在前端拦截一部分，例如防止表单重复提交，按钮置灰，隐藏，不可点击等方式。但是前端进行拦截器显然是针对普通用户，懂点技术的都可以模拟请求调用接口，所以后端幂等性很重要。 后端的幂等性如何实现？将会从以下几个方面介绍。 数据库去重表 在往数据库中插入数据的时候，利用数据库唯一索引特性，保证数据唯一。比如订单的流水号，也可以是多个字段的组合。 实现比较简单，读者可以自己实现看看，这里不再提供demo了。 状态机 很多业务中多有多个状态，比如订单的状态有提交、待支付、已支付、取消、退款等等状态。后端可以根据不同的状态去保证幂等性，比如在退款的时候，一定要保证这笔订单是已支付的状态。 业务中常常出现，读者可以自己实现看看，不再提供demo。 TOKEN机制 针对客户端连续点击或者调用方的超时重试等情况，例如提交订单，此种操作就可以用Token的机制实现防止重复提交。 TOKEN机制如何实现？简单的说就是调用方在调用接口的时候先向后端请求一个全局ID（TOKEN），请求的时候携带这个全局ID一起请求，后端需要对这个全局ID校验来保证幂等操作，流程如下图： 主要的流程步骤如下： 客户端先发送获取token的请求，服务端会生成一个全局唯一的ID保存在redis中，同时把这个ID返回给客户端。 客户端调用业务请求的时候必须携带这个token，一般放在请求头上。 服务端会校验这个Token，如果校验成功，则执行业务。 如果校验失败，则表示重复操作，直接返回指定的结果给客户端。 通过以上的流程分析，唯一的重点就是这个全局唯一ID如何生成，在分布式服务中往往都会有一个生成全局ID的服务来保证ID的唯一性，但是工程量和实现难度比较大，UUID的数据量相对有些大，此处陈某选择的是雪花算法生成全局唯一ID，不了解雪花算法的读者下一篇文章会着重介绍。 代码实现 陈某选择的环境是SpringBoot+Redis单机环境+注解+拦截器的方式实现，只是演示一下思想，具体的代码可以参照实现。 redis如何实现，获取Token接口将全局唯一Id存入Redis（一定要设置失效时间，根据业务需求），业务请求的时候直接从redis中删除，根据delete的返回值判断，返回true表示第一次请求，返回false表示重复请求。代码如下： 12345678910111213141516171819202122@Servicepublic class TokenServiceImpl implements TokenService &#123; @Autowired private StringRedisTemplate stringRedisTemplate; @Override public String getToken() &#123; //获取全局唯一id long nextId = SnowflakeUtil.nextId(); //存入redis，设置10分钟失效 stringRedisTemplate.opsForValue().set(String.valueOf(nextId), UUID.randomUUID().toString(),10, TimeUnit.MINUTES); return String.valueOf(nextId); &#125; /** * 删除记录，true表示第一次提交，false重复提交 */ @Override public Boolean checkToken(String token) &#123; return stringRedisTemplate.delete(token); &#125;&#125; 注解的实现如下，标注在controller类上表示当前类上全部接口都做幂等，标注单个方法上，表示单个接口做幂等操作。 12345678910/** * @Description 幂等操作的注解 * @Author CJB * @Date 2020/3/25 10:19 */@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface RepeatLimiter &#123;&#125; 请求头的拦截器，用于提取请求头和校验请求头，如下： 1234567891011121314151617181920212223242526272829303132/** * @Description 获取请求头的信息，具体校验逻辑读者自己实现 * @Author CJB * @Date 2020/3/25 11:09 */@Componentpublic class HeaderIntercept implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; //获取token String token = request.getHeader(HeaderConstant.TOKEN); //校验逻辑 if (!validToken(token)) throw new TokenInvalidException("TOKEN失效"); //获取其他的参数..... RequestHeader header = RequestHeader.builder() .token(token) .build(); //放入request中 request.setAttribute(HeaderConstant.HEADER_INFO,header); return true; &#125; /** * 校验token，逻辑自己实现 * @param token * @return */ private boolean validToken(String token)&#123; return Boolean.TRUE; &#125;&#125; 保证幂等性的拦截器，直接从redis中删除token，成功则第一次提交，不成功则重复提交。 1234567891011121314151617181920212223242526272829303132333435363738@Componentpublic class RepeatIntercept implements HandlerInterceptor &#123; @Autowired private TokenService tokenService; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; if (handler instanceof HandlerMethod)&#123; //获取方法上的参数 RepeatLimiter repeatLimiter = AnnotationUtils.findAnnotation(((HandlerMethod) handler).getMethod(), RepeatLimiter.class); if (Objects.isNull(repeatLimiter))&#123; //获取controller类上注解 repeatLimiter=AnnotationUtils.findAnnotation(((HandlerMethod) handler).getBean().getClass(),RepeatLimiter.class); &#125; //使用注解，需要拦截验证 if (Objects.nonNull(repeatLimiter))&#123; //获取全局token，表单提交的唯一id RequestHeader info = RequestContextUtils.getHeaderInfo(); //没有携带token，抛异常，这里的异常需要全局捕获 if (StringUtils.isEmpty(info.getToken())) throw new RepeatException(); //校验token Boolean flag = tokenService.checkToken(info.getToken()); //删除失败，表示 if (Boolean.FALSE.equals(flag)) //抛出重复提交的异常 throw new RepeatException(); &#125; &#125; return true; &#125;&#125; 接口幂等实现，代码如下： 12345678910111213141516171819@RestController@RequestMapping("/order")public class OrderController &#123; @Autowired private OrderService orderService; /** * 下单 * @param order * @return */ @PostMapping @RepeatLimiter //幂等性保证 public CommenResult add(@RequestBody Order order)&#123; orderService.save(order); return new CommenResult("200","下单成功"); &#125;&#125; 演示 发送getToken的请求获取Token 携带Token下单第一次： 第二次下单：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mysql性能优化：什么是索引下推？]]></title>
      <url>%2F2020%2F04%2F01%2FMysql%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%9A%E4%BB%80%E4%B9%88%E6%98%AF%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[导读 索引下推（index condition pushdown ）简称ICP，在Mysql5.6的版本上推出，用于优化查询。 在不使用ICP的情况下，在使用非主键索引（又叫普通索引或者二级索引）进行查询时，存储引擎通过索引检索到数据，然后返回给MySQL服务器，服务器然后判断数据是否符合条件 。 在使用ICP的情况下，如果存在某些被索引的列的判断条件时，MySQL服务器将这一部分判断条件传递给存储引擎，然后由存储引擎通过判断索引是否符合MySQL服务器传递的条件，只有当索引符合条件时才会将数据检索出来返回给MySQL服务器 。 索引条件下推优化可以减少存储引擎查询基础表的次数，也可以减少MySQL服务器从存储引擎接收数据的次数。 开撸 在开始之前先先准备一张用户表(user)，其中主要几个字段有：id、name、age、address。建立联合索引（name，age）。 假设有一个需求，要求匹配姓名第一个为陈的所有用户，sql语句如下： 1SELECT * from user where name like '陈%' 根据 “最佳左前缀” 的原则，这里使用了联合索引（name，age）进行了查询，性能要比全表扫描肯定要高。 问题来了，如果有其他的条件呢？假设又有一个需求，要求匹配姓名第一个字为陈，年龄为20岁的用户，此时的sql语句如下： 1SELECT * from user where name like '陈%' and age=20 这条sql语句应该如何执行呢？下面对Mysql5.6之前版本和之后版本进行分析。 Mysql5.6之前的版本 5.6之前的版本是没有索引下推这个优化的，因此执行的过程如下图： 会忽略age这个字段，直接通过name进行查询，在(name,age)这课树上查找到了两个结果，id分别为2,1，然后拿着取到的id值一次次的回表查询，因此这个过程需要回表两次。 Mysql5.6及之后版本 5.6版本添加了索引下推这个优化，执行的过程如下图： InnoDB并没有忽略age这个字段，而是在索引内部就判断了age是否等于20，对于不等于20的记录直接跳过，因此在(name,age)这棵索引树中只匹配到了一个记录，此时拿着这个id去主键索引树中回表查询全部数据，这个过程只需要回表一次。 实践 当然上述的分析只是原理上的，我们可以实战分析一下，因此陈某装了Mysql5.6版本的Mysql，解析了上述的语句，如下图： 根据explain解析结果可以看出Extra的值为Using index condition，表示已经使用了索引下推。 总结 索引下推在非主键索引上的优化，可以有效减少回表的次数，大大提升了查询的效率。 关闭索引下推可以使用如下命令，配置文件的修改不再讲述了，毕竟这么优秀的功能干嘛关闭呢： 1set optimizer_switch='index_condition_pushdown=off';]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mysql性能优化：为什么使用覆盖索引?]]></title>
      <url>%2F2020%2F04%2F01%2FMysql%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E8%A6%86%E7%9B%96%E7%B4%A2%E5%BC%95%2F</url>
      <content type="text"><![CDATA[导读 相信读者看过很多MYSQL索引优化的文章，其中有很多优化的方法，比如最佳左前缀，覆盖索引等方法，但是你真正理解为什么要使用最佳左前缀，为什么使用覆盖索引会提升查询的效率吗？ 本篇文章将从MYSQL内部结构上讲一下为什么覆盖索引能够提升效率。 InnoDB索引模型 在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。 每一个索引在InnoDB里面对应一棵B+树。 主键索引和非主键索引的区别 主键索引又叫聚簇索引 ，非主键索引又叫普通索引，那么这两种索引有什么区别呢？ 主键索引的叶子节点存放的是整行数据，非主键索引的叶子节点存放的是主键的值。 假设有一张User表（id,age,name,address），其中有id和age两个字段，其中id是主键，age是普通索引，有几行数据u1-u5的(id,age)的值是(100,1)、(200,2)、(300,3)、(500,5)和(600,6) ，此时的两棵树的示例如下： 从上图可以看出来，基于主键索引的树的叶子节点存放的是整行User数据，基于普通索引age的叶子节点存放的是id（主键）的值。 什么是回表？ 假设有一条查询语句如下： 1select * from user where age=3; 上面这条sql语句执行的过程如下： 1、根据age这个普通索引在age索引树上搜索，得到主键id的值为300。 2、因为age索引树并没有存储User的全部数据，因此需要根据在age索引树上查询到的主键id的值300再到id索引树搜索一次，查询到了u3。 3、返回结果。 上述执行的过程中，从age索引树再到id索引树的查询的过程叫做回表（回到主键索引树搜索的过程）。 也就是说通过非主键索引的查询需要多扫描一棵索引树，因此需要尽量使用主键索引查询。 为什么使用覆盖索引？ 有了上述提及到的几个概念，便能很清楚的理解为什么覆盖索引能够提升查询效率了，因为少了一次回表的过程。 假设我们使用覆盖索引查询，语句如下： 1select id from user where age=3; 这条语句执行过程很简单，直接在age索引树中就能查询到id的值，不用再去id索引树中查找其他的数据，避免了回表。 总结 覆盖索引的使用能够减少树的搜索次数，避免了回表，显著提升了查询性能，因此覆盖索引是一个常用的性能优化手段。 留给读者一个问题：身份证是一个人的唯一识别凭证，如果有根据身份证号查询市民信息的需求，我们只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[一条查询语句到底是如何执行的?]]></title>
      <url>%2F2020%2F04%2F01%2F%E4%B8%80%E6%9D%A1%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E5%88%B0%E5%BA%95%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84%2F</url>
      <content type="text"><![CDATA[导读 Mysql在中小型企业中是个香饽饽，目前主流的数据库之一，几乎没有一个后端开发者不会使用的，但是作为一个老司机，仅仅会用真的不够。 今天陈某透过一个简单的查询语句来讲述在Mysql内部的执行过程。 1select * from table where id=10; 撸它 首先通过一张图片来了解一下Mysql的基础架构，如下： 从上图可以看出，Mysql大致分为Server层和存储引擎层两部分。 Server层包括连接器、查询缓存、分析器、优化器等，其中包含了Mysql的大多数核心功能以及所有的内置函数（如日期，时间函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。它的架构是可插拔式的，支持InnoDB、MyISAM等多个存储引擎。Mysql中主流的存储引擎是InnoDB，由于它对事务的支持让它从Mysql5.5.5版本开始成为了默认的存储引擎。 大致了解了整体架构，现在说说每一个基础的模块都承担着怎样的责任。 1. 连接器 顾名思义，是客户端和Mysql之间连接的媒介，负责登录、获取权限、维持连接和管理连接。连接命令一般如下： 1mysql [-h] ip [- P] port -u [user] -p 在完成经典的TCP握手后，连接器会开始认证身份，要求输入密码。 密码认证通过，连接器会查询出拥有的权限，即使管理员修改了权限，也不会影响你这次的连接，只有重新连接才会生效。 密码认证失败，会收到提示信息Access denied for user。 连接完成后，没有后续动作的连接将会变成空闲连接，你可以输入show processlist命令看到它。如下图，其中的Command这一列显示为sleep的这一行表示在系统里面有一个空闲连接。 客户端如果太长时间没有执行动作，连接器将会自动断开，这个时间由参数wait_timeout控制，默认值是8小时。 如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。 2. 查询缓存【废材，8.0 版本完全删除】 连接建立完成后，你就可以select语句了，执行之前会查询缓存。 查询缓存在Mysql中的是默认关闭的，因为缓存命中率非常低，只要有对表执行一个更新操作，这个表的所有查询缓存都将被清空。怎么样？一句废材足以形容了！！！ 废材的东西不必多讲，主流的Redis的缓存你不用，别再搞这废材了。 3. 分析器 如果没有命中查询缓存，就要执行查询了，但是在执行查询之前，需要对SQL语句做解析，判断你这条语句有没有语法错误。 分析器会做 ‘词法分析’ ，你输入的无非可就是多个字符串和空格组成的SQL语句，MYSQL需要识别出里面的字符串是什么，代表什么，有没有关键词等。 MYSQL会从你输入的select 这个关键字识别出来是一个查询语句，table是表名，id是列名。 做完这些会做 ‘语法分析’ ，根据MYSQL定义的规则来判断你的SQL语句有没有语法错误，如果你的语法不对，就会收到类似如下的提醒： 1ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;elect * from t where ID=1&apos; at line 1 一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。 4. 优化器 经过分析器词法和语法的分析，此时就能知道这条SQL语句是干什么的。但是在开始执行之前，MYSQL底层还要使用优化器对这条SQL语句进行优化处理。 MYSQL内部会对这条SQL进行评估，比如涉及到多个索引会比较使用哪个索引代价更小、多表join的时候会考虑决定各个表的连接顺序。 优化器的作用一句话总结：根据MYSQL内部的算法决定如何执行这条SQL语句来达到MYSQL认为代价最小目的。 优化器阶段完成后，这个语句的执行方案就确定了，接下来就交给执行器执行了。 5. 执行器 MYSQL通过分析器知道了要做什么，通过优化器知道了如何做，于是就进入了执行器阶段。 执行器开始执行之前，需要检查一下用户对表table有没有执行的权限，没有返回权限不足的错误，有的话就执行。 执行也是分类的，如果Id不是索引则全表扫描，一行一行的查找，如果是索引则在索引组织表中查询，索引的查询很复杂，其中涉及到B+树等算法，这里不再详细介绍。 总结 一条SQL语句在MYSQL内部执行的过程涉及到的内部模块有：连接器、查询缓存、分析器、优化器、执行器、存储引擎。 至此，MYSQL的基础架构已经讲完了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mysql中的三类锁，你知道吗？]]></title>
      <url>%2F2020%2F04%2F01%2FMysql%E4%B8%AD%E7%9A%84%E4%B8%89%E7%B1%BB%E9%94%81%EF%BC%8C%E4%BD%A0%E7%9F%A5%E9%81%93%E5%90%97%EF%BC%9F%2F</url>
      <content type="text"><![CDATA[导读 正所谓有人(锁)的地方就有江湖(事务)，人在江湖飘，怎能一无所知？ 今天来细说一下Mysql中的三类锁，分别是全局锁、表级锁、行级锁。 全局锁 全局锁简单的说就是锁住整个数据库实例，命令是Flush tables with read lock。当你需要为整个数据库处于只读的状态的时候，可以使用这个命令。 一旦使用全局锁，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的使用场景大部分都是用来数据库备份。 为什么备份要加全局锁？ 用户买东西，首先会从余额里扣除金额，然后在订单里添加商品。如果备份数据库，不加锁，并且备份顺序为先备份用余额，再备份订单商品，有可能备份了用户余额后，用户下订单买东西提交事务，然后再备份订单商品表， 此时订单商品已存在。最后备份出来的数据为。最后用户余额为买东西前的余额，没有减少，但是订单商品却多了。演示如下图： 这种情况可能用户会觉得赚了，但是如果备份顺序反过来，先备份商品表再备份余额表，用户就会发现我付了钱，但是商品没有加，这中结果就会更加的严重。 因此保证备份数据的一致性很重要，必要的手段就是加锁。 全局锁有什么坏处？ 全局锁是个啥？介绍完了读者心里已经有数了，让这个库只读？这是多么可怕的操作，简单列举几个危险之处： 如果在主库备份，备份期间不能执行任何更新操作，会导致整个业务停摆，高并发情况下更甚。 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。 全局备份比较好的解决方案 全局锁远瞅不错，近瞅吓一跳，陈某在此不推荐使用。 其实 官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。 一致性备份是好，但前提是存储引擎支持事务，这也是MyISAM被InnoDB取代的原因之一。 表级锁 MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 表锁一般是在数据库引擎不支持行锁的时候才会被用到的 。 MDL会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新 。 如何加表锁 显式加表锁和解锁的语句很简单，如下： 123lock tables tb_name read/write;unlock tables; 需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。 举个例子, 如果在某个线程A中执行lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，自然也不能访问其他表。 MDL MDL不需要显式使用，在访问一个表的时候会被自动加上。 当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 查询表级锁争用 查询表级锁的争用可以通过以下参数分析获得： Table_locks_immediate：能够立即获得表级锁的次数 Table_locks_waited： 不能立即获取表级锁而需要等待的次数 查询语句如下： 1show status like 'table_locks_waited' 如果Table_locks_waited的值比较大，则说明存在着较严重的表级锁争用情况。 行级锁 MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB是支持行锁的，这也是MyISAM被InnoDB替代的重要原因之一。 InnoDB的行锁是针对索引加的锁，不是针对记录加的锁。并且该索引不能失效，否则都会从行锁升级为表锁。 在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。 行级锁分为排它锁（写锁）、共享锁（读锁）、间隙锁。 排他锁 排他锁，也称写锁，独占锁，当前写操作没有完成前，它会阻断其他写锁和读锁。 Mysql中的更新语句(update/delete/insert)会自动加上排它锁。 如上图，事务B中的update语句被阻塞了，直到事务A提交才能执行更新操作。 排他锁也可以手动添加，如下： 1select * from user where id=1 for update; 注意以下两点： 行锁是针对索引加锁的，上述例子中id是主键索引。 加了排他锁并不是其他的事务不能读取这行的数据，而是不能再在这行上面加锁了。 间隙锁 当我们用范围条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做”间隙(GAP)”。InnoDB也会对这个”间隙”加锁，这种锁机制就是所谓的间隙锁(Next-Key锁)。 如上图，给id&gt;5中并不存在的数据加上了间隙锁，当插入id=6的数据时被阻塞了。 这是一个坑：若执行的条件是范围过大，则InnoDB会将整个范围内所有的索引键值全部锁定，很容易对性能造成影响 共享锁 共享锁，也称读锁，多用于判断数据是否存在，多个读操作可以同时进行而不会互相影响。当如果事务对读锁进行修改操作，很可能会造成死锁。如下图所示。 分析行锁定 通过检查InnoDB_row_lock 状态变量分析系统上的行锁的争夺情况 。 1show status like 'innodb_row_lock%' innodb_row_lock_current_waits: 当前正在等待锁定的数量。 innodb_row_lock_time: 从系统启动到现在锁定总时间长度；非常重要的参数 innodb_row_lock_time_avg: 每次等待所花平均时间；非常重要的参数。 innodb_row_lock_time_max: 从系统启动到现在等待最常的一次所花的时间； innodb_row_lock_waits: 系统启动后到现在总共等待的次数；非常重要的参数。直接决定优化的方向和策略。 死锁解决方案1、直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置，默认50秒。注意超时时间不能设置太短，如果仅仅是短暂的等待，一旦设置时间很短，很快便解锁了，会出现误伤。 2、发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑，默认开启。 主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。 当并发很高的时候，检测死锁将会消耗大量的资源，因此控制并发也是很重要的一种策略。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[IDEA调试技巧]]></title>
      <url>%2F2020%2F03%2F23%2FIDEA%E8%B0%83%E8%AF%95%E6%8A%80%E5%B7%A7%2F</url>
      <content type="text"><![CDATA[导读 前天面试了一个985高校的实习生，问了他平时用什么开发工具，他想也没想的说IDEA，于是我抛砖引玉的问了一下IDEA的调试用过吧，你说说怎么设置断点条件？那孩子懵了，想了好一会对我说没用过，甚至都没听说过这个。 作为一名资深的老司机，IDEA调试可以说是家常便饭，如果不会debug，我都不信你读过源码，就别和我说原理了，直接pass掉。 基本界面 ① 以Debug模式启动服务，左边的一个按钮则是以Run模式启动。在开发中，我一般会直接启动Debug模式，方便随时调试代码。 ② 断点：在左边行号栏单击左键，或者快捷键Ctrl+F8 打上/取消断点，断点行的颜色可自己去设置。 ③ Debug窗口：访问请求到达第一个断点后，会自动激活Debug窗口。如果没有自动激活，可以去设置里设置。 ④ 调试按钮：一共有8个按钮，调试的主要功能就对应着这几个按钮，鼠标悬停在按钮上可以查看对应的快捷键。 ⑤ 服务按钮：可以在这里关闭/启动服务，设置断点等。 ⑥ 方法调用栈：这里显示了该线程调试所经过的所有方法，勾选右上角的[Show All Frames]按钮，就不会显示其它类库的方法了，否则这里会有一大堆的方法。 ⑦ Variables：在变量区可以查看当前断点之前的当前方法内的变量。 ⑧ Watches：查看变量，可以将Variables区中的变量拖到Watches中查看 。 变量查看 在调试过程中往往需要观察变量的变化来判断业务逻辑，我们可以在以下的四个地方观察。 ① 最常用的变量的观察区域variables ② IDEA中最人性化的地方之一，会将变量的值阴影显示在变量的后面。 ③ watch区域，眼镜的形状，一般不会展开。如下图： 点击’+’号可以新增需要观察的变量，点击’-‘号可以删除。 ④ 鼠标悬停在变量上也会出现变量的值，点击展开即可查看。 计算表达式 在调试业务逻辑的时候一般总会遇到某个条件或者某个变量的计算值的还不知道的情况下就需要判断下一行代码，那么此处就需要用到计算表达式的功能。计算表达式有两种方法，如下： ① 选择需要计算的代码，鼠标右键—-&gt;Evaluate Expression—&gt;Evaluate即可计算。 ② 直接点击计算器形状控件即可弹出计算的窗口，将代码复制进去即可，注意复制进去的代码一定要符合逻辑，比如局部变量一定要是已经声明的。 断点条件设置 对于新手要看Spring源码的话，再遇到调试UserService的doGetBean的方法时可能要崩溃，因为doGetBean在容器启动的时候可能会被调用几十次，你把断点打在doGetBean方法体中能让你生不如死。 设置断点条件有两种方式： ①直接在断点上右键，添加condition条件即可。 ② view breakpoints(ctrl+shift+8)显示所有的断点，在condition中添加条件即可。 异常断点：设置了异常断点后，比如空指针异常，在程序出现需要拦截的异常时会自动定位到指定的行。如下图： ① ctrl+shift+F8显示所有断点，点击+号添加Java Exception Breakpoints。 ② debug运行，一旦有代码出现该异常，会自动定位到指定代码。 线程切换 通常我们在调试的时候，一个请求过来被拦截了，此时想要发起另外一个请求是无法重新发的，因为另外一个请求被阻塞了，只有当前线程执行完成之后才会走其他的线程。在IDEA中可以改变一下阻塞级别，有两种方法： 断点上右键—&gt;选择Thread—-&gt;Make Default，如下图： 显示所有断点(crtl+shift+F8)，选中某一个断点，选择Thread，Make Default即可。如下图： 设置了阻塞级别，此时就可以在线程切换了，如下图： 强制抛异常这是IDEA 2018年加入的新功能，可以直接在调试中抛出指定的异常。使用方法跟上面的弃栈帧类似，右击栈帧并选择Throw Exception，然后输入抛异常的代码，比如throw new NullPointerException，操作如下图： 强制返回 这是IDEA2015版时增加的功能，类似上面的手动抛异常，只不过是返回一个指定值罢了。使用方法跟上面也都类似，右击栈帧并选择Force Return，然后输入要返回的值即可。如果是void的方法那就更简单了，连返回值都不用输。如下图： 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring生命周期]]></title>
      <url>%2F2020%2F03%2F23%2FSpring%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
      <content type="text"><![CDATA[导读 Spring中Bean的生命周期从容器的启动到停止，涉及到的源码主要是在org.springframework.context.support.AbstractApplicationContext.refresh方法中，下面也是围绕其中的逻辑进行讲解。 开撸【1】 prepareRefresh() 内部其实很简单，就是设置一些标志，比如开始时间，激活的状态等。 【2】prepareBeanFactory(beanFactory) 做一些简单的准备工作，此处不再赘述！！！ 【3】postProcessBeanFactory(beanFactory) 主要的作用就是添加了一个后置处理器ServletContextAwareProcessor 【4】invokeBeanFactoryPostProcessors(beanFactory) 调用容器中的所有的BeanFactoryPostProcessor中的postProcessBeanFactory方法，按照优先级调用，主要实现逻辑在org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(org.springframework.beans.factory.config.ConfigurableListableBeanFactory, java.util.List) (1) 执行所有BeanDefinitionRegistryPostProcessor(对BeanFactoryPostProcessor的扩展，运行在普通的实现类之前注册bean)的方法，同样是内部按照优先级进行排序调用 (2) 对剩余的进行按照优先级排序调用，同样是内部进行排序执行 【5】registerBeanPostProcessors(beanFactory) 注册所有的BeanPostProcessor（后置处理器），按照优先级注册，分别是PriorityOrdered，Ordered，普通的，内部的。主要的实现逻辑在PostProcessorRegistrationDelegate.registerBeanPostProcessors() 【6】initMessageSource()注册MessageSource,提供消息国际化等功能 【7】initApplicationEventMulticaster(); 注册事件广播器ApplicationEventMulticaster，用于spring事件的广播和事件监听器的处理 【8】registerListeners() 注册事件监听器ApplicationListener，并且广播一些早期的事件，主要的逻辑在org.springframework.context.support.AbstractApplicationContext.registerListeners 【9】finishBeanFactoryInitialization(beanFactory) 实例化所有非懒加载的Bean，spring生命周期中的主要方法，主要逻辑在org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons，深入进去其实就是getBean()方法创建，详情向下看。 【10】finishRefresh() 主要的功能是发布事件ContextRefreshedEvent 【11】destroyBeans() 容器启动出现异常时销毁Bean 以上就是Spring容器启动的过程，主要的逻辑都在org.springframework.context.support.AbstractApplicationContext#refresh中，其他的都很容易理解，现在我们着重分析一下单例Bean的创建过程，入口是第9步。 实例化单例Bean【1】debug进入，实际主要的逻辑都在org.springframework.beans.factory.support.DefaultListableBeanFactory#preInstantiateSingletons方法中，逻辑如下： 1234567891011121314151617//获取所有注入到ioc容器中的bean定义信息List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames); //循环创建 for (String beanName : beanNames) &#123; RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); //非抽象，单例，非懒加载的bean初始化 if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; //如果是FactoryBean if (isFactoryBean(beanName)) &#123; //getBean Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); //非FactoryBean，getBean else &#123; getBean(beanName); &#125; &#125; &#125; 以上源码总结得知，最终实例化Bean的方法肯定在getBean中的，debug进入，得知doGetBean是大boss，spring源码有趣的是最终的实现都是在doxxxx()。 【2】AbstractBeanFactory#doGetBean，由于篇幅太短，就不贴源码了，只贴关键代码 实例化的主要流程全部都在这里，下面一一解析即可。 (1) Object sharedInstance = getSingleton(beanName) 从早期的缓存中获取，如果存在返回Bean，实例化 （2）BeanFactory parentBeanFactory = getParentBeanFactory() 从父工厂的中获取Bean （3）if (mbd.isSingleton()) 分单例和多例进行分开创建Bean，这里只分析单例Bean的创建 （4）sharedInstance = getSingleton(beanName, () -&gt; { try { return createBean(beanName, mbd, args); } createBean方法创建Bean，进入createBean(） ​ a. Object bean = resolveBeforeInstantiation(beanName, mbdToUse)：执行所有的InstantiationAwareBeanPostProcessor中的postProcessBeforeInstantiation，在实例化之前调用，返回null继续下一步，返回一个bean，那么bean实例化完成，将调用其中的postProcessAfterInstantiation方法 ​ b. Object beanInstance = doCreateBean(beanName, mbdToUse, args)：创建Bean的完成过程 ​ c. 进入doCreateBean，instanceWrapper = createBeanInstance(beanName, mbd, args)：创建Bean的实例 ​ d. populateBean(beanName, mbd, instanceWrapper)：属性装配，执行InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation，再执行postProcessProperties方法。 ​ e. exposedObject = initializeBean(beanName, exposedObject, mbd)：初始化Bean，debug进入 ​ f. invokeAwareMethods(beanName, bean)：调用BeanNameAware，BeanClassLoaderAware，BeanFactoryAware中的对应方法 ​ g. wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName)：调用BeanPostProcessord中的postProcessBeforeInitialization方法 ​ h. invokeInitMethods(beanName, wrappedBean, mbd)：执行InitializingBean中的afterPropertiesSet ​ i. wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName)：调用BeanPostProcessor中的postProcessAfterInitialization方法 总结以上是spring容器从启动到销毁的全部过程，根据源码陈某画了一张生命周期的图，仅供参考，请勿转载！！！ 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[接口限流算法]]></title>
      <url>%2F2020%2F03%2F23%2F%E6%8E%A5%E5%8F%A3%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95%2F</url>
      <content type="text"><![CDATA[导读 前几天和一个朋友讨论了他们公司的系统问题，传统的单体应用，集群部署，他说近期服务的并发量可能会出现瞬时增加的风险，虽然部署了集群，但是通过压测后发现请求延迟仍然是很大，想问问我有什么改进的地方。我沉思了一会，现在去改架构显然是不可能的，于是我给出了一个建议，让他去做个接口限流，这样能够保证瞬时并发量飙高也不会出现请求延迟的问题，用户的体验度也会上去。 至于什么是接口限流？怎么实现接口限流？如何实现单机应用的限流？如何实现分布式应用的限流？本篇文章将会详细阐述。 限流的常见几种算法 常见的限流算法有很多，但是最常用的算法无非以下四种。 固定窗口计数器 固定算法的概念如下 将时间划分为多个窗口 在每个窗口内每有一次请求就将计数器加一 如果计数器超过了限制数量，则本窗口内所有的请求都被丢弃当时间到达下一个窗口时，计数器重置。 固定窗口计数器是最为简单的算法，但这个算法有时会让通过请求量允许为限制的两倍。考虑如下情况：限制 1 秒内最多通过 5 个请求，在第一个窗口的最后半秒内通过了 5 个请求，第二个窗口的前半秒内又通过了 5 个请求。这样看来就是在 1 秒内通过了 10 个请求。 滑动窗口计数器 滑动窗口计数器算法概念如下： 将时间划分为多个区间； 在每个区间内每有一次请求就将计数器加一维持一个时间窗口，占据多个区间； 每经过一个区间的时间，则抛弃最老的一个区间，并纳入最新的一个区间； 如果当前窗口内区间的请求计数总和超过了限制数量，则本窗口内所有的请求都被丢弃。 滑动窗口计数器是通过将窗口再细分，并且按照时间 “ 滑动 “，这种算法避免了固定窗口计数器带来的双倍突发请求，但时间区间的精度越高，算法所需的空间容量就越大。 漏桶算法 漏桶算法概念如下： 将每个请求视作 “ 水滴 “ 放入 “ 漏桶 “ 进行存储； “漏桶 “ 以固定速率向外 “ 漏 “ 出请求来执行如果 “ 漏桶 “ 空了则停止 “ 漏水”； 如果 “ 漏桶 “ 满了则多余的 “ 水滴 “ 会被直接丢弃。 漏桶算法多使用队列实现，服务的请求会存到队列中，服务的提供方则按照固定的速率从队列中取出请求并执行，过多的请求则放在队列中排队或直接拒绝。 漏桶算法的缺陷也很明显，当短时间内有大量的突发请求时，即便此时服务器没有任何负载，每个请求也都得在队列中等待一段时间才能被响应。 令牌桶算法 令牌桶算法概念如下： 令牌以固定速率生成。 生成的令牌放入令牌桶中存放，如果令牌桶满了则多余的令牌会直接丢弃，当请求到达时，会尝试从令牌桶中取令牌，取到了令牌的请求可以执行。 如果桶空了，那么尝试取令牌的请求会被直接丢弃。 令牌桶算法既能够将所有的请求平均分布到时间区间内，又能接受服务器能够承受范围内的突发请求，因此是目前使用较为广泛的一种限流算法。 单体应用实现 在传统的单体应用中限流只需要考虑到多线程即可，使用Google开源工具类guava即可。其中有一个RateLimiter专门实现了单体应用的限流，使用的是令牌桶算法。 单体应用的限流不是本文的重点，官网上现成的API，读者自己去看看即可，这里不再详细解释。 分布式限流 分布式限流和熔断现在有很多的现成的工具，比如Hystrix，Sentinel 等，但是还是有些企业不引用外来类库，因此就需要自己实现。 Redis作为单线程多路复用的特性，很显然能够胜任这项任务。 Redis如何实现 使用令牌桶的算法实现，根据前面的介绍，我们了解到令牌桶算法的基础需要两个个变量，分别是桶容量，产生令牌的速率。 这里我们实现的就是每秒产生的速率加上一个桶容量。但是如何实现呢？这里有几个问题。 需要保存什么数据在redis中？ 当前桶的容量，最新的请求时间 以什么数据结构存储？ 因为是针对接口限流，每个接口的业务逻辑不同，对并发的处理也是不同，因此要细化到每个接口的限流，此时我们选用HashMap的结构，hashKey是接口的唯一id，可以是请求的uri，里面的分别存储当前桶的容量和最新的请求时间。 如何计算需要放令牌？ 根据redis保存的上次的请求时间和当前时间比较，如果相差大于的产生令牌的时间（陈某实现的是1秒）则再次产生令牌，此时的桶容量为当前令牌+产生的令牌 如何保证redis的原子性？ 保证redis的原子性，使用lua脚本即可解决。 有了上述的几个问题，便能很容易的实现。 开撸1、lua脚本如下： 1234567891011121314151617181920212223242526272829303132local ratelimit_info = redis.pcall('HMGET',KEYS[1],'last_time','current_token')local last_time = ratelimit_info[1]local current_token = tonumber(ratelimit_info[2])local max_token = tonumber(ARGV[1])local token_rate = tonumber(ARGV[2])local current_time = tonumber(ARGV[3])if current_token == nil then current_token = max_token last_time = current_timeelse local past_time = current_time-last_time if past_time&gt;1000 then current_token = current_token+token_rate last_time = current_time end ## 防止溢出 if current_token&gt;max_token then current_token = max_token last_time = current_time endendlocal result = 0if(current_token&gt;0) then result = 1 current_token = current_token-1 last_time = current_timeendredis.call('HMSET',KEYS[1],'last_time',last_time,'current_token',current_token)return result 调用lua脚本出四个参数，分别是接口方法唯一id，桶容量，每秒产生令牌的数量，当前请求的时间戳。 2、 SpringBoot代码实现 采用Spring-data-redis实现lua脚本的执行。 Redis序列化配置： 12345678910111213141516171819202122/** * 重新注入模板 */ @Bean(value = "redisTemplate") @Primary public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory)&#123; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;&gt;(); template.setConnectionFactory(redisConnectionFactory); ObjectMapper objectMapper = new ObjectMapper(); objectMapper.setSerializationInclusion(JsonInclude.Include.NON_NULL); objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); //设置序列化方式，key设置string 方式，value设置成json StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); Jackson2JsonRedisSerializer jsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); jsonRedisSerializer.setObjectMapper(objectMapper); template.setEnableDefaultSerializer(false); template.setKeySerializer(stringRedisSerializer); template.setHashKeySerializer(stringRedisSerializer); template.setValueSerializer(jsonRedisSerializer); template.setHashValueSerializer(jsonRedisSerializer); return template; &#125; 限流工具类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * @Description 限流工具类 * @Author CJB * @Date 2020/3/19 17:21 */public class RedisLimiterUtils &#123; private static StringRedisTemplate stringRedisTemplate=ApplicationContextUtils.applicationContext.getBean(StringRedisTemplate.class); /** * lua脚本，限流 */ private final static String TEXT="local ratelimit_info = redis.pcall('HMGET',KEYS[1],'last_time','current_token')\n" + "local last_time = ratelimit_info[1]\n" + "local current_token = tonumber(ratelimit_info[2])\n" + "local max_token = tonumber(ARGV[1])\n" + "local token_rate = tonumber(ARGV[2])\n" + "local current_time = tonumber(ARGV[3])\n" + "if current_token == nil then\n" + " current_token = max_token\n" + " last_time = current_time\n" + "else\n" + " local past_time = current_time-last_time\n" + " \n" + " if past_time&gt;1000 then\n" + "\t current_token = current_token+token_rate\n" + "\t last_time = current_time\n" + " end\n" + "\n" + " if current_token&gt;max_token then\n" + " current_token = max_token\n" + "\tlast_time = current_time\n" + " end\n" + "end\n" + "\n" + "local result = 0\n" + "if(current_token&gt;0) then\n" + " result = 1\n" + " current_token = current_token-1\n" + " last_time = current_time\n" + "end\n" + "redis.call('HMSET',KEYS[1],'last_time',last_time,'current_token',current_token)\n" + "return result"; /** * 获取令牌 * @param key 请求id * @param max 最大能同时承受多少的并发（桶容量） * @param rate 每秒生成多少的令牌 * @return 获取令牌返回true，没有获取返回false */ public static boolean tryAcquire(String key, int max,int rate) &#123; List&lt;String&gt; keyList = new ArrayList&lt;&gt;(1); keyList.add(key); DefaultRedisScript&lt;Long&gt; script = new DefaultRedisScript&lt;&gt;(); script.setResultType(Long.class); script.setScriptText(TEXT); return Long.valueOf(1).equals(stringRedisTemplate.execute(script,keyList,Integer.toString(max), Integer.toString(rate), Long.toString(System.currentTimeMillis()))); &#125;&#125; 采用拦截器+注解的方式实现，注解如下： 123456789101112131415161718192021/** * @Description 限流的注解，标注在类上或者方法上。在方法上的注解会覆盖类上的注解，同@Transactional * @Author CJB * @Date 2020/3/20 13:36 */@Inherited@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface RateLimit &#123; /** * 令牌桶的容量，默认100 * @return */ int capacity() default 100; /** * 每秒钟默认产生令牌数量，默认10个 * @return */ int rate() default 10;&#125; 拦截器如下： 1234567891011121314151617181920212223242526272829303132333435/** * @Description 限流的拦器 * @Author CJB * @Date 2020/3/19 14:34 */@Componentpublic class RateLimiterIntercept implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; if (handler instanceof HandlerMethod)&#123; HandlerMethod handlerMethod=(HandlerMethod)handler; Method method = handlerMethod.getMethod(); /** * 首先获取方法上的注解 */ RateLimit rateLimit = AnnotationUtils.findAnnotation(method, RateLimit.class); //方法上没有标注该注解，尝试获取类上的注解 if (Objects.isNull(rateLimit))&#123; //获取类上的注解 rateLimit = AnnotationUtils.findAnnotation(handlerMethod.getBean().getClass(), RateLimit.class); &#125; //没有标注注解，放行 if (Objects.isNull(rateLimit)) return true; //尝试获取令牌，如果没有令牌了 if (!RedisLimiterUtils.tryAcquire(request.getRequestURI(),rateLimit.capacity(),rateLimit.rate()))&#123; //抛出请求超时的异常 throw new TimeOutException(); &#125; &#125; return true; &#125;&#125; SpringBoot配置拦截器的代码就不贴了，以上就是完整的代码，至此分布式限流就完成了。 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SpringBoot整合多数据源的巨坑]]></title>
      <url>%2F2020%2F03%2F18%2FSpringBoot%E6%95%B4%E5%90%88%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E5%B7%A8%E5%9D%91%2F</url>
      <content type="text"><![CDATA[导读 本篇文章接上篇SpringBoot整合多数据源，你会了吗？，前面文章最后留了几个问题供大家思考，今天一一揭晓。 配置如何优化 上文整合的过程中的还顺带整合Mybatis和TransactionManager，为什么还要重新定义他们呢？SpringBoot不是给我们都配置好了吗？注意，此处优化就是这两个配置去掉，直接用SpringBoot的自动配置，顿时高级了，别人一看你的代码如此简单就实现了多数据源的切换，牛叉不？ 如何去掉？SpringBoot万变不离自动配置类，且看MybatisAutoConfiguration，如下： 123456@org.springframework.context.annotation.Configuration@ConditionalOnClass(&#123; SqlSessionFactory.class, SqlSessionFactoryBean.class &#125;)@ConditionalOnSingleCandidate(DataSource.class)@EnableConfigurationProperties(MybatisProperties.class)@AutoConfigureAfter(DataSourceAutoConfiguration.class)public class MybatisAutoConfiguration implements InitializingBean &#123; 不多帖了，都是废话，看前几行就行了，醒目的一行啊，@ConditionalOnSingleCandidate(DataSource.class)，什么鬼？该注解的意思就是IOC容器中只有一个指定的候选对象才起作用，但是我们注入了几个DataSource，足足三个啊，这还起作用吗？那不废话嘛。 事务管理器也是一样，且看DataSourceTransactionManagerAutoConfiguration，如下： 12345public class DataSourceTransactionManagerAutoConfiguration &#123; @Configuration @ConditionalOnSingleCandidate(DataSource.class) static class DataSourceTransactionManagerConfiguration &#123; 又看到了什么，@ConditionalOnSingleCandidate(DataSource.class)同样的醒目，mmp，这不玩我呢吗。这怎么搞？ 咦，不着急，此时就要看看@ConditionalOnSingleCandidate注解搞了什么，进去看看，有如下的介绍： 1The condition will also match if multiple matching bean instances are already contained in the BeanFactory but a primary candidate has been defined; essentially, the condition match if auto-wiring a bean with the defined type will succeed. 什么鬼，看不懂，英语太差了吧，不着急，陈某给大家推荐一个IDEA插件，文档翻译更加专注于程序员的专业术语，不像xx度翻译，如下： 好了，翻译准确了就知道了，大致意思就是IOC容器中允许你有多个候选对象，但是你必须有一个主（primary）候选对象，顿时灵光一现，这不就是@Primary注解吗，艹，我这也太优秀了吧。 二话不说，直接开撸，轻轻松松一个注解搞定，此时的数据源配置变得简单多了，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * @Description 数据源的配置 * @Author CJB * @Date 2020/3/9 13:45 */@Configuration@MapperScan(basePackages = &#123;"com.vivachek.service.dao","com.vivachek.service.dao2"&#125;)public class DatasourceConfig &#123; /** * 注入数据源1 */ @ConfigurationProperties(prefix = "spring.datasource1") @Bean(value = "dataSource1") public DataSource dataSource1() &#123; return new DruidDataSource(); &#125; /** * 第二个数据源 */ @Bean(name = "dataSource2") @ConfigurationProperties(prefix = "spring.datasource2") public DataSource dataSource2() &#123; return new DruidDataSource(); &#125; /** * 动态数据源 * * @return */ @Bean @Primary public DynamicDataSource dynamicDataSource() &#123; DynamicDataSource dataSource = new DynamicDataSource(); //默认数据源，在没有切换数据源的时候使用该数据源 dataSource.setDefaultTargetDataSource(dataSource2()); HashMap&lt;Object, Object&gt; map = Maps.newHashMap(); map.put("dataSource1", dataSource1()); map.put("dataSource2", dataSource2()); //设置数据源Map，动态切换就是根据key从map中获取 dataSource.setTargetDataSources(map); return dataSource; &#125;&#125; 直接在DynamicDataSource添加了一个@Primary就省去了SqlSessionFactory和TransactionManager的手动配置，是不是很easy并且显得自己很牛叉，太有成就感了….. 好了，牛也吹了，运行一下吧，满怀期待等待30秒…….，what？什么鬼？失败了，抛出了异常，如下： 什么鬼，循环依赖异常，搞什么飞机，一万个草泥马在奔腾在横无际涯的草原上。。。。。。。。 别急，还有后续，关注我，将会定时更新后续文章。另外需要源码的联系我，微信联系方式在个人独立博客【关于我】中，加我注明来意，谢谢。 别忘了点赞哟，多来走动走动呗………. 动态路由数据源添加@Primary报循环依赖异常 前面文章Spring解决循环依赖有说过Spring对于循环依赖是完全能够解决的，没有读过的小伙伴建议看一下，里面详细的讲述了Spring是如何解决循环依赖的，此处就不再赘述了。 既然Spring能够解决循环依赖，为什么这里又会报循环依赖的异常呢？我们不妨跟着代码看看是怎样的循环依赖，如下： 上面两个数据源都是自己定义的，先不用看，那么肯定是DataSourceInitializerInvoker造成的循环依赖了，果不其然，其中确实依赖了DataSource，源码如下： 123456DataSourceInitializerInvoker(ObjectProvider&lt;DataSource&gt; dataSource, DataSourceProperties properties, ApplicationContext applicationContext) &#123; this.dataSource = dataSource; this.properties = properties; this.applicationContext = applicationContext; &#125; what？即使依赖了又怎样？Spring不是可以解决循环依赖吗？别着急下面分析 ObjectProvider应该不陌生吧，其实内部就是从IOC容器中获取Bean而已，但是，转折来了……… ，这是什么，这是构造器，Spring能解决构造器的循环依赖吗？答案是不能，所以原因找到了，这里不再细说了，欲知原因请读Spring解循环依赖 问题找到了，如何解决？此时心中一万个草泥马奔腾，怎么解决呢？ 哈哈，此时插播一条广告，本人的独立博客已经发布了很多文章，感兴趣的可以收藏一下，【关于我】中有我的微信联系方式，欢迎交流。 回到正题，如何解决？很简单，找到这个DataSourceInitializerInvoker是什么时候注入到IOC容器中的，因此我们找到了DataSourceAutoConfiguration，继而找到了DataSourceInitializationConfiguration这个配置类，源码如下： 12345678910111213141516171819202122232425262728@Configuration@ConditionalOnClass(&#123; DataSource.class, EmbeddedDatabaseType.class &#125;)@EnableConfigurationProperties(DataSourceProperties.class)@Import(&#123; DataSourcePoolMetadataProvidersConfiguration.class, DataSourceInitializationConfiguration.class &#125;)public class DataSourceAutoConfiguration &#123; @Configuration @Conditional(EmbeddedDatabaseCondition.class) @ConditionalOnMissingBean(&#123; DataSource.class, XADataSource.class &#125;) @Import(EmbeddedDataSourceConfiguration.class) protected static class EmbeddedDatabaseConfiguration &#123; &#125; @Configuration @Conditional(PooledDataSourceCondition.class) @ConditionalOnMissingBean(&#123; DataSource.class, XADataSource.class &#125;) @Import(&#123; DataSourceConfiguration.Hikari.class, DataSourceConfiguration.Tomcat.class, DataSourceConfiguration.Dbcp2.class, DataSourceConfiguration.Generic.class, DataSourceJmxConfiguration.class &#125;) protected static class PooledDataSourceConfiguration &#123; &#125; &#125; @Configuration@Import(&#123; DataSourceInitializerInvoker.class, DataSourceInitializationConfiguration.Registrar.class &#125;)class DataSourceInitializationConfiguration &#123; 贴了那么多代码谁看的懂？草泥马又奔腾了，可以看到源码中出现了两次@ConditionalOnMissingBean({ DataSource.class, XADataSource.class })，这什么鬼，不多说了，相信读过SpringBoot源码的都知道，这个配置类根本不起作用啊，那还要它干嘛，直接搞掉不就完事了。好了，分析到这里终于知道解决的方案了，搞掉DataSourceAutoConfiguration，怎么搞呢？一个注解搞定。 12//排除配置类@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class&#125;) 问题迎刃而解了，简单不，惊喜不，不好，又奔腾了。。。。 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spring解决循环依赖]]></title>
      <url>%2F2019%2F07%2F17%2FSpring%E8%A7%A3%E5%86%B3%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96%2F</url>
      <content type="text"><![CDATA[导读 前几天发表的文章SpringBoot多数据源动态切换和SpringBoot整合多数据源的巨坑中，提到了一个坑就是动态数据源添加@Primary接口就会造成循环依赖异常，如下图： 这个就是典型的构造器依赖，详情请看上面两篇文章，这里不再详细赘述了。本篇文章将会从源码深入解析Spring是如何解决循环依赖的？为什么不能解决构造器的循环依赖？ 什么是循环依赖 简单的说就是A依赖B，B依赖C，C依赖A这样就构成了循环依赖。 循环依赖分为构造器依赖和属性依赖，众所周知的是Spring能够解决属性的循环依赖（set注入）。下文将从源码角度分析Spring是如何解决属性的循环依赖。 思路 如何解决循环依赖，Spring主要的思路就是依据三级缓存，在实例化A时调用doGetBean，发现A依赖的B的实例，此时调用doGetBean去实例B，实例化的B的时候发现又依赖A，如果不解决这个循环依赖的话此时的doGetBean将会无限循环下去，导致内存溢出，程序奔溃。spring引用了一个早期对象，并且把这个”早期引用”并将其注入到容器中，让B先完成实例化，此时A就获取B的引用，完成实例化。 三级缓存 Spring能够轻松的解决属性的循环依赖正式用到了三级缓存，在AbstractBeanFactory中有详细的注释。 12345678/**一级缓存，用于存放完全初始化好的 bean，从该缓存中取出的 bean 可以直接使用*/private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);/**三级缓存 存放 bean 工厂对象，用于解决循环依赖*/private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16);/**二级缓存 存放原始的 bean 对象（尚未填充属性），用于解决循环依赖*/private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;&gt;(16); 一级缓存：singletonObjects，存放完全实例化属性赋值完成的Bean，直接可以使用。 二级缓存：earlySingletonObjects，存放早期Bean的引用，尚未属性装配的Bean 三级缓存：singletonFactories，三级缓存，存放实例化完成的Bean工厂。 开撸 先上一张流程图看看Spring是如何解决循环依赖的 上图标记蓝色的部分都是涉及到三级缓存的操作，下面我们一个一个方法解析 【1】 getSingleton(beanName)：源码如下： 1234567891011121314151617181920212223242526272829303132333435 //查询缓存 Object sharedInstance = getSingleton(beanName); //缓存中存在并且args是null if (sharedInstance != null &amp;&amp; args == null) &#123; //.......省略部分代码 //直接获取Bean实例 bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; //getSingleton源码，DefaultSingletonBeanRegistry#getSingletonprotected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; //先从一级缓存中获取已经实例化属性赋值完成的Bean Object singletonObject = this.singletonObjects.get(beanName); //一级缓存不存在，并且Bean正处于创建的过程中 if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; //从二级缓存中查询，获取Bean的早期引用，实例化完成但是未赋值完成的Bean singletonObject = this.earlySingletonObjects.get(beanName); //二级缓存中不存在，并且允许创建早期引用（二级缓存中添加） if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; //从三级缓存中查询，实例化完成，属性未装配完成 ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); //二级缓存中添加 this.earlySingletonObjects.put(beanName, singletonObject); //从三级缓存中移除 this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return singletonObject; &#125; 从源码可以得知，doGetBean最初是查询缓存，一二三级缓存全部查询，如果三级缓存存在则将Bean早期引用存放在二级缓存中并移除三级缓存。（升级为二级缓存） 【2】addSingletonFactory：源码如下 123456789101112131415161718192021222324252627282930313233 //中间省略部分代码。。。。。 //创建Bean的源码，在AbstractAutowireCapableBeanFactory#doCreateBean方法中 if (instanceWrapper == null) &#123; //实例化Bean instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; //允许提前暴露 if (earlySingletonExposure) &#123; //添加到三级缓存中 addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean)); &#125; try &#123; //属性装配，属性赋值的时候，如果有发现属性引用了另外一个Bean，则调用getBean方法 populateBean(beanName, mbd, instanceWrapper); //初始化Bean，调用init-method，afterproperties方法等操作 exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; &#125;//添加到三级缓存的源码，在DefaultSingletonBeanRegistry#addSingletonFactoryprotected void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; synchronized (this.singletonObjects) &#123; //一级缓存中不存在 if (!this.singletonObjects.containsKey(beanName)) &#123; //放入三级缓存 this.singletonFactories.put(beanName, singletonFactory); //从二级缓存中移除， this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); &#125; &#125; &#125; 从源码得知，Bean在实例化完成之后会直接将未装配的Bean工厂存放在三级缓存中，并且移除二级缓存 【3】addSingleton：源码如下： 123456789101112131415161718192021//获取单例对象的方法，DefaultSingletonBeanRegistry#getSingleton//调用createBean实例化BeansingletonObject = singletonFactory.getObject();//。。。。中间省略部分代码 //doCreateBean之后才调用，实例化，属性赋值完成的Bean装入一级缓存，可以直接使用的BeanaddSingleton(beanName, singletonObject);//addSingleton源码，在DefaultSingletonBeanRegistry#addSingleton方法中protected void addSingleton(String beanName, Object singletonObject) &#123; synchronized (this.singletonObjects) &#123; //一级缓存中添加 this.singletonObjects.put(beanName, singletonObject); //移除三级缓存 this.singletonFactories.remove(beanName); //移除二级缓存 this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); &#125; &#125; 总之一句话，Bean添加到一级缓存，移除二三级缓存。 扩展 【1】为什么Spring不能解决构造器的循环依赖？ 从流程图应该不难看出来，在Bean调用构造器实例化之前，一二三级缓存并没有Bean的任何相关信息，在实例化之后才放入三级缓存中，因此当getBean的时候缓存并没有命中，这样就抛出了循环依赖的异常了。 【2】为什么多实例Bean不能解决循环依赖？ 多实例Bean是每次创建都会调用doGetBean方法，根本没有使用一二三级缓存，肯定不能解决循环依赖。 总结 根据以上的分析，大概清楚了Spring是如何解决循环依赖的。假设A依赖B，B依赖A（注意：这里是set属性依赖）分以下步骤执行： A依次执行doGetBean、查询缓存、createBean创建实例，实例化完成放入三级缓存singletonFactories中，接着执行populateBean方法装配属性，但是发现有一个属性是B的对象。 因此再次调用doGetBean方法创建B的实例，依次执行doGetBean、查询缓存、createBean创建实例，实例化完成之后放入三级缓存singletonFactories中，执行populateBean装配属性，但是此时发现有一个属性是A对象。 因此再次调用doGetBean创建A的实例，但是执行到getSingleton查询缓存的时候，从三级缓存中查询到了A的实例(早期引用，未完成属性装配)，此时直接返回A，不用执行后续的流程创建A了，那么B就完成了属性装配，此时是一个完整的对象放入到一级缓存singletonObjects中。 B创建完成了，则A自然完成了属性装配，也创建完成放入了一级缓存singletonObjects中。 Spring三级缓存的应用完美的解决了循环依赖的问题，下面是循环依赖的解决流程图。 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[设计模式之适配器模式]]></title>
      <url>%2F2018%2F04%2F16%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[结构型模式之适配器模式定义 适配器模式用于将一个接口转化成客户想要的另一个接口，使接口不兼容的那些类可以一起工作，其别名为包装器(Wrapper)。适配器模式既可以作为类结构型模式，也可以作为对象结构型模式。 Target（目标抽象类）：目标抽象类定义客户所需接口，可以是一个抽象类或接口，也可以是具体类。 Adapter（适配器类）：适配器可以调用另一个接口，作为一个转换器，对Adaptee和Target进行适配，适配器类是适配器模式的核心，在对象适配器中，它通过继承Target并关联一个Adaptee对象使二者产生联系。 Adaptee（适配者类）：适配者即被适配的角色，它定义了一个已经存在的接口，这个接口需要适配，适配者类一般是一个具体类，包含了客户希望使用的业务方法，在某些情况下可能没有适配者类的源代码。 根据对象适配器模式结构图，在对象适配器中，客户端需要调用request()方法，而适配者类Adaptee没有该方法，但是它所提供的specificRequest()方法却是客户端所需要的。为了使客户端能够使用适配者类，需要提供一个包装类Adapter，即适配器类。这个包装类包装了一个适配者的实例，从而将客户端与适配者衔接起来，在适配器的request()方法中调用适配者的specificRequest()方法。因为适配器类与适配者类是关联关系（也可称之为委派关系），所以这种适配器模式称为对象适配器模式 类适配器 类适配器是继承适配者类实现的，其中对象适配器是使用组合的方式实现的，就是适配者类作为适配器类的成员变量而实现的 一般目标抽象类是一个接口，适配者类一般是一个具体的实现类，有时候甚至不知道其中的源代码，因此需要适配器类将适配者类转换成适合用户的目标类 实例 我们知道笔记本充电的电压是5v，但是我们的高压电是220v，那么我们此时就需要一个适配器将这个220v电压转换成为5v的电压给笔记本充电 这里的220v电压就是适配者类，即是需要转换的类 5v电压是目标抽象类，由适配器将220v转换而来 这里的适配器类的主要功能就是将220v电压转换成5v电压 目标接口(5v电压) 1234567/* * 接口为5v电压的接口 ， 这个目标抽象类 */public interface Power5 &#123; void getPower5();&#125; 220v电压的类（这里是一个具体的类，适配者类） 12345public class Power220 &#123; public void getPower220()&#123; System.out.println("正在输出220v电压....."); &#125;&#125; 适配器类（将220v电压转换成5v） 1234567891011121314151617181920212223242526/* * 适配器类，主要的目的就是将220v电压转换为5v的电压供笔记本充电 * 其中Power5是目标抽象接口，是最终需要的接口，Power220是一个适配者类，是已经存在的，只需要适配器转换即可 */public class AdapterPower5 extends Power220 implements Power5 &#123; /** * 重载Power5中的方法，获取需要的5v电压 * 过程： 先获取220v电压，然后进行转换即可 * */ @Override public void getPower5() &#123; super.getPower220(); //首先获取220v电压 this.transform(); //将220v电压转换成5v的电压 System.out.println("获取5v电压......."); &#125; /* * 将220v电压转换成5v电压的方法 */ public void transform() &#123; System.out.println("现在将220v电压转换成5v电压......."); &#125;&#125; 笔记本充电的类 12345678910111213/* * 笔记本类 */public class NoteBook &#123; /** * 笔记本充电的方法 * @param power5 电压为5v的对象 */ public void PowerOn(Power5 power5)&#123; power5.getPower5(); //获取5v电压 System.out.println("笔记本获取了5v的电压，正在开始充电......"); &#125;&#125; 测试类 123456public class Client &#123; public static void main(String[] args) &#123; NoteBook noteBook=new NoteBook(); //创建笔记本的类 noteBook.PowerOn(new AdapterPower5()); //调用笔记本充电的类 &#125;&#125; 对象适配器 对象适配器是将适配者类作为适配器类的成员变量并不是继承，这个是一种组合方式 这种方式使用的更加普遍 实例 这里的实例还是前面的例子 这里唯一不同的就是适配器类，不是继承适配者类，而是使用组合的方式 1234567891011121314151617181920212223242526/* * 适配器类，这个是对象适配器，适配者类是作为成员变量存在，是组合关系 */public class Adapter implements Power5 &#123; private Power220 power; //220v电压类的对象，作为成员变量 /* * 构造方法，主要是为类初始化Power220v的对象 */ public Adapter(Power220 power)&#123; this.power=power; &#125; @Override public void getPower5() &#123; power.getPower220(); //获取220v电压 transform(); //转换电压 System.out.println("正在输出5v电压......."); &#125; public void transform()&#123; System.out.println("将220v电压转换成5v的电压......"); &#125;&#125; 总结 类适配器是使用类继承的方式，适配器类继承适配者类(不提倡使用) 对象适配器使用的是一种组合的方式，将适配者类作为其中的成员变量，那么也是可以实现（提倡使用） 麻烦支持下博主的广告事业，点击下即可 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[设计模式之桥接模式]]></title>
      <url>%2F2018%2F04%2F16%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[结构型模式之桥接模式 桥接模式是一种很实用的结构型设计模式，如果软件系统中某个类存在两个独立变化的维度，通过该模式可以将这两个维度分离出来，使两者可以独立扩展，让系统更加符合“单一职责原则”。与多层继承方案不同，它将两个独立变化的维度设计为两个独立的继承等级结构，并且在抽象层建立一个抽象关联，该关联关系类似一条连接两个独立继承结构的桥，故名桥接模式。 桥接模式用一种巧妙的方式处理多层继承存在的问题，用抽象关联取代了传统的多层继承，将类之间的静态继承关系转换为动态的对象组合关系，使得系统更加灵活，并易于扩展，同时有效控制了系统中类的个数。桥接定义如下： 桥接模式(Bridge Pattern)：将抽象部分与它的实现部分分离，使它们都可以独立地变化。它是一种对象结构型模式，又称为柄体(Handle and Body)模式或接口(Interface)模式。 在桥接模式结构图中包含如下几个角色： Abstraction（抽象类）：用于定义抽象类的接口，它一般是抽象类而不是接口，其中定义了一个Implementor（实现类接口）类型的对象并可以维护该对象，它与Implementor之间具有关联关系，它既可以包含抽象业务方法，也可以包含具体业务方法。 RefinedAbstraction（扩充抽象类）：扩充由Abstraction定义的接口，通常情况下它不再是抽象类而是具体类，它实现了在Abstraction中声明的抽象业务方法，在RefinedAbstraction中可以调用在Implementor中定义的业务方法。 Implementor（实现类接口）：定义实现类的接口，这个接口不一定要与Abstraction的接口完全一致，事实上这两个接口可以完全不同，一般而言，Implementor接口仅提供基本操作，而Abstraction定义的接口可能会做更多更复杂的操作。Implementor接口对这些基本操作进行了声明，而具体实现交给其子类。通过关联关系，在Abstraction中不仅拥有自己的方法，还可以调用到Implementor中定义的方法，使用关联关系来替代继承关系。 ConcreteImplementor（具体实现类）：具体实现Implementor接口，在不同的ConcreteImplementor中提供基本操作的不同实现，在程序运行时，ConcreteImplementor对象将替换其父类对象，提供给抽象类具体的业务操作方法。 实例 从上面的这个实例我们可以看出，如果使用多层继承的话，那么我们可以定义是三个抽象类（台式机，笔记本，平板电脑），在这个三个抽象类的下面每个都有三个不同品牌的具体实现类，那么总共要有3x3=9个具体的实现类。不仅仅是类的数量多，在扩展性能上也是成倍的增加，如果想要添加一个品牌，那么需要添加三个类，这个是极其浪费的。 针对上面的缺点，我们可以使用桥接模式，将电脑分类，品牌分类分成两个维度，如下图： 其中Computer是一个抽象类，不是接口，其中Brand（品牌）是其中的成员变量，我们就完成了一个电脑具有不同品牌，那么如果我们想添加一个品牌，就只是添加一个具体的实现类即可，就不需要添加三个了。 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[设计模式之原型模式]]></title>
      <url>%2F2018%2F04%2F16%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[创建型模式之原型模式定义 原型模式（Prototype Pattern）是用于创建重复的对象，同时又能保证性能。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 这种模式是实现了一个原型接口，该接口用于创建当前对象的克隆。当直接创建对象的代价比较大时，则采用这种模式。例如，一个对象需要 在一个高代价的数据库操作之后被创建。我们可以缓存该对象，在下一个请求时返回它的克隆，在需要的时候更新数据库，以此来减少数据库调用。 原型模式可以分为浅克隆和深度克隆 角色 java语言中实现克隆的两种方式 直接创建一个对象，然后设置成员变量的值 123Obj obj=new Obj(); //创建一个新的对象obj.setName(this.name); //设置其中变量的值obj.setAge(this.age); 实现cloneable接口 浅克隆 如果克隆的对象的成员变量是值类型的，比如int，double那么使用浅克隆就可以实现克隆完整的原型对象，但是如果其中的成员变量有引用类型的，那么这个引用类型的克隆过去的其实是地址，克隆对象的这个引用类型变量改变了，那么原来变量的值也是会改变的。 简单的说，浅克隆只能复制值类型的，对于引用类型的数据只能复制地址 实例 一个公司出版周报，那么这个周报的格式一般是相同的，只是将其中的内容稍作修改即可。但是一开始没有这个原型，员工每周都需要重新手写这个周报，现在有了这个周报的原型，只需要在这个clone这个原型，然后在其基础上修改即可。 其中的Cloneable就是抽象原型类 附件类（这个是一个引用类型的对象，验证浅克隆只是复制其中的地址，如果两个对象中的任何一个改变了这个变量的值，那么另外一个也会随之改变） 12345678910111213141516171819/* * 附件类，这个是周报的附件 */public class Attachment &#123; private String name; // 名称 public Attachment(String name) &#123; super(); this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 周报的类（其中实现了Cloneable接口） 其中的clone()方法返回的就是一个克隆的对象，因此我们调用这个方法克隆一个新的对象 1234567891011121314151617181920212223242526272829303132333435/* * 这个是周报类，这个类是实现接口Prototype这个接口的 */public class WeeklyLog implements Cloneable &#123; private String name; // 姓名 private String date; // 日期 private String content; // 内容 private Attachment attachment; //附件，是一个引用对象，这个只能实现浅克隆 public WeeklyLog() &#123; super(); &#125; /** * 构造方法 */ public WeeklyLog(String name, String date, String content) &#123; super(); this.name = name; this.date = date; this.content = content; &#125; /** * 提供一个clone方法，返回的是一个clone对象 */ public WeeklyLog clone() &#123; Object object = null; // 创建一个Object对象 try &#123; object = super.clone(); // 直接调用clone方法，复制对象 return (WeeklyLog) object; // 返回即可 &#125; catch (CloneNotSupportedException e) &#123; System.out.println("这个对象不能复制....."); return null; &#125; &#125;&#125; 测试类 测试浅克隆的值类型是是否完成复制了 测试引用类型的值能否完成克隆，还是只是复制了一个引用地址 从结果来看，对象是完成复制了，因为判断两个对象的地址是不一样的，但是其中的引用类型的成员变量没有完成复制，只是复制了一个地址 12345678910111213141516public class Client &#123; public static void main(String[] args) throws CloneNotSupportedException &#123; WeeklyLog p1 = new WeeklyLog("陈加兵", "第一周", "获得劳动模范的称号..."); // 创建一个对象 Attachment attachment = new Attachment("消息"); p1.setAttachment(attachment); // 添加附件 WeeklyLog p2 = p1.clone(); System.out.println(p1 == p2); // 判断是否正确 p2.setName("Jack"); // 修改P2对象的内容 p2.setDate("第二周"); p2.setContent("工作认真....."); System.out.println(p2.getName()); // 返回true，可以知道这两个附件的地址是一样的 System.out.println(p1.getAttachment() == p2.getAttachment()); &#125;&#125; 总结 浅克隆对于值类型的数据可以复制成功，但是对于引用卡类型的数据只能复制一个地址，如果一个对象中的引用类型的变量的值改变了，那么另外一个也会随之改变 深度克隆 浅克隆只能完成复制值类型，深度克隆可以完成复制引用类型和值类型 条件 引用类型的变量类实现序列化(实现Serializabl接口） 需要克隆的类实现序列化(实现Serializable接口) 为什么实现序列化 因为深度克隆的实现的原理是使用输入和输出流，如果想要将一个对象使用输入和输出流克隆，必须序列化。 实现 附件类(引用类型的成员变量，实现序列化) 1234567891011121314151617/* * 附件类，这个是周报的附件 */public class Attachment implements Serializable&#123; private static final long serialVersionUID = -799959163401886355L; private String name; // 名称 public Attachment(String name) &#123; super(); this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 周报类（需要克隆的类，因为其中有引用类型的成员变量，因此需要实现序列化) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/* * 这个是周报类，这个类是实现接口Prototype这个接口的 */public class WeeklyLog implements Serializable &#123; private static final long serialVersionUID = -8782492113927035907L; private String name; // 姓名 private String date; // 日期 private String content; // 内容 private Attachment attachment; // 附件，是一个引用对象，这个只能实现浅克隆 public WeeklyLog() &#123; super(); &#125; /** * 构造方法 */ public WeeklyLog(String name, String date, String content) &#123; super(); this.name = name; this.date = date; this.content = content; &#125; /** * 提供一个clone方法，返回的是一个clone对象 */ public WeeklyLog clone() &#123; // 将对象写入到对象流中 ByteArrayOutputStream arrayOutputStream = new ByteArrayOutputStream(); try &#123; ObjectOutputStream objectOutputStream = new ObjectOutputStream( arrayOutputStream); // 创建对象输出流 objectOutputStream.writeObject(this); // 将这个类的对象写入到输出流中 &#125; catch (IOException e) &#123; e.printStackTrace(); return null; &#125; // 将对象从流中读出 ByteArrayInputStream arrayInputStream = new ByteArrayInputStream( arrayOutputStream.toByteArray()); WeeklyLog weeklyLog; try &#123; ObjectInputStream objectInputStream = new ObjectInputStream( arrayInputStream);// 新建对象输入流 weeklyLog = (WeeklyLog) objectInputStream.readObject(); // 读取对象从流中 return weeklyLog; &#125; catch (IOException | ClassNotFoundException e) &#123; e.printStackTrace(); return null; &#125; &#125;&#125; 测试类 从中可以看出其中的附件地址是不同的，如果一个对象的附件变量改变了，那么另外一个将保持不变，因此实现了深度克隆，是两个完全不同的对象 12345678910111213141516public class Client &#123; public static void main(String[] args) throws CloneNotSupportedException &#123; WeeklyLog p1 = new WeeklyLog("陈加兵", "第一周", "获得劳动模范的称号..."); // 创建一个对象 Attachment attachment = new Attachment("消息"); p1.setAttachment(attachment); // 添加附件 WeeklyLog p2 = p1.clone(); System.out.println(p1 == p2); // 判断是否正确 p2.setName("Jack"); // 修改P2对象的内容 p2.setDate("第二周"); p2.setContent("工作认真....."); System.out.println(p2.getName()); //返回false，可以看出这个是不同的地址，因此完成了深克隆 System.out.println(p1.getAttachment() == p2.getAttachment()); &#125;&#125; 总结 因为深度克隆使用的是将对象写入输入和输出流中的，因此需要实现序列化，否则将不能完成 总结 浅克隆只能克隆对象中的值类型，不能克隆有引用类型成员变量的对象 使用深度克隆： 引用类型的成员变量的类必须实现序列化 需要克隆的类必须实现序列化 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[设计模式之建造模式]]></title>
      <url>%2F2018%2F04%2F16%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BB%BA%E9%80%A0%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[创建型模式之建造者模式定义 建造者模式(Builder Pattern)：将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。建造者模式是一种对象创建型模式。 简单说，建造者的功能就是先构造复杂对象的每一个部件，指挥者的功能就是将这些部件以一定的步骤组装起来，形成一个具有一定功能的产品或者对象。当然这个步骤是透明的对于客户端。 建造者模式一步一步创建一个复杂的对象，它允许用户只通过指定复杂对象的类型和内容就可以构建它们，用户不需要知道内部的具体构建细节。建造者模式结构如图8-2所示： 实例 下面是一个组装汽车的例子，其中汽车由发动机和轮胎组成，那么我们只需要组装轮胎，发动机即可组装完成一个汽车。 汽车包括轮胎，引擎，我们通常在组装汽车的时候一般都是一步一步的组装，比如先装引擎，后装轮胎。使用建造者模式就是将建造汽车的这个过程抽离成几个不同的过程，比如建造引擎和建造轮胎就是两个过程。 轮胎的JavaBean 12345678910111213141516171819/* * 轮胎 */class Tyre &#123; private String name; public Tyre(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 引擎的JavaBean 1234567891011121314151617181920/* * 引擎 */class Engine &#123; private String name; public Engine(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 汽车的JavaBean(汽车包含轮胎和引擎，因此使用聚合的关系) 123456789101112131415161718192021222324/* * 汽车的类 */public class Car &#123; private Tyre tyre; // 轮胎 private Engine engine; // 引擎 public Tyre getTyre() &#123; return tyre; &#125; public void setTyre(Tyre tyre) &#123; this.tyre = tyre; &#125; public Engine getEngine() &#123; return engine; &#125; public void setEngine(Engine engine) &#123; this.engine = engine; &#125;&#125; 抽象建造者(实际上是一个接口，其中定义了建造轮胎和引擎的方法) 1234567891011public interface Builder &#123; /** * 构造引擎的方法 */ Engine buliderEngine(); /** * 构造轮胎的方法 */ Tyre builderTyre();&#125; 具体的建造者(实现了抽象建造者，实现建造轮胎和引擎的详细过程) 12345678910111213141516/* * 具体的建造者，主要是构造汽车的部件 */public class BuilderCar implements Builder &#123; @Override public Engine buliderEngine() &#123; System.out.println("构造汽车发动机"); return new Engine("傻逼牌发动机"); &#125; @Override public Tyre builderTyre() &#123; System.out.println("构造汽车轮胎"); return new Tyre("傻逼牌轮胎"); &#125;&#125; 抽象指挥者(定义了一个构造汽车的方法)，指挥者的作用就是按照一定步骤将构造者建造的部件组装起来 123456/* * 指挥者的接口，用来按照顺序组装汽车 */public interface Director &#123; Car CreateCar();&#125; 具体的指挥者(实现了指挥者接口) 1234567891011121314151617181920212223242526/* * 指挥者的实现类 */public class DirectorCar implements Director &#123; private Builder builder; // 建造者的对象 /** * 构造方法，主要用来初始化建造者对象 * * @param builder Builder的对象 */ public DirectorCar(Builder builder) &#123; this.builder = builder; &#125; @Override public Car CreateCar() &#123; Car car = new Car(); // 创建汽车对象 Engine engine = builder.buliderEngine(); // 构建发动机 Tyre tyre = builder.builderTyre(); // 构造轮胎 car.setEngine(engine); // 设置属性 car.setTyre(tyre); // 设置属性 return car; // 返回构造好的汽车 &#125;&#125; 测试类 12345678public class Client &#123; public static void main(String[] args) &#123; Director director = new DirectorCar(new BuilderCar()); // 创建指挥者的对象 Car car = director.CreateCar(); // 获取组装完成的 System.out.println(car.getEngine().getName()); // 输出引擎的名字 System.out.println(car.getTyre().getName()); // 输出轮胎的名字 &#125;&#125; 适用场景 基本部件不变，但是其中的组合经常变化的情况 比如你去肯德基点餐，汉堡，可乐，鸡翅这些食物是不变的，但是套餐的组合是经常变化的，建造者模式的指挥者就是将这些部件按照一定步骤将其组合起来的。 java中StringBuilder 需要生成的对象具有复杂的内部结构 复杂的内部结构，我们可以使用建造者模式将其分离，先将其中的各个小的部件组装成功，然后由指挥者按照一定的步骤将其组装成一个复杂的对象 需要生成的对象内部属性本身相互依赖。 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[设计模式之代理模式]]></title>
      <url>%2F2018%2F04%2F16%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[结构型模式之代理模式(静态代理) 由于某些原因，客户端不想或不能直接访问一个对象，此时可以通过一个称之为“代理”的第三者来实现间接访问，该方案对应的设计模式被称为代理模式。 代理其实是实现简介访问的媒介，当然在代理类中还可以在执行代理操作之前，之后，之中，环绕等执行相关动作。Spring 中面向切面编程就是这个原理 代理模式是一种应用很广泛的结构型设计模式，而且变化形式非常多，常见的代理形式包括远程代理、保护代理、虚拟代理、缓冲代理、智能引用代理等，后面将学习这些不同的代理形式 当使用代理类的时候， 真实类中的信息对用户来说是透明的(不可见的) 主要就是用于对象的间接访问提供了一个方案，可以对对象的访问进行控制 定义 Subject（抽象主题角色）：它声明了真实主题和代理主题的共同接口，这样一来在任何使用真实主题的地方都可以使用代理主题，客户端通常需要针对抽象主题角色进行编程。 Proxy（代理主题角色）：它包含了对真实主题的引用，从而可以在任何时候操作真实主题对象；在代理主题角色中提供一个与真实主题角色相同的接口，以便在任何时候都可以替代真实主题；代理主题角色还可以控制对真实主题的使用，负责在需要的时候创建和删除真实主题对象，并对真实主题对象的使用加以约束。通常，在代理主题角色中，客户端在调用所引用的真实主题操作之前或之后还需要执行其他操作，而不仅仅是单纯调用真实主题对象中的操作。 RealSubject（真实主题角色）：它定义了代理角色所代表的真实对象，在真实主题角色中实现了真实的业务操作，客户端可以通过代理主题角色间接调用真实主题角色中定义的操作。 实例第一个例子 需求： 我们知道mac笔记本是在美国生产的，那么如果中国供销商想要卖mac笔记本，那么必须从美国供销商那里先进货，然后中国的顾客才可以在中国供销商买mac。这里的中国供销商就相当于代理，美国供销商就相当于真实主题角色 Mac笔记本抽象接口(相当于其中的抽象主题) 123456/* * 苹果笔记本的接口，其中有一个方法实现了买笔记本的动作 */public interface MacBook &#123; public void buy(); //购买笔记本的行为&#125; 美国供销商(相当于这里RealSubject) 1234567891011/* * 美国的笔记本，实现了MacBook接口，表示在美国买笔记本 */public class USAMac implements MacBook &#123; @Override public void buy() &#123; System.out.println("在美国买笔记本"); &#125;&#125; 中国供销商(相当于这里的代理角色) 我们可以看到我们在使用代理模式的时候可以在之前和之后进行操作12345678910111213141516171819202122232425262728293031/* * 中国的笔记本，实现了MacBook 表示在中国买笔记本 * 但是中国想要买到苹果笔记本，那么还是需要先从美国进货，因此中国只是一个中间的代理作用而已 * 当然代理的最大作用就是在代理之前、之后、之中执行相关的操作，这就是面向切面编程的原理 */public class ChinaMac implements MacBook &#123; private MacBook mcBook=new USAMac(); //创建USAMac的对象 /** * 在购买之前执行的操作 */ public void preBuy()&#123; System.out.println("购买之前执行的操作"); &#125; /** * 在购买之后执行的操作 */ public void afterBuy()&#123; System.out.println("购买之后执行的操作"); &#125; @Override public void buy() &#123; this.preBuy(); //之前执行的操作 mcBook.buy(); //在美国买笔记本 System.out.println("在中国买笔记本"); this.afterBuy(); //之后执行的操作 &#125;&#125; 测试类 我们在使用的时候直接使用代理类即可，我们根本不知道在真实类的使用，完全是代理类为我们提供了 1234567public class Client &#123; public static void main(String[] args) &#123; MacBook macBook=new ChinaMac(); //创建ChinaMac对象，在中国买笔记本 macBook.buy(); //直接在中国买笔记本 &#125;&#125; 第二个例子 我们登录一个网站的服务器端的验证步骤： 读取用户名和密码 验证用户名和密码 记录到日志中 这里的验证密码和记录到日志中可以在代理类中实现，在用户执行操作之前需要读取用户名和密码，并且验证，在操作之后需要将用户的一些操作记录到日志中。其实这里的真实用户需要做的只是执行自己的操作，而验证和记录都是交给代理类实现的。 实现 用户接口(User) 1234567/* * 用户的抽象类 */public interface User &#123; public void DoAction(); //执行动作&#125; 真实的用户类（实现了用户接口） 主要的做的就是执行自己的操作 12345678910111213141516public class RealUser implements User &#123; public String name; public String password; public RealUser(String name, String password) &#123; this.name = name; this.password = password; &#125; public RealUser() &#123;&#125; /* * 执行一些操作 */ @Override public void DoAction() &#123; System.out.println("开始执行操作......"); &#125;&#125; 代理类(实现了User接口) 在执行操作之前验证密码和用户名是否正确 在执行操作之后记录到日志中 实际上这里就是面向切面编程 12345678910111213141516171819202122232425262728293031323334353637383940public class ProxUser implements User &#123; private RealUser user; // 真实用户的对象 /** * 创建对象 * @param name 姓名 * @param password 密码 */ public ProxUser(String name, String password) &#123; user = new RealUser(name, password); &#125; @Override public void DoAction() &#123; //验证用户名和密码 if (Validate()) &#123; user.DoAction(); //调用真实用户的DoAction方法执行相关操作 logger(); //调用日志记录信息 &#125; else &#123; System.out.println("请重新登录......."); &#125; &#125; /* * 验证用户的用户名和密码 */ public Boolean Validate() &#123; if ("陈加兵".equals(user.name) &amp;&amp; "123456".equals(user.password)) &#123; return true; &#125; return false; &#125; /** * 添加日志记录信息 */ public void logger() &#123; System.out.println(user.name + "登录成功......"); &#125;&#125; 测试类 实际上执行了验证用户名和密码，记录日志的操作，但是对于客户端来说只能看到自己执行的操作 123456public class Client &#123; public static void main(String[] args) &#123; ProxUser proxUser=new ProxUser("陈加兵", "123456"); //创建代理对象 proxUser.DoAction(); //执行操作，实际执行了验证信息，doaction(),日志记录这个三个动作 &#125;&#125; 缺点 如果增加一个接口就需要增加一个代理类，如果是要增加很多，那么就要增加很多代理类，代码将会重复 解决方法 下面我们将会讲解到动态代理，仅仅需要一个代理类即可 结构型模式之动态代理模式 前面我们说的代理模式其实是属于静态代理模式，就是说在程序执行之前已经写好了代理类，但是缺点也是说过，必须为每个接口都实现一个代理类，如果有多个接口需要代理，那么代码肯定是要重复的，因此就需要动态代理了。 动态代理可以实现多个接口共用一个代理类，只需要改变初始化的参数即可，可以省去很多的重复的代码。 JDK的动态代理需要一个类一个接口，分别为Proxy和InvocationHandler 主要原理就是利用了反射的原理 InvocationHandler 这个是代理类必须实现的接口，其中有一个方法public Object invoke(Object proxy,Method method,Object[] args) Object proxy：指被代理的对象。 Method method：要调用的方法 Object[] args：方法调用时所需要的参数 Proxy Proxy类是专门完成代理的操作类，可以通过此类为一个或多个接口动态地生成实现类，此类提供了如下的操作方法：public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) ClassLoader loader：类加载器 Class&lt;?&gt;[] interfaces：得到全部的接口 InvocationHandler h：得到InvocationHandler接口的子类实例 实例 肯德基的接口 123456/* * 肯德基的接口，其中一个eat方法 */public interface IKFC &#123; public void eat();&#125; 肯德基的实现类(RealSubject) 1234567891011/* * IKFC的实现类 */public class KFC implements IKFC &#123; @Override public void eat() &#123; System.out.println("我在肯德基吃了饭......"); &#125;&#125; 苹果笔记本的接口 123456/* * 苹果笔记本的接口 */public interface MacBook &#123; public void buy();&#125; 美国供销商的类(RealSubject) 12345678910/* * 美国笔记本的类，实现了MacBook接口 */public class USAMacBook implements MacBook &#123; @Override public void buy() &#123; System.out.println("在美国买了一个苹果电脑......"); &#125;&#125; 动态代理的类（实现了InvocationHandler接口） 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;/** * 这个是代理类，实现了InvocationHandler接口 * */public class ProxyHandler implements InvocationHandler &#123; private Object Realobject; //被代理的对象 //构造方法，用来初始化被代理的对象 public ProxyHandler(Object obj)&#123; this.Realobject=obj; //初始化真实类的对象 &#125; /** * @param proxy 表示被代理的对象的，就是真实类的对象 * @param method 表示要调用真实类的方法 * @param args 表示方法调用的时候所需要的参数 * @return 方法调用之后的返回值 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; prefunction(); //执行之前调用的方法 Object res=method.invoke(Realobject, args); //Method类中的执行方法的函数，在反射中常用 afterFunction(); //执行之后调用的方法 return res; &#125; /** * 执行方法之前调用的方法 */ public void prefunction()&#123; System.out.println("执行方法之前......"); &#125; /** * 执行方法之后调用的方法 */ public void afterFunction()&#123; System.out.println("执行方法之后......"); &#125;&#125; 测试类 1234567891011121314151617181920212223import java.lang.reflect.Proxy;import com.sun.org.apache.bcel.internal.generic.NEW;import com.sun.org.apache.bcel.internal.util.Class2HTML;public class Client &#123; public static void main(String[] args) &#123; Class[] cls1=&#123;IKFC.class&#125;; //第一个代理的所有接口数组，直接用接口的反射即可 Class[] cls2=USAMacBook.class.getInterfaces(); //直接具体的实现类的反射调用getInterfaces即可返回所有的接口数组 // 返回KFC的代理对象 IKFC kfc = (IKFC) Proxy.newProxyInstance(Client.class.getClassLoader(), cls1, new ProxyHandler(new KFC())); kfc.eat(); //执行方法 MacBook macBook = (MacBook) Proxy.newProxyInstance(Client.class.getClassLoader(), cls2, new ProxyHandler( new USAMacBook())); macBook.buy(); //执行方法 &#125;&#125; 总结 动态代理的好处 即使有多个接口，也仅仅只有一个动态代理类 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[设计模式之单例模式]]></title>
      <url>%2F2018%2F04%2F12%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[创建型模式之单例模式(Singleton)什么是单例模式 该类只有一个实例 构造方法是私有的 有一个获取该类对象的静态方法getInstance() 应用场景 一个国家只有一个主席 如果此时的限定必须是抽象出来的类只能是一个对象，这个时候就需要使用单例模式 懒汉式什么是懒汉式 懒汉式是当用到这个对象的时候才会创建，即是在getInstance()方法创建这个单例对象 优缺点 只有用到的时候才会创建这个对象，因此节省资源 线程不安全 我们知道一旦我们使用了懒汉式就是在getInstance()方法中创建这个单例对象，那么不可避免的就是线程安全问题 实现12345678910111213141516171819202122/** * 懒汉式的单例模式： 不是线程安全的 * 优点： 在使用的时候才会初始化，可以节省资源 */public class SignalLazy &#123; // 将默认的构造器设置为private类型的 private SignalLazy() &#123; &#125; // 静态的单例对象 private static SignalLazy instance; //静态的获取单例的对象，其中有一个判断，如果没有初始化，那么就创建 public static SignalLazy getInstance() &#123; // 如果instance没有被初始化，那么就创建即可，这个是保证了单例，但是并不是线程安全的 if (instance == null) &#123; System.out.println("this is SignalLazy"); instance = new SignalLazy(); // 创建一 个对象 &#125; return instance; // 返回这个对象 &#125;&#125; 从上面的代码中我们可以知道一旦使用多线程创建对象，那么就会出现线程不安全，最后创建出来的就不是单例了 测试代码如下 12345678910111213141516171819public class MainTest &#123; public static void main(String[] args) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; //创建实例，并且输出其中的地址，如果地址相同， 那么就是同一个实例 System.out.println("this is"+ SignalLazy.getInstance()); &#125; &#125;).start(); //主线程也是创建输出其中的地址，运行可以看出这两个地址是不一样的 System.out.println("this is"+SignalLazy.getInstance()); &#125;&#125; 解决线程不安全 线程同步锁(synchronized) 我们知道每一个类都有一个把锁，我们可以使用线程同步锁来实现线程同步方法 但是使用线程同步锁浪费资源，因为每次创建实例都需要请求同步锁，浪费资源 12345678public synchronized static SignalLazy getInstance() &#123; // 如果instance没有被初始化，那么就创建即可，这个是保证了单例，但是并不是线程安全的 if (instance == null) &#123; System.out.println("this is SignalLazy"); instance = new SignalLazy(); // 创建一个对象 &#125; return instance; // 返回这个对象 &#125; 双重校验 双重校验： 两次判断单例对象是否为 null，这样的话，当当线程经过这个判断的时候就会先判断，而不是等待，一旦判断不成立，那么就会继续执行，不需要等待 相对于前面的同步方法更加节省资源 123456789101112131415161718192021222324public class SignalTonDoubleCheck &#123; private volatile static SignalTonDoubleCheck instance = null; private SignalTonDoubleCheck() &#123; &#125;; // 将默认的构造方法设置私有 public static SignalTonDoubleCheck getInstance() &#123; if (instance == null) &#123; synchronized (SignalTonDoubleCheck.class) &#123; if (instance == null) &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // 这个new 并不是原子操作，因此当多线程进行到这里需要及时刷新这个值，因此要设置为voliate instance = new SignalTonDoubleCheck(); &#125; &#125; &#125; return instance; &#125;&#125; 匿名内部类 （推荐使用） 我们知道静态变量、静态代码块、静态方法都是在类加载的时候只加载一次 12345678910111213141516public class SignalTonInnerHolder &#123; //私有构造函数 private SignalTonInnerHolder() &#123; &#125; /* * 匿名内部类，其中利用了静态成员变量在类加载的时候初始化，并且只加载一次，因此保证了单例 */ private static class InnerHolder &#123; private static SignalTonInnerHolder instance = new SignalTonInnerHolder(); &#125; public static SignalTonInnerHolder getInstance() &#123; return InnerHolder.instance; //加载类 &#125;&#125; 一旦加载SignalTonInnerHolder类的时候就会加载其中的静态类，随之加载的就是其中的创建对象语句，因此在类加载的时候就完成了创建，这个和我们后面说的饿汉式有点相同 饿汉式什么是饿汉式 在类加载的时候就创建单例对象，而不是在getInstance()方法创建 所谓的饿汉式就是利用静态成员变量或者静态语句块在类加载的时候初始化，并且只初始化一次，因此这个是线程安全的，但是在没有用到的时候就初始化，那么是浪费资源 优缺点 还没用到就创建，浪费资源 类加载的时候就创建，线程安全 实现12345678910111213141516/* * 饿汉式：线程安全 * */public class SignalHungry &#123; private SignalHungry() &#123; &#125; // 静态变量只有在类加载的时候初始化一次，因此这个是线程安全的 private static SignalHungry instance = new SignalHungry(); public static SignalHungry getInstance() &#123; return instance; &#125;&#125; 测试12345678910111213141516171819public class MainTest &#123; public static void main(String[] args) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; //创建实例，并且输出其中的地址，如果地址相同， 那么就是同一个实例 System.out.println("this is"+ SignalHungry.getInstance()); &#125; &#125;).start(); //主线程也是创建输出其中的地址，运行可以看出这两个地址是不一样的 System.out.println("this is"+SignalHungry.getInstance()); &#125;&#125; 总结 饿汉式在类加载的时候就会创建单例对象，因此浪费资源 懒汉式在用到的时候才创建，节省资源，但是线程不安全，但是我们可以使用匿名内部类的方式使其线程安全 一般在使用的时候会使用懒汉式的匿名内部类的实现和饿汉式的创建方式 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[设计模式之常见关系]]></title>
      <url>%2F2018%2F04%2F12%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B8%B8%E8%A7%81%E5%85%B3%E7%B3%BB%2F</url>
      <content type="text"><![CDATA[继承和泛化 泛华关系是一种继承关系，表示一般与特殊的关系，它指定了子类如何特化父类的所有特征和行为。 使用三角箭头的实线表示继承，其中箭头指向的是父类 接口与实现 在java中一个类只能继承一个父类，但是可以实现多个接口 使用的是带三角的虚线表示，其中箭头指向的是接口 依赖 是一种使用关系，即一个类的实现需要另外一个类的协助，所以尽量不使用双向的依赖关系。 最典型的就是import 比如：一个类要定义String类型的变量，那么这个类就是依赖String这个类 关联 是一种拥有的关系，它使一个类知道另外一个类的属性和方法，比如数据库中的关系，通过学生可以查找到自己课程的成绩，只需要在学生中定义一个课程的对象即可。 代码体现： 成员变量 带普通箭头的实心线，指向被拥有者 聚合 是整体和部分的关系，且部分可以离开整体而单独的存在。车和轮胎是整体和部分的关系，但是轮胎离开车还是可以单独存在的 代码体现： 成员变量 带空心菱形的实心线，菱形指向整体 组合 是整体和部分的关系，但是部分不能离开整体而单独存在 代码体现：成员变量 带实心菱形的实线，菱形指向整体 笔者有话说 最近建了一个微信交流群，提供给大家一个交流的平台，扫描下方笔者的微信二维码，备注【交流】，我会把大家拉进群]]></content>
    </entry>

    
  
  
</search>
