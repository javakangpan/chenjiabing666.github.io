<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[scrapy设置请求池]]></title>
      <url>%2F2017%2F03%2F26%2Fscrapy%E8%AE%BE%E7%BD%AE%E8%AF%B7%E6%B1%82%E6%B1%A0%2F</url>
      <content type="text"><![CDATA[scrapy设置”请求池”引言 相信大家有时候爬虫发出请求的时候会被ban，返回的是403错误，这个就是请求头的问题，其实在python发出请求时，使用的是默认的自己的请求头，网站管理者肯定会不允许机器访问的，但是有些比较low的网站还是可以访问的，有时候网站管理者看到同一个请求头在一秒内请求多次，傻子都知道这是机器在访问，因此会被ban掉，这时就需要设置请求池了，这个和ip代理池是一个概念 爬虫请求常见的错误 200：请求成功 处理方式：获得响应的内容，进行处理201：请求完成，结果是创建了新资源。新创建资源的 URI 可在响应的实体中得到 处理方式：爬虫中不会遇到202：请求被接受，但处理尚未完成 处理方式：阻塞等待204：服务器端已经实现了请求，但是没有返回新的信 息。如果客户是用户代理，则无须为此更新自身的文档视图。 处理方式：丢弃300：该状态码不被 HTTP/1.0 的应用程序直接使用， 只是作为 3XX 类型回应的默认解释。存在多个可用的被请求资源。 处理方式：若程序中能够处理，则进行进一步处理，如果程序中不能处理，则丢弃301：请求到的资源都会分配一个永久的 URL，这样就可以在将来通过该 URL 来访问此资源 处理方式：重定向到分配的 URL302：请求到的资源在一个不同的 URL 处临时保存 处理方式：重定向到临时的 URL304 请求的资源未更新 处理方式：丢弃400 非法请求 处理方式：丢弃401 未授权 处理方式：丢弃403 禁止 处理方式：丢弃404 没有找到 处理方式：丢弃5XX 回应代码以“5”开头的状态码表示服务器端发现自己出现错误，不能继续执行请求 处理方式：丢弃 话不多说直接撸代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from scrapy import logimport randomfrom scrapy.downloadermiddlewares.useragent import UserAgentMiddlewareclass RotateUserAgentMiddleware(UserAgentMiddleware):# for more user agent strings,you can find it in http://www.useragentstring.com/pages/useragentstring.phpuser_agent_list = [ "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 " "(KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1", "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 " "(KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 " "(KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6", "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 " "(KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6", "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 " "(KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1", "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 " "(KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5", "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 " "(KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 " "(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3", "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 " "(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 " "(KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3", "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 " "(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 " "(KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3", "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 " "(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3", "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 " "(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3", "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 " "(KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3", "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 " "(KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3", "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 " "(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24", "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 " "(KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"]def process_request(self, request, spider): ua = random.choice(self.user_agent_list) if ua: # 显示当前使用的useragent print "********Current UserAgent:%s************" % ua # 记录 log.msg('Current UserAgent: ' + ua) request.headers.setdefault('User-Agent', ua) 说明 这里的思路就是在下载器中间件中对request设置请求，这里是使用request.headers.setdefault(&quot;User-Agent&quot;,user_agent)这个函数设置请求头，对于下载器中间件在我博客前面的文章已经有说明，想要了解的请点击 注意 这里还要说明的是设置了请求池还要在配置文件settins中设置一下，具体设置方法和设置代理ip一样，详情请看scrapy代理ip的设置 作者说 本人秉着方便他人的想法才开始写技术文章的，因为对于自学的人来说想要找到系统的学习教程很困难，这一点我深有体会，我也是在不断的摸索中才小有所成，如果你们觉得我写的不错就帮我推广一下，让更多的人看到。另外如果有什么错误的地方也要及时联系我，方便我改进，谢谢大家对我的支持 版权信息所有者：chenjiabing如若转载请标明出处：chenjiabing666.github.io6]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Scrapy中使用cookie免于验证登录和模拟登录]]></title>
      <url>%2F2017%2F03%2F26%2FScrapy%E4%B8%AD%E4%BD%BF%E7%94%A8cookie%E5%85%8D%E4%BA%8E%E9%AA%8C%E8%AF%81%E7%99%BB%E5%BD%95%E5%92%8C%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%2F</url>
      <content type="text"><![CDATA[Scrapy中使用cookie免于验证登录和模拟登录引言 python爬虫我认为最困难的问题一个是ip代理，另外一个就是模拟登录了，更操蛋的就是模拟登录了之后还有验证码，真的是不让人省心，不过既然有了反爬虫，那么就有反反爬虫的策略，这里就先介绍一个cookie模拟登陆，后续还有seleminum+phantomjs模拟浏览器登录的文章。还不知道cookie是什么朋友们，可以点击这里 cookie提取方法： 打开谷歌浏览器或者火狐浏览器，如果是谷歌浏览器的按F12这个键就会跳出来浏览器控制台，然后点击Network，之后就是刷新网页开始抓包了，之后在抓到的页面中随便打开一个，就能看到cokie了，但是这里的cookie并不符合python中的格式，因此需要转换格式，下面提供了转换的代码 1234567891011121314151617181920212223# -*- coding: utf-8 -*-class transCookie:def __init__(self, cookie): self.cookie = cookiedef stringToDict(self): ''' 将从浏览器上Copy来的cookie字符串转化为Scrapy能使用的Dict :return: ''' itemDict = &#123;&#125; items = self.cookie.split(';') for item in items: key = item.split('=')[0].replace(' ', '') value = item.split('=')[1] itemDict[key] = value return itemDictif __name__ == "__main__":cookie = "你复制的cookie"trans = transCookie(cookie)print trans.stringToDict() 补充说明： 只需要将你网页上的cookie复制到上述代码中直接运行就可以了 使用cookie操作scrapy 直接撸代码 123456789101112131415161718# -*- coding: utf-8 -*-import scrapyfrom scrapy.conf import settings #从settings文件中导入Cookie，这里也可以室友from scrapy.conf import settings.COOKIEclass DemoSpider(scrapy.Spider):name = "demo"#allowed_domains = ["csdn.com"]start_urls = ["http://write.blog.csdn.net/postlist"]cookie = settings['COOKIE'] # 带着Cookie向网页发请求\headers = &#123; 'Connection': 'keep - alive', # 保持链接状态 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.82 Safari/537.36'&#125;def start_requests(self): yield scrapy.Request(url=self.start_urls[0],headers=self.headers,cookies=self.cookie)# 这里带着cookie发出请求def parse(self, response): print response.body 说明 这里是scrapy工程目录下spiders目录下的主要的解析网页的py文件相信学过scrapy的应该不会陌生，上述代码中的cookie值是放在Settings文件中的，因此使用的时候需要导入，当然你也可以直接将cookie粘贴到这个文件中 注意 虽说这里使用直接使用cookie可以省去很多麻烦，但是cookie的生命周期特别的短，不过小型的项目足够使用了，向那些需要爬两三天甚至几个月的项目就不适用了，因此在隔一段时间就要重新换cookie的值，虽说有很多麻烦，但是我还是比较喜欢这种方法的，因为可以省去不少脑筋 作者说 本人秉着方便他人的想法才开始写技术文章的，因为对于自学的人来说想要找到系统的学习教程很困难，这一点我深有体会，我也是在不断的摸索中才小有所成，如果你们觉得我写的不错就帮我推广一下，让更多的人看到。另外如果有什么错误的地方也要及时联系我，方便我改进，谢谢大家对我的支持。 最后欢迎大家看看我的其他scrapy文章 scrapy设置代理ip scrapy架构初探 scrapy初试 scrapy下载器中间件 版权信息所有者：chenjiabing如若转载请标明出处：chenjiabing666.github.io6]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[scrapy设置代理ip]]></title>
      <url>%2F2017%2F03%2F26%2Fscrapy%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86ip%2F</url>
      <content type="text"><![CDATA[scrapy代理的设置 在我的上一篇文章介绍了scrapy下载器中间件的使用,这里的scrapyIP的代理就是用这个原理实现的，重写了下载器中间件的process_request(self,request,spider)这个函数,这个函数的主要作用就是对request进行处理。 话不多说直接撸代码 123456789101112131415161718192021222324import random import scrapyimport loggingclass proxMiddleware(object):#proxy_list=[&#123;'http': 'http://123.157.146.116:8123'&#125;, &#123;'http': 'http://116.55.16.233:8998'&#125;, &#123;'http': 'http://115.85.233.94:80'&#125;, &#123;'http': 'http://180.76.154.5:8888'&#125;, &#123;'http': 'http://139.213.135.81:80'&#125;, &#123;'http': 'http://124.88.67.14:80'&#125;, &#123;'http': 'http://106.46.136.90:808'&#125;, &#123;'http': 'http://106.46.136.226:808'&#125;, &#123;'http': 'http://124.88.67.21:843'&#125;, &#123;'http': 'http://113.245.84.253:8118'&#125;, &#123;'http': 'http://124.88.67.10:80'&#125;, &#123;'http': 'http://171.38.141.12:8123'&#125;, &#123;'http': 'http://124.88.67.52:843'&#125;, &#123;'http': 'http://106.46.136.237:808'&#125;, &#123;'http': 'http://106.46.136.105:808'&#125;, &#123;'http': 'http://106.46.136.190:808'&#125;, &#123;'http': 'http://106.46.136.186:808'&#125;, &#123;'http': 'http://101.81.120.58:8118'&#125;, &#123;'http': 'http://106.46.136.250:808'&#125;, &#123;'http': 'http://106.46.136.8:808'&#125;, &#123;'http': 'http://111.78.188.157:8998'&#125;, &#123;'http': 'http://106.46.136.139:808'&#125;, &#123;'http': 'http://101.53.101.172:9999'&#125;, &#123;'http': 'http://27.159.125.68:8118'&#125;, &#123;'http': 'http://183.32.88.133:808'&#125;, &#123;'http': 'http://171.38.37.193:8123'&#125;]proxy_list=[ "http://180.76.154.5:8888", "http://14.109.107.1:8998", "http://106.46.136.159:808", "http://175.155.24.107:808", "http://124.88.67.10:80", "http://124.88.67.14:80", "http://58.23.122.79:8118", "http://123.157.146.116:8123", "http://124.88.67.21:843", "http://106.46.136.226:808", "http://101.81.120.58:8118", "http://180.175.145.148:808"]def process_request(self,request,spider): # if not request.meta['proxies']: ip = random.choice(self.proxy_list) print ip #print 'ip=' %ip request.meta['proxy'] = ip 主要的原理： 给出一个代理列表，然后在这个列表中随机取出一个代理，设置在request中，其中request.meta[&#39;proxy&#39;]就是设置代理的格式 但是现在主要的问题就是没有代理ip可用，如果去买的话又太贵了，自己玩玩买代理不值当，所以只好自己写爬虫去爬取免费的代理了，但是免费的代理存活的时间是有限的，这是个非常麻烦的事情，我提供的方法就是实现自己的一个ip代理池，每天定时更新自己的代理池，具体的实现方法会在下一篇文章中介绍，现在提供一段代码用来爬取西刺网站的代理 直接撸代码，接招吧 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#coding:utf-8import requestsfrom bs4 import BeautifulSoupimport threadingimport Queueclass Get_ips():def __init__(self,page): self.ips=[] self.urls=[] for i in range(page): self.urls.append("http://www.xicidaili.com/nn/" + str(i)) self.header = &#123;"User-Agent": 'Mozilla/5.0 (Windows NT 6.3; WOW64; rv:43.0) Gecko/20100101 Firefox/43.0'&#125; #self.file=open("ips",'w') self.q=Queue.Queue() self.Lock=threading.Lock()def get_ips(self): for url in self.urls: res = requests.get(url, headers=self.header) soup = BeautifulSoup(res.text, 'lxml') ips = soup.find_all('tr') for i in range(1, len(ips)): ip = ips[i] tds = ip.find_all("td") ip_temp = "http://" + tds[1].contents[0] + ":" + tds[2].contents[0] # print str(ip_temp) self.q.put(str(ip_temp))def review_ips(self): while not self.q.empty(): ip=self.q.get() try: proxy=&#123;"http": ip&#125; #print proxy res = requests.get("http://www.baidu.com", proxies=proxy,timeout=5) self.Lock.acquire() if res.status_code == 200: self.ips.append(ip) print ip self.Lock.release() except Exception: pass #print 'error'def main(self): self.get_ips() threads=[] for i in range(40): threads.append(threading.Thread(target=self.review_ips,args=[])) for t in threads: t.start() for t in threads: t.join() return self.ipsdef get_ip():my=Get_ips(4)return my.main()get_ip() 实现的原理 这里用到了BeautifulSoup解析页面，然后将提取到的代理交给队列，然后再通过共享队列分配给线程，这里主要开启线程通过设置代理ip访问一个网站，因为访问网站的时间比较长，因此要开起多个线程，相信大家能够学习设置代理ip了应该都是比较上手的了，这里具体的代码就不一一解释了，如果代码有什么问题可以及时联系我，我的联系方式在关于我的一栏中有提到 补充 想要ip应用起来，还要在配置文件settings中添加DOWNLOADER_MIDDLEWARES = { &#39;demo.proxy.proxMiddleware&#39;:400 }这里的demo是工程的名字，proxy是py文件的名,proxMiddleware是类的名字 当然这里可能你觉得proxy_list写在这里有点冗余，你可以在配置文件中定义，然后将配置文件的内容import到py文件中 以上全是博主慢慢摸索出来的，可以说自学一门技术真的很难，学习python爬虫已经有两三个月了，可以说全是自己通过看项目，网上查资料才有了今天的成功，不过现在还有几个问题没有解决，就是分布式爬虫、移动端爬取，博主接下来就要主攻这两个方面，学好之后会在自己的博客上分享学习心得的，因为网上没有系统的学习教程，对于自学的人来说实在是太痛苦了 版权信息所有者：chenjiabing如若转载请标明出处：chenjiabing666.github.io6]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[scrapy的下载器中间件]]></title>
      <url>%2F2017%2F03%2F25%2Fscrapy%E7%9A%84%E4%B8%8B%E8%BD%BD%E5%99%A8%E4%B8%AD%E9%97%B4%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[scrapy中的下载器中间件下载中间件 下载器中间件是介于Scrapy的request/response处理的钩子框架。 是用于全局修改Scrapy request和response的一个轻量、底层的系统。 编写下载器中间件 1. process_request(request, spider)当每个request通过下载中间件时，该方法被调用。process_request() 必须返回其中之一: 返回 None 、返回一个 Response 对象、返回一个 Request对象或raise IgnoreRequest 。 如果其返回 None ，Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用， 该request被执行(其response被下载)。 如果其返回 Response 对象，Scrapy将不会调用 任何 其他的 process_request() 或 process_exception() 方法，或相应地下载函数； 其将返回该response。 已安装的中间件的 process_response() 方法则会在每个response返回时被调用。 如果其返回 Request 对象，Scrapy则停止调用 process_request方法并重新调度返回的request。当新返回的request被执行后， 相应地中间件链将会根据下载的response被调用。 如果其raise一个 IgnoreRequest 异常，则安装的下载中间件的 process_exception() 方法会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)。 参数: request (Request 对象) – 处理的request spider (Spider 对象) – 该request对应的spider 2. process_response(request, response, spider) process_response() 必须返回以下之一: 返回一个 Response对象、 返回一个Request 对象或raise一个 IgnoreRequest 异常。 如果其返回一个 Response (可以与传入的response相同，也可以是全新的对象)， 该response会被在链中的其他中间件的 process_response() 方法处理。 如果其返回一个 Request 对象，则中间件链停止， 返回的request会被重新调度下载。处理类似于 process_request() 返回request所做的那样。 如果其抛出一个 IgnoreRequest 异常，则调用request的errback(Request.errback)。 如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)。 参数: request (Request对象) – response所对应的request response (Response 对象) – 被处理的response spider (Spider 对象) – response所对应的spider 3.process_exception(request, exception, spider) 当下载处理器(download handler)或 process_request() (下载中间件)抛出异常(包括 IgnoreRequest 异常)时， Scrapy调用 process_exception() 。 process_exception() 应该返回以下之一: 返回 None 、 一个 Response 对象、或者一个 Request 对象。 如果其返回 None ，Scrapy将会继续处理该异常，接着调用已安装的其他中间件的 process_exception() 方法，直到所有中间件都被调用完毕，则调用默认的异常处理。 如果其返回一个 Response 对象，则已安装的中间件链的 process_response() 方法被调用。Scrapy将不会调用任何其他中间件的 process_exception() 方法。 如果其返回一个 Request 对象， 则返回的request将会被重新调用下载。这将停止中间件的 process_exception() 方法执行，就如返回一个response的那样。 参数: request (是 Request 对象) – 产生异常的request exception (Exception 对象) – 抛出的异常 spider (Spider 对象) – request对应的spider 总结： 总的来说下载器中间件就是起到处理request请求并且返回response的作用，一切从网页爬取的url发起的请求会组成一个请求队列，然后一个一个排队经过下载器中间件，之后下载器中间件会对request做出相应的处理，比如添加请求头，添加代理等等，然后通过process_response返回一个response，之后就是用得到的response做出相应的分析，当然这里的内容页可以不实现，但是如果要爬取大型的网站，会遇到被ban的可能就要在下载器中间件这里着手，设置一些相应的请求头，ip代理等等内容。以上纯属个人逐渐摸索总结出来的内容，如果有什么错误欢迎指正 版权信息所有者：chenjiabing如若转载请标明出处：chenjiabing666.github.io6]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[scrapy初试]]></title>
      <url>%2F2017%2F03%2F25%2Fscrapy%E5%88%9D%E8%AF%95%2F</url>
      <content type="text"><![CDATA[scrapy初试 创建项目 打开cmd，在终端输入scrapy startproject tutorial,这里将在指定的文件夹下创建一个scrapy工程 其中将会创建以下的文件： scrapy.cfg: 项目的配置文件 tutorial/: 该项目的python模块。之后您将在此加入代码。 tutorial/items.py: 项目中的item文件. tutorial/pipelines.py: 项目中的pipelines文件. tutorial/settings.py: 项目的设置文件. tutorial/spiders/: 放置spider代码的目录. 定义item Item是保存爬取到的数据的容器；其使用方法和python字典类似， 并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。 类似在ORM中做的一样，您可以通过创建一个 scrapy.Item 类， 并且定义类型为 scrapy.Field的类属性来定义一个Item。 (如果不了解ORM, 不用担心，您会发现这个步骤非常简单) 首先根据需要从dmoz.org获取到的数据对item进行建模。 我们需要从dmoz中获取名字，url，以及网站的描述。 对此，在item中定义相应的字段。编辑 tutorial 目录中的 items.py 文件: 12345import scrapyclass DmozItem(scrapy.Item):title = scrapy.Field()link = scrapy.Field()desc = scrapy.Field() 一开始这看起来可能有点复杂，但是通过定义item， 您可以很方便的使用Scrapy的其他方法。而这些方法需要知道您的item的定义. 编写第一个爬虫 在工程的根目录下打开终端输入scrapy genspider demo douban.com这里的demo是spders文件下的主要py文件douban.com是要爬取的域名，会在demo.py中的 allowed_domains中显示，主要的功能就是限制爬取的url spider代码中内容解析 name: 用于区别Spider。 该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。 start_urls: 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。 parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request对象。 以下是spider目录下的demo.py的代码 1234567891011121314import scrapyclass DmozSpider(scrapy.Spider):name = "dmoz"allowed_domains = ["dmoz.org"]start_urls = [ "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/", "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"]def parse(self, response): filename = response.url.split("/")[-2] with open(filename, 'wb') as f: f.write(response.body) spider的爬取 进入工程的根目录下打开终端输入：scrapy crawl dmoz spider中的数据存取 在工程的根目录下打开终端输入scrapy crawl dmoz -o items.json这里是将数据存储到json文件中]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java中的IO操作]]></title>
      <url>%2F2017%2F03%2F25%2Fjava%E4%B8%AD%E7%9A%84IO%E6%93%8D%E4%BD%9C%2F</url>
      <content type="text"><![CDATA[java中IO操作读取文件中的内容 使用Scanner读取文本中的内容 相信大家都知道Scanner console=new Scanner(System.in)是用来读取控制台上输入的内容，但是这里是用来读取文件的内容，原理是一样的，只是对象不同罢了，这里用到的是File对象，用来创建一个文件对象 123456Scanner input=new Scanner(new File("hello.txt"));//创建一个对象inputwhile(input.hasNextLine()) //这里用来判断是否还有内容， 以免读到最后发生错误&#123;String content=input.nextLine();System.out.println(content);&#125; 这里顺便补充一下Scannner中的几个函数： nextLine():读取一行的内容，包括空格，换行 nextInt():读取一个整型内容 nexDouble():读取一个双精度的浮点数 next():读取下一个内容，无论什么类型，其中遇到空格和换行默认是一个标记（即是跳过）和nextLine()类似 hasNext():用来判断文件中的还有下一个内容，无论什么类型的 hasNextInt() hasNextDouble()://相似，不在赘述 使用FileReader读取 用来读取字符文件的便捷类。此类的构造方法假定默认字符编码和默认字节缓冲区大小都是适当的。要自己指定这些值，可以先在 FileInputStream上构造一个 InputStreamReader。FileReader 用于读取字符流。要读取原始字节流，请考虑使用 FileInputStream。 //这里使用new File创建一个对象，同样的也可以直接将文件的绝对路径传入 FileReader file=new FileReader(new File(&quot;hello.txt&quot;)); while(file.ready()) //用来判断是否还有字符可读 { int content=file.read(); //这里的read是读取将单个字符 返回的是int，即是ascii码,这里官方文档说返回的是读取的字符数，但是我实验了一下返回的ascii码 System.out.println((char)content); //所以要将ascii码转换成字符 } file.close(); 常用的几个方法： read(): return int 上面介绍过 read(char[] cbuf,int int length):将内容读入到一个char类型的数组，length是读取的字符数，offest是偏移量 使用BufferedReader的类实现高效的读取文件 123456//传入一个reader创建一个对象 BufferedReader file= new BufferedReader(new FileReader("hello.txt")); System.out.println(file.skip(3));//实现将指针跳过3个字符 System.out.println((char)file.read()); //read的方法，和FileReader中的read一样 String line=file.readLine(); //读取一行 System.out.println(line); 常用的方法： readLine() read()：如果到了末尾返回-1 read(char [],int off,int length):和FileReader中的一样 ready():判断是否还可以读取，一般和read配对使用 skip(long n):跳过的字符数 close() 文件的写入 用FileWriter写入文件 12345/*创建将对象f传入FileWriter,其中Filewriter有两个参数，第一个是File对象后者是一个String(即是文件的路径），第二个参数是boolean类型的，表示是否在文件的末尾追加内容，默认的是false表示不用在末尾追加，如果想要在末尾追加要写入另外一个参数true,当然这里可以用更加简洁的方式创建：FileWriter file=new FileWriter("hello.txt",false);*/FileWriter file=new FileWriter(f,true);file.write("chenjiabing");//写入函数writefile.close(); //最后必须关闭文件的输入流，否则写入将会失败，这里不想c和c++ 其中Filewriter中的方法还有 flush：刷新缓存流 close append():当前的领会的就是写入数组:append(Arrays.toString(list)); getEncoding():返回此流使用的字符编码 用PrintStream写入文件 这里同样的是和System.out.println()一样的原理，System.out.println只是内部实现了PrintStream，这里是用来将指定的内容写入到文件中而已 12345PrintStream output=new PrintStream(new File("hello.txt"));//创建一个写入的对象outputoutput.print("flan");output.println("vmlkfamla");output.println("vmslfkmadvmfs;dm"); 这里是用BufferedWriter类写入文件(一个高效的写入方式) 简单介绍 将文本写入字符输出流，缓冲各个字符，从而提供单个字符、数组和字符串的高效写入。可以指定缓冲区的大小，或者接受默认的大小。在大多数情况下，默认值就足够大了。该类提供了 newLine() 方法，它使用平台自己的行分隔符概念，此概念由系统属性 line.separator 定义。并非所有平台都使用新行符 (‘\n’) 来终止各行。因此调用此方法来终止每个输出行要优于直接写入新行符。通常 Writer 将其输出立即发送到底层字符或字节流。除非要求提示输出，否则建议用 BufferedWriter 包装所有其 write() 操作可能开销很高的 Writer（如 FileWriters 和 OutputStreamWriters）。例如， PrintWriter out= new PrintWriter(new BufferedWriter(new FileWriter(&quot;foo.out&quot;))); 将缓冲 PrintWriter对文件的输出。如果没有缓冲，则每次调用 print() 方法会导致将字符转换为字节，然后立即入到文件，而这是极其低效的。 例子 12345BufferedWriter input=new BufferedWriter(new FileWriter("hello.txt")); input.write("这是一个文件读入的方法"); input.newLine(); input.write("一个高效的方法"); input.close(); 其他的方法 close() flush() newLine():写入一个换行，因为每一个操作系统上的换行符可能不一样，不能系统的都用”\n”表示 write() 详情参见API 版权信息所有者：chenjiabing如若转载请标明出处：chenjiabing666.github.io6]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[scrapy架构初探]]></title>
      <url>%2F2017%2F03%2F25%2Fscrapy%E6%9E%B6%E6%9E%84%E5%88%9D%E6%8E%A2%2F</url>
      <content type="text"><![CDATA[scrapy架构初探引言 Python即时网络爬虫启动的目标是一起把互联网变成大数据库。单纯的开放源代码并不是开源的全部，开源的核心是“开放的思想”，聚合最好的想法、技术、人员，所以将会参照众多领先产品，比如，Scrapy，ScrapingHub，import io等。 本文简单讲解一下Scrapy的架构。没错，通用提取器gsExtractor就是要集成到Scrapy架构中。 请注意，本文不想复述原文内容，而是为了开源Python爬虫的发展方向找参照，而且以9年来开发网络爬虫经验作为对标，从而本文含有不少笔者主观评述，如果想读Scrapy官方原文，请点击Scrapy官网的Architecture。 scrapy数据流 Scrapy中的数据流由执行引擎控制，下面的原文摘自Scrapy官网，我根据猜测做了点评，为进一步开发GooSeeker开源爬虫指示方向： The Engine gets the first URLs to crawl from the Spider and schedules them in the Scheduler, as Requests. URL谁来准备呢？看样子是Spider自己来准备，那么可以猜测Scrapy架构部分（不包括Spider）主要做事件调度，不管网址的存储。看起来类似GooSeeker会员中心的爬虫罗盘，为目标网站准备一批网址，放在罗盘中准备执行爬虫调度操作。所以，这个开源项目的下一个目标是把URL的管理放在一个集中的调度库里面。 The Engine asks the Scheduler for the next URLs to crawl. 看到这里其实挺难理解的，要看一些其他文档才能理解透。接第1点，引擎从Spider中把网址拿到以后，封装成一个Request，交给了事件循环，会被Scheduler收来做调度管理的，暂且理解成对Request做排队。引擎现在就找Scheduler要接下来要下载的网页地址。 The Scheduler returns the next URLs to crawl to the Engine and the Engine sends them to the Downloader, passing through the Downloader Middleware (request direction). 从调度器申请任务，把申请到的任务交给下载器，在下载器和引擎之间有个下载器中间件，这是作为一个开发框架的必备亮点，开发者可以在这里进行一些定制化扩展。 Once the page finishes downloading the Downloader generates a Response (with that page) and sends it to the Engine, passing through the Downloader Middleware (response direction). 下载完成了，产生一个Response，通过下载器中间件交给引擎。注意，Response和前面的Request的首字母都是大写，虽然我还没有看其它Scrapy文档，但是我猜测这是Scrapy框架内部的事件对象，也可以推测出是一个异步的事件驱动的引擎，就像DS打数机的三级事件循环一样，对于高性能、低开销引擎来说，这是必须的。 The Engine receives the Response from the Downloader and sends it to the Spider for processing, passing through the Spider Middleware (input direction). 再次出现一个中间件，给开发者足够的发挥空间。 The Spider processes the Response and returns scraped items and new Requests (to follow) to the Engine. 每个Spider顺序抓取一个个网页，完成一个就构造另一个Request事件，开始另一个网页的抓取。 The Engine passes scraped items and new Requests returned by a spider through Spider Middleware (output direction), and then sends processed items to Item Pipelines and processed Requests to the Scheduler. 引擎作事件分发 The process repeats (from step 1) until there are no more requests from the Scheduler. 持续不断地运行。 版权信息所有者：chenjiabing如若转载请标明出处：chenjiabing666.github.io6]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[java图形与文本处理一]]></title>
      <url>%2F2017%2F03%2F25%2Fjava%E5%9B%BE%E5%BD%A2%E4%B8%8E%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%80%2F</url>
      <content type="text"><![CDATA[java绘制图形和文本&lt;一&gt;开篇介绍(官方文档) java.awt类 Graphicsjava.lang.Object继承者 java.awt.Graphics直接已知子类：DebugGraphics, Graphics2Dpublic abstract class Graphics extends Object Graphics 类是所有图形上下文的抽象基类，允许应用程序在组件（已经在各种设备上实现）以及闭屏图像上进行绘制。Graphics 对象封装了 Java 支持的基本呈现操作所需的状态信息。此状态信息包括以下属性：要在其上绘制的 Component 对象。呈现和剪贴坐标的转换原点。当前剪贴区。当前颜色。当前字体。当前逻辑像素操作函数（XOR 或 Paint）。当前 XOR 交替颜色（参见 setXORMode(java.awt.Color)）。坐标是无限细分的，并且位于输出设备的像素之间。绘制图形轮廓的操作是通过使用像素大小的画笔遍历像素间无限细分路径的操作，画笔从路径上的锚点向下和向右绘制。填充图形的操作是填充图形内部区域无限细分路径操作。呈现水平文本的操作是呈现字符字形完全位于基线坐标之上的上升部分。图形画笔从要遍历的路径向下和向右绘制。其含义如下：如果绘制一个覆盖给定矩形的图形，那么该图形与填充被相同矩形所限定的图形相比，在右侧和底边多占用一行像素。如果沿着与一行文本基线相同的 y 坐标绘制一条水平线，那么除了文字的所有下降部分外，该线完全画在文本的下面。所有作为此 Graphics 对象方法的参数而出现的坐标，都是相对于调用该方法前的此 Graphics 对象转换原点的。所有呈现操作仅修改当前剪贴区所限定区域内的像素，此剪贴区是由用户空间中的 Shape 指定的，并通过使用 Graphics 对象的程序来控制。此用户剪贴区 被转换到设备空间中，并与设备剪贴区 组合，后者是通过窗口可见性和设备范围定义的。用户剪贴区和设备剪贴区的组合定义复合剪贴区，复合剪贴区确定最终的剪贴区域。用户剪贴区不能由呈现系统修改，以反映得到的复合剪贴区。用户剪贴区只能通过 setClip 或 clipRect 方法更改。所有的绘制或写入都以当前的颜色、当前绘图模式和当前字体完成。 绘制直线主要用到的内容是Graphics类中的drawLine函数定义： public abstract void drawLine(int x1,int y1,int x2,int y2)x1,y1是起始点的坐标，x2,y2是尾点的坐标 拓展 SetColor(Color color) setColor是Graphics类中的一个函数，主要是设置颜色作用，其中参数是Color类中的一个对象，用于定义自己的颜色，里面的变量的是RGB,定义的方法：Color color=newe Color(R,G,B) 代码 123456789101112131415161718192021222324252627282930 import java.awt.Graphics; import javax.swing.JFrame; import javax.swing.JPanel; public class DrawLineFrame extends JFrame &#123; DrawLinePanel linePanel = new DrawLinePanel(); public static void main(String args[]) &#123; // 主函数 DrawLineFrame frame = new DrawLineFrame(); // 创建一个继承JFrame的一个类对象 frame.setVisible(true); // 设置窗体可见，true为可见，false为不可见 &#125; public DrawLineFrame() &#123; super(); setTitle("绘制直线"); // 设置窗体的标题 setBounds(100, 100, 273, 167); // 设置窗体的显示位置和大小 setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); // 设置窗体的关闭方式，具体见官方文档 add(linePanel); // 将继承Jpanel类的容器对象添加在窗体中 &#125; class DrawLinePanel extends JPanel &#123; // 继承在JPanel类的一个内部类，用于定义直线 public void paint(Graphics g) &#123; // 重写JCommponent类中的paint方法，用来绘制直线 Color color=new Color(Color.Red);//这里用的是Color提供的颜色，当然读者也可以自己定义RGB颜色 g.setColor(Color);//将颜色作用于绘图上下文 g.drawLine(70, 50, 180, 50); // 调用方法 g.drawLine(70, 80, 180, 80); // 第二条直线 g.drawLine(110, 10, 140, 120); // 第三条 &#125; &#125;&#125; 绘制矩形主要用到的函数是：public abstract void drawRect(int x,int y,int width,int height)这里的x,y是矩形左上角的坐标，width，height是矩形的长和宽 拓展fillRect(int x,int y,int width,int height):绘制实心矩形 代码 1234567891011121314151617181920212223242526import java.awt.Graphics;import javax.swing.JFrame;import javax.swing.JPanel;public class DrawRectangleFrame extends JFrame &#123;DrawRectanglePanel rectPanel = new DrawRectanglePanel(); // 创建面板类的实例public static void main(String args[]) &#123; // 主方法 DrawRectangleFrame frame = new DrawRectangleFrame(); // 创建窗体类的实例 frame.setVisible(true); // 显示窗体&#125;public DrawRectangleFrame() &#123; super(); // 调用超类的构造方法 setTitle("绘制矩形"); // 窗体标题 setBounds(100, 100, 269, 184); // 窗体的显示位置和大小 setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); // 窗体关闭方式 add(rectPanel); // 将面板类的实例添加到窗体容器中&#125;class DrawRectanglePanel extends JPanel &#123; // 创建内部面板类 public void paint(Graphics g) &#123; // 重写paint()方法 g.drawRect(30, 40, 80, 60); // 绘制空心矩形 g.fillRect(140, 40, 80, 60); // 绘制实心矩形 &#125;&#125;&#125; 绘制椭圆 函数：public abstract void drawOval(int x,int y,int width,int height),其中x,y是外切矩形的左上角的坐标，width，height是长宽 拓展 其中将令width=height，即是一个圆了，fillOval(int x,int y,int width,int height)用来绘制实心的椭圆 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869 package com.zzk; import java.awt.Graphics; import javax.swing.JFrame; import javax.swing.JPanel; public class DrawEllipseFrame extends JFrame &#123; DrawEllipsePanel ellipsePanel = new DrawEllipsePanel(); // 创建面板类的实例 public static void main(String args[]) &#123; // 主方法 DrawEllipseFrame frame = new DrawEllipseFrame(); // 创建窗体类的实例 frame.setVisible(true); // 显示窗体 &#125; public DrawEllipseFrame() &#123; super(); // 调用超类的构造方法 setTitle("绘制椭圆"); // 窗体标题 setBounds(100, 100, 269, 222); // 窗体的显示位置和大小 setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); // 窗体关闭方式 add(ellipsePanel); // 将面板类的实例添加到窗体容器中 &#125; class DrawEllipsePanel extends JPanel &#123; // 创建内部面板类 public void paint(Graphics g) &#123; // 重写paint()方法 g.drawOval(30, 20, 80, 50); // 绘制空心椭圆 g.drawOval(150, 10, 50, 80); // 绘制空心椭圆 g.fillOval(40, 90, 50, 80); // 绘制实心椭圆 g.fillOval(140, 110, 80, 50); // 绘制实心椭圆 &#125; &#125; &#125; ``` &gt;&gt;## 绘制圆弧&gt;&gt;&gt;主要用到的函数`public astract void drawArc(int x,int y,int width,int height,int startAngle,int arcAngle)`，其中x,y是要绘制圆弧的左上角的坐标，width，height是要绘制的长宽，startAngle是开始角度，arcAngle是相对于开始角度而言的，弧跨越的角度，&gt;&gt;&gt;&gt;### 拓展:&gt;&gt;&gt;&gt;&gt;fillArc(int x,int y,int width,int height,int startAngle,int arcAngle)用来绘制实心圆弧&gt;&gt;&gt;&gt;&gt;当然你也可以用这个来绘制扇形，用drawLine方法将圆弧的两端连起来就可以了，不过这个对坐标的精确度就要求很高了，暂时不想费那个脑筋来搞了&gt;&gt;&gt;&gt;### 代码```java package com.zzk; import java.awt.Graphics; import javax.swing.JFrame; import javax.swing.JPanel; public class DrawArcFrame extends JFrame &#123; DrawArcPanel arcPanel = new DrawArcPanel(); // 创建面板类的实例 public static void main(String args[]) &#123; // 主方法 DrawArcFrame frame = new DrawArcFrame(); // 创建窗体类的实例 frame.setVisible(true); // 显示窗体 &#125; public DrawArcFrame() &#123; super(); // 调用超类的构造方法 setTitle("绘制圆弧"); // 窗体标题 setBounds(100, 100, 269, 184); // 窗体的显示位置和大小 setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); // 窗体关闭方式 add(arcPanel); // 将面板类的实例添加到窗体容器中 &#125; class DrawArcPanel extends JPanel &#123; // 创建内部面板类 public void paint(Graphics g) &#123; // 重写paint()方法 g.drawArc(20, 20, 80, 80, 0, 120); // 绘制圆弧 g.drawArc(20, 40, 80, 80, 0, -120); // 绘制圆弧 g.drawArc(150, 20, 80, 80, 180, -120);// 绘制圆弧 g.drawArc(150, 40, 80, 80, 180, 120); // 绘制圆弧 &#125; &#125; &#125; 绘制多边形 主要用到的函数是：public abstract void drawPolygon(int[] xpoints,int[] ypoints,int npoints)，其中xpoints：要绘制多边形的x坐标组，ypoints是要绘制多边形的y坐标组，npoints是多边形的n条边 拓展 fillPolygon(...)是绘制实心多边形的函数 代码 1234567891011121314151617181920212223242526package com.zzk;import java.awt.Graphics;import javax.swing.JFrame;import javax.swing.JPanel;public class DrawSectorFrame extends JFrame &#123;DrawSectorPanel sectorPanel = new DrawSectorPanel(); // 创建面板类的实例public static void main(String args[]) &#123; // 主方法 DrawSectorFrame frame = new DrawSectorFrame(); // 创建窗体类的实例 frame.setVisible(true); // 显示窗体&#125;public DrawSectorFrame() &#123; super(); // 调用超类的构造方法 setTitle("绘制填充扇形"); // 窗体标题 setBounds(100, 100, 278, 184); // 窗体的显示位置和大小 setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); // 窗体关闭方式 add(sectorPanel); // 将面板类的实例添加到窗体容器中&#125;class DrawSectorPanel extends JPanel &#123; // 创建内部面板类 public void paint(Graphics g) &#123; // 重写paint()方法 g.fillArc(40, 20, 80, 80, 0, 150); // 绘制填充扇形 g.fillArc(140, 20, 80, 80, 180, -150);// 绘制填充扇形 g.fillArc(40, 40, 80, 80, 0, -110); // 绘制填充扇形 g.fillArc(140, 40, 80, 80, 180, 110); // 绘制填充扇形 &#125;&#125;&#125; 绘制文本 主要用到的函数是：public abstract void drawString(String value,int x,int y),其中value是要绘制的文本，x,y是第一个字的坐标 拓展 SetFont(Font font):这个函数是用来设置文本的字体大小，颜色的，其中参数font是Font类中的 代码 12345678910111213141516171819202122232425262728293031323334353637package com.zzk;import java.awt.Font;import java.awt.Graphics;import javax.swing.JFrame;import javax.swing.JPanel;public class TextFontFrame extends JFrame &#123;ChangeTextFontPanel changeTextFontPanel = new ChangeTextFontPanel(); // 创建面板类的实例public static void main(String args[]) &#123; // 主方法 TextFontFrame frame = new TextFontFrame(); // 创建窗体类的实例 frame.setVisible(true); // 显示窗体&#125;public TextFontFrame() &#123; super(); // 调用超类的构造方法 setTitle("设置文本的字体"); // 窗体标题 setBounds(100, 100, 333, 199); // 窗体的显示位置和大小 setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); // 窗体关闭方式 add(changeTextFontPanel); // 将面板类的实例添加到窗体容器中&#125;class ChangeTextFontPanel extends JPanel &#123; // 创建内部面板类 public void paint(Graphics g) &#123; // 重写paint()方法 String value = "明日编程词典社区"; int x = 40; // 文本位置的横坐标 int y = 50; // 文本位置的纵坐标 Font font = new Font("华文行楷", Font.BOLD + Font.ITALIC, 26); // 创建字体对象 g.setFont(font); // 设置字体 g.drawString(value, x, y); // 绘制文本 value = "http://community.mrbccd.com"; x = 10; // 文本位置的横坐标 y = 100; // 文本位置的纵坐标 font = new Font("宋体", Font.BOLD, 20); // 创建字体对象 g.setFont(font); // 设置字体 g.drawString(value, x, y); // 绘制文本 &#125;&#125;&#125; 补充 字体样式包括Font.BLOD(粗体)，Font.ITALIC(斜体)，Font.PLAIN(普通字体)，其中如果要设置两种样式，可以用”+”连接，如：Font.BLOD+Font.ITALIC，这样就会同时设置了斜体和粗体样式 以上是本人的学习成果，通过不断的学习和探索，发现网上没有什么系统的学习java图形处理的文章，就下定决心准备好好写，于是前几天就花了一晚上的时间搭建了博客，以前都是在CSDN上写的，发现在那上面写，没有逼格，为了提高逼格，自己撸了一个博客，让我来自由发挥，另外喜欢编程的朋友可以加我的联系方式，我们可以一起探讨，在下面留言也是可以的哦,联系方式可以在我的关于我可以找到 版权信息所有者：chenjiabing如若转载请标明出处：chenjiabing666.github.io6]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2017%2F03%2F23%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
